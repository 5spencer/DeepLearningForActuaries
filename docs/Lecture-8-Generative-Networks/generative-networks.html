<!DOCTYPE html>
<html lang="en"><head>
<script src="generative-networks_files/libs/clipboard/clipboard.min.js"></script>
<script src="generative-networks_files/libs/quarto-html/tabby.min.js"></script>
<script src="generative-networks_files/libs/quarto-html/popper.min.js"></script>
<script src="generative-networks_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="generative-networks_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="generative-networks_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="generative-networks_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.1.251">

  <meta name="author" content="Dr Patrick Laub">
  <title>Generative Networks</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="generative-networks_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="generative-networks_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    div.columns{display: flex; gap: min(4vw, 1.5em);}
    div.column{flex: auto; overflow-x: auto;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
    ul.task-list li input[type="checkbox"] {
      width: 0.8em;
      margin: 0 0.8em 0.2em -1.6em;
      vertical-align: middle;
    }
    pre > code.sourceCode { white-space: pre; position: relative; }
    pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
    pre > code.sourceCode > span:empty { height: 1.2em; }
    .sourceCode { overflow: visible; }
    code.sourceCode > span { color: inherit; text-decoration: inherit; }
    div.sourceCode { margin: 1em 0; }
    pre.sourceCode { margin: 0; }
    @media screen {
    div.sourceCode { overflow: auto; }
    }
    @media print {
    pre > code.sourceCode { white-space: pre-wrap; }
    pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
    }
    pre.numberSource code
      { counter-reset: source-line 0; }
    pre.numberSource code > span
      { position: relative; left: -4em; counter-increment: source-line; }
    pre.numberSource code > span > a:first-child::before
      { content: counter(source-line);
        position: relative; left: -1em; text-align: right; vertical-align: baseline;
        border: none; display: inline-block;
        -webkit-touch-callout: none; -webkit-user-select: none;
        -khtml-user-select: none; -moz-user-select: none;
        -ms-user-select: none; user-select: none;
        padding: 0 4px; width: 4em;
      }
    pre.numberSource { margin-left: 3em;  padding-left: 4px; }
    div.sourceCode
      { color: #1f1c1b;  }
    @media screen {
    pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
    }
    code span { color: #1f1c1b; } /* Normal */
    code span.al { color: #bf0303; background-color: #f7e6e6; font-weight: bold; } /* Alert */
    code span.an { color: #ca60ca; } /* Annotation */
    code span.at { color: #0057ae; } /* Attribute */
    code span.bn { color: #b08000; } /* BaseN */
    code span.bu { color: #644a9b; font-weight: bold; } /* BuiltIn */
    code span.cf { color: #1f1c1b; font-weight: bold; } /* ControlFlow */
    code span.ch { color: #924c9d; } /* Char */
    code span.cn { color: #aa5500; } /* Constant */
    code span.co { color: #898887; } /* Comment */
    code span.cv { color: #0095ff; } /* CommentVar */
    code span.do { color: #607880; } /* Documentation */
    code span.dt { color: #0057ae; } /* DataType */
    code span.dv { color: #b08000; } /* DecVal */
    code span.er { color: #bf0303; text-decoration: underline; } /* Error */
    code span.ex { color: #0095ff; font-weight: bold; } /* Extension */
    code span.fl { color: #b08000; } /* Float */
    code span.fu { color: #644a9b; } /* Function */
    code span.im { color: #ff5500; } /* Import */
    code span.in { color: #b08000; } /* Information */
    code span.kw { color: #1f1c1b; font-weight: bold; } /* Keyword */
    code span.op { color: #ca60ca; } /* Operator */
    code span.ot { color: #006e28; } /* Other */
    code span.pp { color: #006e28; } /* Preprocessor */
    code span.re { color: #0057ae; background-color: #e0e9f8; } /* RegionMarker */
    code span.sc { color: #3daee9; } /* SpecialChar */
    code span.ss { color: #ff5500; } /* SpecialString */
    code span.st { color: #bf0303; } /* String */
    code span.va { color: #0057ae; } /* Variable */
    code span.vs { color: #bf0303; } /* VerbatimString */
    code span.wa { color: #bf0303; } /* Warning */
  </style>
  <link rel="stylesheet" href="generative-networks_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="generative-networks_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="generative-networks_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="generative-networks_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="generative-networks_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="generative-networks_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="generative-networks_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>
  <script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>
</head>
<body class="quarto-light">
<div class="line right"></div>
  <div class="reveal">
    <div class="slides">

<section id="title-slide" data-background-image="unsw-yellow-shape.png" data-background-size="contain !important" class="center">
  <h1 class="title">Generative Networks</h1>
  <p class="subtitle">ACTL3143/5111: Deep Learning for Actuaries</p>
  <p class="author">Dr Patrick Laub</p>
  <p class="date">Week 9</p>
</section>

<section id="lecture-outline" class="slide level2">
<h2>Lecture Outline</h2>
<p><br><br></p>
<div class="columns">
<div class="column">
<ul>
<li>Recap (project, lecture, Story Wall)</li>
<li>Continue car crash police report example</li>
<li>Word embeddings</li>
</ul>
</div><div class="column">
<ul>
<li>Text Generation</li>
<li>Image Generation</li>
<li>Autoencoders</li>
</ul>
</div>
</div>
<p><br></p>
<p>Thanks Hang Nguyen &amp; Michael Jacinto for draft slides.</p>
</section>
<section id="load-packages" class="slide level2" data-visibility="uncounted">
<h2>Load packages</h2>
<p><br></p>
<div class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="im">import</span> tensorflow <span class="im">as</span> tf</span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="im">from</span> tensorflow <span class="im">import</span> keras</span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="im">from</span> tensorflow.keras <span class="im">import</span> layers</span>
<span id="cb1-7"><a href="#cb1-7"></a></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="op">%</span>load_ext watermark</span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="op">%</span>watermark <span class="op">-</span>p matplotlib,numpy,pandas,tensorflow</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>matplotlib: 3.5.2
numpy     : 1.21.5
pandas    : 1.4.4
tensorflow: 2.8.0
</code></pre>
</div>
</div>
</section>
<section>
<section id="project" class="title-slide slide level1 center" data-background-image="unsw-yellow-shape.png" data-visibility="uncounted">
<h1>Project</h1>
<div class="footer">
<p>Now possible to upload report to shared Moodle page, under Week 10.</p>
</div>
</section>
<section id="on-time-series-splits" class="slide level2">
<h2>On time series splits</h2>
<p>If you have a lot of time series data, then use:</p>
<div class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">from</span> tensorflow.keras.utils <span class="im">import</span> timeseries_dataset_from_array</span>
<span id="cb3-2"><a href="#cb3-2"></a>data <span class="op">=</span> <span class="bu">range</span>(<span class="dv">20</span>)<span class="op">;</span> seq <span class="op">=</span> <span class="dv">3</span><span class="op">;</span> ts <span class="op">=</span> data[:<span class="op">-</span>seq]<span class="op">;</span> target <span class="op">=</span> data[seq:]</span>
<span id="cb3-3"><a href="#cb3-3"></a>nTrain <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.5</span> <span class="op">*</span> <span class="bu">len</span>(ts))<span class="op">;</span> nVal <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.25</span> <span class="op">*</span> <span class="bu">len</span>(ts))</span>
<span id="cb3-4"><a href="#cb3-4"></a>nTest <span class="op">=</span> <span class="bu">len</span>(ts) <span class="op">-</span> nTrain <span class="op">-</span> nVal</span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="bu">print</span>(<span class="ss">f"# Train: </span><span class="sc">{</span>nTrain<span class="sc">}</span><span class="ss">, # Val: </span><span class="sc">{</span>nVal<span class="sc">}</span><span class="ss">, # Test: </span><span class="sc">{</span>nTest<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code># Train: 8, # Val: 4, # Test: 5</code></pre>
</div>
</div>
<div class="columns">
<div class="column" style="width:33%;">
<div class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1"></a>trainDS <span class="op">=</span> <span class="op">\</span></span>
<span id="cb5-2"><a href="#cb5-2"></a>  timeseries_dataset_from_array(</span>
<span id="cb5-3"><a href="#cb5-3"></a>    ts, target, seq,</span>
<span id="cb5-4"><a href="#cb5-4"></a>    end_index<span class="op">=</span>nTrain)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1"></a>valDS <span class="op">=</span> <span class="op">\</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>  timeseries_dataset_from_array(</span>
<span id="cb6-3"><a href="#cb6-3"></a>    ts, target, seq,</span>
<span id="cb6-4"><a href="#cb6-4"></a>    start_index<span class="op">=</span>nTrain,</span>
<span id="cb6-5"><a href="#cb6-5"></a>    end_index<span class="op">=</span>nTrain<span class="op">+</span>nVal)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1"></a>testDS <span class="op">=</span> <span class="op">\</span></span>
<span id="cb7-2"><a href="#cb7-2"></a>  timeseries_dataset_from_array(</span>
<span id="cb7-3"><a href="#cb7-3"></a>    ts, target, seq,</span>
<span id="cb7-4"><a href="#cb7-4"></a>    start_index<span class="op">=</span>nTrain<span class="op">+</span>nVal)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="columns">
<div class="column" style="width:33%;">
<div class="cell" data-execution_count="7">
<div class="cell-output cell-output-stdout">
<pre><code>Training dataset
[0, 1, 2] 3
[1, 2, 3] 4
[2, 3, 4] 5
[3, 4, 5] 6
[4, 5, 6] 7
[5, 6, 7] 8</code></pre>
</div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell" data-execution_count="8">
<div class="cell-output cell-output-stdout">
<pre><code>Validation dataset
[8, 9, 10] 11
[9, 10, 11] 12</code></pre>
</div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell" data-execution_count="9">
<div class="cell-output cell-output-stdout">
<pre><code>Test dataset
[12, 13, 14] 15
[13, 14, 15] 16
[14, 15, 16] 17</code></pre>
</div>
</div>
</div>
</div>
<div class="footer">
<p>Adapted from: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Listing 10.7.</p>
</div>
</section>
<section id="on-time-series-splits-ii" class="slide level2">
<h2>On time series splits II</h2>
<p>If you <em>don’t</em> have a lot of time series data, consider:</p>
<div class="cell" data-execution_count="10">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1"></a>X <span class="op">=</span> []<span class="op">;</span> y <span class="op">=</span> []</span>
<span id="cb11-2"><a href="#cb11-2"></a><span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(data)<span class="op">-</span>seq):</span>
<span id="cb11-3"><a href="#cb11-3"></a>    X.append(data[i:i<span class="op">+</span>seq])</span>
<span id="cb11-4"><a href="#cb11-4"></a>    y.append(data[i<span class="op">+</span>seq])</span>
<span id="cb11-5"><a href="#cb11-5"></a>X <span class="op">=</span> np.array(X)<span class="op">;</span> y <span class="op">=</span> np.array(y)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column" style="width:33%;">
<div class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1"></a>nTrain <span class="op">=</span> <span class="bu">int</span>(<span class="fl">0.5</span> <span class="op">*</span> X.shape[<span class="dv">0</span>])</span>
<span id="cb12-2"><a href="#cb12-2"></a>X_train <span class="op">=</span> X[:nTrain]</span>
<span id="cb12-3"><a href="#cb12-3"></a>y_train <span class="op">=</span> y[:nTrain]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell" data-execution_count="12">
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1"></a>nVal <span class="op">=</span> <span class="bu">int</span>(np.ceil(<span class="fl">0.25</span> <span class="op">*</span> X.shape[<span class="dv">0</span>]))</span>
<span id="cb13-2"><a href="#cb13-2"></a>X_val <span class="op">=</span> X[nTrain:nTrain<span class="op">+</span>nVal]</span>
<span id="cb13-3"><a href="#cb13-3"></a>y_val <span class="op">=</span> y[nTrain:nTrain<span class="op">+</span>nVal]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell" data-execution_count="13">
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1"></a>nTest <span class="op">=</span> X.shape[<span class="dv">0</span>] <span class="op">-</span> nTrain <span class="op">-</span> nVal</span>
<span id="cb14-2"><a href="#cb14-2"></a>X_test <span class="op">=</span> X[nTrain<span class="op">+</span>nVal:]</span>
<span id="cb14-3"><a href="#cb14-3"></a>y_test <span class="op">=</span> y[nTrain<span class="op">+</span>nVal:]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</div>
</div>
<div class="columns">
<div class="column" style="width:33%;">
<div class="cell" data-execution_count="14">
<div class="cell-output cell-output-stdout">
<pre><code>Training dataset
[0, 1, 2] 3
[1, 2, 3] 4
[2, 3, 4] 5
[3, 4, 5] 6
[4, 5, 6] 7
[5, 6, 7] 8
[6, 7, 8] 9
[7, 8, 9] 10</code></pre>
</div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell" data-execution_count="15">
<div class="cell-output cell-output-stdout">
<pre><code>Validation dataset
[8, 9, 10] 11
[9, 10, 11] 12
[10, 11, 12] 13
[11, 12, 13] 14
[12, 13, 14] 15</code></pre>
</div>
</div>
</div><div class="column" style="width:33%;">
<div class="cell" data-execution_count="16">
<div class="cell-output cell-output-stdout">
<pre><code>Test dataset
[13, 14, 15] 16
[14, 15, 16] 17
[15, 16, 17] 18
[16, 17, 18] 19</code></pre>
</div>
</div>
</div>
</div>
</section></section>
<section>
<section id="previous-lecture" class="title-slide slide level1 center" data-background-image="unsw-yellow-shape.png" data-visibility="uncounted">
<h1>Previous lecture</h1>

</section>
<section id="escape-characters" class="slide level2">
<h2>Escape characters</h2>
<div class="columns">
<div class="column">
<div class="cell" data-execution_count="17">
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1"></a><span class="bu">print</span>(<span class="st">"Hello,</span><span class="ch">\t</span><span class="st">world!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Hello,  world!</code></pre>
</div>
</div>
<div class="cell" data-execution_count="18">
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1"></a><span class="bu">print</span>(<span class="st">"Line 1</span><span class="ch">\n</span><span class="st">Line 2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Line 1
Line 2</code></pre>
</div>
</div>
<div class="cell" data-execution_count="19">
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1"></a><span class="bu">print</span>(<span class="st">"Patrick</span><span class="ch">\r</span><span class="st">Laub"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="20">
<div class="cell-output cell-output-stdout">
<pre><code>Laubick</code></pre>
</div>
</div>
</div><div class="column">
<div class="cell" data-execution_count="21">
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1"></a><span class="bu">print</span>(<span class="st">"C:</span><span class="ch">\t</span><span class="st">om</span><span class="ch">\n</span><span class="st">ew folder"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>C:  om
ew folder</code></pre>
</div>
</div>
<p>Escape the backslash:</p>
<div class="cell" data-execution_count="22">
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1"></a><span class="bu">print</span>(<span class="st">"C:</span><span class="ch">\\</span><span class="st">tom</span><span class="ch">\\</span><span class="st">new folder"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>C:\tom\new folder</code></pre>
</div>
</div>
<div class="cell" data-execution_count="23">
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1"></a><span class="bu">repr</span>(<span class="st">"Hello,</span><span class="ch">\r</span><span class="st">world!"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="22">
<pre><code>"'Hello,\\rworld!'"</code></pre>
</div>
</div>
</div>
</div>
</section>
<section id="a-more-robust-permutation_test" class="slide level2">
<h2>A more robust <code>permutation_test</code></h2>
<div class="cell" data-execution_count="24">
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1"></a><span class="im">import</span> numpy.random <span class="im">as</span> rnd</span>
<span id="cb30-2"><a href="#cb30-2"></a></span>
<span id="cb30-3"><a href="#cb30-3"></a><span class="kw">def</span> permutation_test(model, X, y, numReps<span class="op">=</span><span class="dv">1</span>, seed<span class="op">=</span><span class="dv">42</span>):</span>
<span id="cb30-4"><a href="#cb30-4"></a>    <span class="co">"""</span></span>
<span id="cb30-5"><a href="#cb30-5"></a><span class="co">    Run the permutation test for variable importance.</span></span>
<span id="cb30-6"><a href="#cb30-6"></a><span class="co">    Returns matrix of shape (X.shape[1], len(model.evaluate(X, y))).</span></span>
<span id="cb30-7"><a href="#cb30-7"></a><span class="co">    """</span></span>
<span id="cb30-8"><a href="#cb30-8"></a>    rnd.seed(seed)</span>
<span id="cb30-9"><a href="#cb30-9"></a>    scores <span class="op">=</span> []    </span>
<span id="cb30-10"><a href="#cb30-10"></a></span>
<span id="cb30-11"><a href="#cb30-11"></a>    <span class="cf">for</span> j <span class="kw">in</span> <span class="bu">range</span>(X.shape[<span class="dv">1</span>]):</span>
<span id="cb30-12"><a href="#cb30-12"></a>        originalColumn <span class="op">=</span> np.copy(X[:, j])</span>
<span id="cb30-13"><a href="#cb30-13"></a>        colScores <span class="op">=</span> []</span>
<span id="cb30-14"><a href="#cb30-14"></a></span>
<span id="cb30-15"><a href="#cb30-15"></a>        <span class="cf">for</span> r <span class="kw">in</span> <span class="bu">range</span>(numReps):</span>
<span id="cb30-16"><a href="#cb30-16"></a>            rnd.shuffle(X[:,j])</span>
<span id="cb30-17"><a href="#cb30-17"></a>            colScores.append(model.evaluate(X, y, verbose<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb30-18"><a href="#cb30-18"></a></span>
<span id="cb30-19"><a href="#cb30-19"></a>        scores.append(np.mean(colScores, axis<span class="op">=</span><span class="dv">0</span>))</span>
<span id="cb30-20"><a href="#cb30-20"></a>        X[:,j] <span class="op">=</span> originalColumn</span>
<span id="cb30-21"><a href="#cb30-21"></a>    </span>
<span id="cb30-22"><a href="#cb30-22"></a>    <span class="cf">return</span> np.array(scores)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="pretrained-model" class="slide level2">
<h2>Pretrained model</h2>
<div class="cell" data-execution_count="25">
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1"></a><span class="im">from</span> tensorflow.keras.applications <span class="im">import</span> mobilenet</span>
<span id="cb31-2"><a href="#cb31-2"></a><span class="im">from</span> PIL <span class="im">import</span> Image</span>
<span id="cb31-3"><a href="#cb31-3"></a></span>
<span id="cb31-4"><a href="#cb31-4"></a>model <span class="op">=</span> mobilenet.MobileNet(weights<span class="op">=</span><span class="st">"imagenet"</span>)</span>
<span id="cb31-5"><a href="#cb31-5"></a></span>
<span id="cb31-6"><a href="#cb31-6"></a>imageFilenames <span class="op">=</span> [<span class="st">"patrick-0.jpg"</span>, <span class="st">"umbrella-0.jpg"</span>, <span class="st">"hand-15.jpg"</span>]</span>
<span id="cb31-7"><a href="#cb31-7"></a>images <span class="op">=</span> [np.asarray(Image.<span class="bu">open</span>(name)) <span class="cf">for</span> name <span class="kw">in</span> imageFilenames]</span>
<span id="cb31-8"><a href="#cb31-8"></a></span>
<span id="cb31-9"><a href="#cb31-9"></a>images_resized <span class="op">=</span> tf.image.resize(images, [<span class="dv">224</span>, <span class="dv">224</span>])</span>
<span id="cb31-10"><a href="#cb31-10"></a>inputs <span class="op">=</span> mobilenet.preprocess_input(images_resized)</span>
<span id="cb31-11"><a href="#cb31-11"></a></span>
<span id="cb31-12"><a href="#cb31-12"></a>Y_proba <span class="op">=</span> model.predict(inputs, verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb31-13"><a href="#cb31-13"></a>top_K <span class="op">=</span> mobilenet.decode_predictions(Y_proba, top<span class="op">=</span><span class="dv">3</span>)</span>
<span id="cb31-14"><a href="#cb31-14"></a></span>
<span id="cb31-15"><a href="#cb31-15"></a><span class="cf">for</span> image_index <span class="kw">in</span> <span class="bu">range</span>(<span class="bu">len</span>(images)):</span>
<span id="cb31-16"><a href="#cb31-16"></a>    <span class="bu">print</span>(<span class="ss">f"Image #</span><span class="sc">{</span>image_index<span class="sc">}</span><span class="ss">:"</span>)</span>
<span id="cb31-17"><a href="#cb31-17"></a>    <span class="cf">for</span> class_id, name, y_proba <span class="kw">in</span> top_K[image_index]:</span>
<span id="cb31-18"><a href="#cb31-18"></a>        <span class="bu">print</span>(<span class="ss">f" </span><span class="sc">{</span>class_id<span class="sc">}</span><span class="ss"> - </span><span class="sc">{</span>name<span class="sc">}</span><span class="ss"> </span><span class="sc">{</span><span class="bu">int</span>(y_proba<span class="op">*</span><span class="dv">100</span>)<span class="sc">}</span><span class="ss">%"</span>)</span>
<span id="cb31-19"><a href="#cb31-19"></a>    <span class="bu">print</span>()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="predicted-classes-mobilenet" class="slide level2">
<h2>Predicted classes (MobileNet)</h2>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:50%;">
<p><br><br></p>
<div class="cell" data-execution_count="26">
<div class="cell-output cell-output-stdout">
<pre><code>Image #0:
 n04350905 - suit 40%
 n04591157 - Windsor_tie 33%
 n02749479 - assault_rifle 12%

Image #1:
 n03529860 - home_theater 23%
 n02749479 - assault_rifle 9%
 n04009552 - projector 5%

Image #2:
 n03529860 - home_theater 8%
 n03924679 - photocopier 7%
 n02786058 - Band_Aid 6%
</code></pre>
</div>
</div>
</div><div class="column" style="width:20%;">
<p><img src="patrick-0.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="umbrella-0.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="hand-15.jpg" data-lazy-loaded="" style="padding: 0px; margin=0px"></p>
</div><div class="column" style="width:15%;">

</div>
</div>
</section>
<section id="predicted-classes-mobilenetv2" class="slide level2">
<h2>Predicted classes (MobileNetV2)</h2>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:50%;">
<p><br><br></p>
<div class="cell" data-execution_count="27">
<div class="cell-output cell-output-stdout">
<pre><code>Image #0:
 n04350905 - suit 36%
 n04591157 - Windsor_tie 8%
 n03630383 - lab_coat 6%

Image #1:
 n04023962 - punching_bag 8%
 n04336792 - stretcher 4%
 n03529860 - home_theater 4%

Image #2:
 n04404412 - television 43%
 n02977058 - cash_machine 5%
 n04152593 - screen 3%
</code></pre>
</div>
</div>
</div><div class="column" style="width:20%;">
<p><img src="patrick-0.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="umbrella-0.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="hand-15.jpg" data-lazy-loaded="" style="padding: 0px; margin=0px"></p>
</div><div class="column" style="width:15%;">

</div>
</div>
</section>
<section id="predicted-classes-inceptionv3" class="slide level2">
<h2>Predicted classes (InceptionV3)</h2>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:50%;">
<p><br><br></p>
<div class="cell" data-execution_count="28">
<div class="cell-output cell-output-stdout">
<pre><code>Image #0:
 n04350905 - suit 24%
 n04591157 - Windsor_tie 8%
 n03630383 - lab_coat 5%

Image #1:
 n04507155 - umbrella 43%
 n04344873 - studio_couch 2%
 n03529860 - home_theater 2%

Image #2:
 n04404412 - television 22%
 n03630383 - lab_coat 8%
 n02777292 - balance_beam 6%
</code></pre>
</div>
</div>
</div><div class="column" style="width:20%;">
<p><img src="patrick-0.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="umbrella-0.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="hand-15.jpg" data-lazy-loaded="" style="padding: 0px; margin=0px"></p>
</div><div class="column" style="width:15%;">

</div>
</div>
</section>
<section id="predicted-classes-mobilenet-1" class="slide level2">
<h2>Predicted classes (MobileNet)</h2>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:50%;">
<p><br><br></p>
<div class="cell" data-execution_count="29">
<div class="cell-output cell-output-stdout">
<pre><code>Image #0:
 n03483316 - hand_blower 24%
 n03271574 - electric_fan 8%
 n03476684 - hair_slide 4%

Image #1:
 n03942813 - ping-pong_ball 84%
 n02782093 - balloon 4%
 n04023962 - punching_bag 2%

Image #2:
 n04557648 - water_bottle 34%
 n04336792 - stretcher 14%
 n03529860 - home_theater 7%
</code></pre>
</div>
</div>
</div><div class="column" style="width:20%;">
<p><img src="charger-4.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="table-tennis-17.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="water-bottle-15.jpg" data-lazy-loaded="" style="padding: 0px; margin=0px"></p>
</div><div class="column" style="width:15%;">

</div>
</div>
</section>
<section id="predicted-classes-mobilenetv2-1" class="slide level2">
<h2>Predicted classes (MobileNetV2)</h2>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:50%;">
<p><br><br></p>
<div class="cell" data-execution_count="30">
<div class="cell-output cell-output-stdout">
<pre><code>Image #0:
 n03868863 - oxygen_mask 32%
 n03271574 - electric_fan 9%
 n03483316 - hand_blower 8%

Image #1:
 n03942813 - ping-pong_ball 31%
 n04270147 - spatula 11%
 n03970156 - plunger 7%

Image #2:
 n02815834 - beaker 38%
 n03868863 - oxygen_mask 16%
 n04557648 - water_bottle 6%
</code></pre>
</div>
</div>
</div><div class="column" style="width:20%;">
<p><img src="charger-4.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="table-tennis-17.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="water-bottle-15.jpg" data-lazy-loaded="" style="padding: 0px; margin=0px"></p>
</div><div class="column" style="width:15%;">

</div>
</div>
</section>
<section id="predicted-classes-inceptionv3-1" class="slide level2">
<h2>Predicted classes (InceptionV3)</h2>
<div class="columns">
<div class="column" style="width:15%;">

</div><div class="column" style="width:50%;">
<p><br><br></p>
<div class="cell" data-execution_count="31">
<div class="cell-output cell-output-stdout">
<pre><code>Image #0:
 n02815834 - beaker 37%
 n03868863 - oxygen_mask 12%
 n03630383 - lab_coat 7%

Image #1:
 n03942813 - ping-pong_ball 71%
 n02782093 - balloon 15%
 n02790996 - barbell 1%

Image #2:
 n04557648 - water_bottle 75%
 n03983396 - pop_bottle 9%
 n03868863 - oxygen_mask 1%
</code></pre>
</div>
</div>
</div><div class="column" style="width:20%;">
<p><img src="charger-4.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="table-tennis-17.jpg" data-lazy-loaded="" style="padding: 0px; margin: 0px"> <img src="water-bottle-15.jpg" data-lazy-loaded="" style="padding: 0px; margin=0px"></p>
</div><div class="column" style="width:15%;">

</div>
</div>
</section>
<section id="transfer-learned-model" class="slide level2">
<h2>Transfer learned model</h2>
<div class="columns">
<div class="column" style="width:65%;">
<div class="cell" data-execution_count="32">
<div class="sourceCode cell-code" id="cb38"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb38-1"><a href="#cb38-1"></a>modelFile <span class="op">=</span> <span class="st">"teachable-machine-model-3143.h5"</span></span>
<span id="cb38-2"><a href="#cb38-2"></a>model <span class="op">=</span> keras.models.load_model(modelFile)</span>
<span id="cb38-3"><a href="#cb38-3"></a>model.layers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="31">
<pre><code>[&lt;keras.engine.sequential.Sequential at 0x16e7999d0&gt;,
 &lt;keras.engine.sequential.Sequential at 0x16b7580a0&gt;]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="33">
<div class="sourceCode cell-code" id="cb40"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb40-1"><a href="#cb40-1"></a>model.layers[<span class="dv">0</span>].layers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="32">
<pre><code>[&lt;keras.engine.functional.Functional at 0x16e2e4bb0&gt;,
 &lt;keras.layers.pooling.GlobalAveragePooling2D at 0x16e25f2b0&gt;]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="34">
<div class="sourceCode cell-code" id="cb42"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb42-1"><a href="#cb42-1"></a>model.layers[<span class="dv">1</span>].layers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="33">
<pre><code>[&lt;keras.layers.core.dense.Dense at 0x16e097bb0&gt;,
 &lt;keras.layers.core.dense.Dense at 0x16e0f5640&gt;]</code></pre>
</div>
</div>
</div><div class="column" style="width:35%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="turtles-all-the-way-down.jpeg"></p>
<p></p><figcaption>Models inside of models…</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Source: <a href="https://www.behance.net/gallery/66946885/Turtles-All-the-Way-Down">Behance</a>.</p>
</div>
</section>
<section id="transfer-learned-model-ii" class="slide level2">
<h2>Transfer learned model II</h2>
<div class="cell" data-execution_count="35">
<div class="sourceCode cell-code" id="cb44"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb44-1"><a href="#cb44-1"></a>model.layers[<span class="dv">0</span>].layers[<span class="dv">0</span>].layers</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="34">
<pre><code>[&lt;keras.engine.input_layer.InputLayer at 0x16e258640&gt;,
 &lt;keras.layers.convolutional.ZeroPadding2D at 0x16e2a4f40&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16e21cbe0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e21cee0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4bd040&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b4bda90&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4bd340&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4bdb80&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4bdaf0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4b2910&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4b2d90&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4b2fd0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4ba730&gt;,
 &lt;keras.layers.convolutional.ZeroPadding2D at 0x16b4ba370&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b4ba8e0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4bae80&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4ac580&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4ac1c0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4acc70&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4acd30&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4b9610&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4b9a90&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b4b96d0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4b1040&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4b1700&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4b1130&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4b1df0&gt;,
 &lt;keras.layers.merge.Add at 0x16b4b1f10&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4b1eb0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4a0940&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4a0dc0&gt;,
 &lt;keras.layers.convolutional.ZeroPadding2D at 0x16b4a0f70&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b4af1c0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4af550&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4afc10&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4af640&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4afe80&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4b07c0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4b0ca0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4b0d60&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b4b0dc0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4406d0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b440d90&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4407c0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b440f40&gt;,
 &lt;keras.layers.merge.Add at 0x16b468970&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b468a00&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b468dc0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b455490&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b4550d0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b455a00&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b455af0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b455fa0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4477f0&gt;,
 &lt;keras.layers.merge.Add at 0x16b447ca0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b447d30&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b447cd0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b4697c0&gt;,
 &lt;keras.layers.convolutional.ZeroPadding2D at 0x16b469400&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b469970&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b469f10&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b46f610&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b46f040&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b46fd00&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b46fdc0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b46c6a0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b46cb20&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b46c760&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b4480d0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b448790&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b4481c0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b448e80&gt;,
 &lt;keras.layers.merge.Add at 0x16b448fa0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16b448f40&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16b46e9d0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16b46ee50&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16b46efd0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169fd0400&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169fd0ac0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169fd04f0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169fd0fa0&gt;,
 &lt;keras.layers.merge.Add at 0x169ff96a0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169ff9730&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169ff9d00&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169ff9dc0&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x169ff9e20&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169fc1730&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169fc1df0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169fc1fa0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169fdd520&gt;,
 &lt;keras.layers.merge.Add at 0x169fdd9d0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169fdda60&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169fdde20&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169ff44f0&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x169ff4130&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169ff4a60&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169ff4b50&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169ff4dc0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169fcf850&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169fcfcd0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169fcffa0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169f99670&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x169f992b0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169f99be0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169f99cd0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169f99f40&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169f959d0&gt;,
 &lt;keras.layers.merge.Add at 0x169f95e80&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169f95f10&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169f95e50&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169f899a0&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x169f895e0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169f89f10&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169fba610&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169fba040&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169fbad00&gt;,
 &lt;keras.layers.merge.Add at 0x169fbae20&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169fbadc0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169f88850&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169f88cd0&gt;,
 &lt;keras.layers.convolutional.ZeroPadding2D at 0x169f88910&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x169f88e80&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169f9c460&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x169f9cb20&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x169f9c550&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x169f9cf70&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16e1e26d0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1e2bb0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16e1e2c70&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16e1e2fd0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1e15e0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16e1e1ca0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16e1e16d0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1e1f10&gt;,
 &lt;keras.layers.merge.Add at 0x16e1c4880&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16e1c4910&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1c4ee0&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16e1c4fa0&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16e1c4fd0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1d5910&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16e1d5a00&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16e1d5fd0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1dd700&gt;,
 &lt;keras.layers.merge.Add at 0x16e1ddbb0&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16e1ddc40&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1ddf70&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16e1f26d0&gt;,
 &lt;keras.layers.convolutional.DepthwiseConv2D at 0x16e1f2310&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1f2c40&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16e1f2d30&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16e1f2fa0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1cda30&gt;,
 &lt;keras.layers.convolutional.Conv2D at 0x16e1cdeb0&gt;,
 &lt;keras.layers.normalization.batch_normalization.BatchNormalization at 0x16e1cdf70&gt;,
 &lt;keras.layers.advanced_activations.ReLU at 0x16e259850&gt;]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="36">
<div class="sourceCode cell-code" id="cb46"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb46-1"><a href="#cb46-1"></a><span class="bu">len</span>(model.layers[<span class="dv">0</span>].layers[<span class="dv">0</span>].layers)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="35">
<pre><code>155</code></pre>
</div>
</div>
</section>
<section id="transfer-learned-model-iii" class="slide level2">
<h2>Transfer learned model III</h2>
<div style="overflow:auto; height: 90%">
<p><img src="mobilenet-model.png"></p>
</div>
</section></section>
<section>
<section id="car-crash-nlp-part-ii" class="title-slide slide level1 center" data-background-image="unsw-yellow-shape.png" data-visibility="uncounted">
<h1>Car Crash NLP Part II</h1>
<div class="footer">
<p>Dataset source: <a href="https://github.com/JSchelldorfer/ActuarialDataScience/blob/master/12%20-%20NLP%20Using%20Transformers/Actuarial_Applications_of_NLP_Part_1.ipynb">Dr Jürg Schelldorfer’s GitHub</a>.</p>
</div>
</section>
<section id="the-data" class="slide level2">
<h2>The data</h2>
<div class="cell" data-execution_count="37">
<div class="sourceCode cell-code" id="cb48"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb48-1"><a href="#cb48-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb48-2"><a href="#cb48-2"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> train_test_split</span>
<span id="cb48-3"><a href="#cb48-3"></a><span class="im">from</span> sklearn.preprocessing <span class="im">import</span> LabelEncoder</span>
<span id="cb48-4"><a href="#cb48-4"></a></span>
<span id="cb48-5"><a href="#cb48-5"></a><span class="cf">if</span> <span class="kw">not</span> Path(<span class="st">"NHTSA_NMVCCS_extract.parquet.gzip"</span>).exists():</span>
<span id="cb48-6"><a href="#cb48-6"></a>    <span class="bu">print</span>(<span class="st">"Downloading dataset"</span>)</span>
<span id="cb48-7"><a href="#cb48-7"></a>    <span class="op">!</span>wget https:<span class="op">//</span>github.com<span class="op">/</span>JSchelldorfer<span class="op">/</span>ActuarialDataScience<span class="op">/</span>raw<span class="op">/</span>master<span class="op">/</span><span class="dv">12</span><span class="op">%</span><span class="dv">20</span><span class="op">-%</span><span class="dv">20</span><span class="er">NLP</span><span class="op">%</span><span class="dv">20</span><span class="er">Using</span><span class="op">%</span><span class="dv">20</span><span class="er">Transformers</span><span class="op">/</span>NHTSA_NMVCCS_extract.parquet.gzip</span>
<span id="cb48-8"><a href="#cb48-8"></a></span>
<span id="cb48-9"><a href="#cb48-9"></a>df <span class="op">=</span> pd.read_parquet(<span class="st">"NHTSA_NMVCCS_extract.parquet.gzip"</span>)</span>
<span id="cb48-10"><a href="#cb48-10"></a></span>
<span id="cb48-11"><a href="#cb48-11"></a>features <span class="op">=</span> df[<span class="st">"SUMMARY_EN"</span>]</span>
<span id="cb48-12"><a href="#cb48-12"></a>target <span class="op">=</span> LabelEncoder().fit_transform(df[<span class="st">"INJSEVB"</span>])</span>
<span id="cb48-13"><a href="#cb48-13"></a></span>
<span id="cb48-14"><a href="#cb48-14"></a>X_main, X_test, y_main, y_test <span class="op">=</span> <span class="op">\</span></span>
<span id="cb48-15"><a href="#cb48-15"></a>    train_test_split(features, target, test_size<span class="op">=</span><span class="fl">0.2</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb48-16"><a href="#cb48-16"></a>X_train, X_val, y_train, y_val <span class="op">=</span> <span class="op">\</span></span>
<span id="cb48-17"><a href="#cb48-17"></a>    train_test_split(X_main, y_main, test_size<span class="op">=</span><span class="fl">0.25</span>, random_state<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb48-18"><a href="#cb48-18"></a>X_train.shape, X_val.shape, X_test.shape</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="36">
<pre><code>((4169,), (1390,), (1390,))</code></pre>
</div>
</div>
</section>
<section id="what-is-tf-idf" class="slide level2">
<h2>What is TF-IDF?</h2>
<p>Stands for <em>term frequency-inverse document frequency</em>.</p>

<img data-src="td-idf-graphic.png" class="r-stretch quarto-figure-center"><p class="caption">Infographic explaining TF-IDF</p><div class="footer">
<p>Source: FiloTechnologia (2014), <a href="http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html">A simple java class for tf-idf scoring</a>, Blog post.</p>
</div>
</section>
<section id="using-keras-textvectorization" class="slide level2">
<h2>Using Keras <code>TextVectorization</code></h2>
<div class="cell" data-execution_count="38">
<div class="sourceCode cell-code" id="cb50"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb50-1"><a href="#cb50-1"></a>max_tokens <span class="op">=</span> <span class="dv">1_000</span></span>
<span id="cb50-2"><a href="#cb50-2"></a>vect <span class="op">=</span> layers.TextVectorization(</span>
<span id="cb50-3"><a href="#cb50-3"></a>    max_tokens<span class="op">=</span>max_tokens,</span>
<span id="cb50-4"><a href="#cb50-4"></a>    output_mode<span class="op">=</span><span class="st">"tf_idf"</span>,</span>
<span id="cb50-5"><a href="#cb50-5"></a>)</span>
<span id="cb50-6"><a href="#cb50-6"></a></span>
<span id="cb50-7"><a href="#cb50-7"></a>vect.adapt(X_train)</span>
<span id="cb50-8"><a href="#cb50-8"></a>vocab <span class="op">=</span> vect.get_vocabulary()</span>
<span id="cb50-9"><a href="#cb50-9"></a></span>
<span id="cb50-10"><a href="#cb50-10"></a>X_train_txt <span class="op">=</span> vect(X_train)</span>
<span id="cb50-11"><a href="#cb50-11"></a>X_val_txt <span class="op">=</span> vect(X_val)</span>
<span id="cb50-12"><a href="#cb50-12"></a>X_test_txt <span class="op">=</span> vect(X_test)</span>
<span id="cb50-13"><a href="#cb50-13"></a></span>
<span id="cb50-14"><a href="#cb50-14"></a><span class="bu">print</span>(vocab[:<span class="dv">50</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['[UNK]', 'the', 'was', 'a', 'to', 'of', 'and', 'vehicle', 'v1', 'in', 'driver', 'for', 'this', 'critical', 'v2', 'lane', 'he', 'on', 'with', 'that', 'left', 'roadway', 'coded', 'she', 'event', 'crash', 'not', 'at', 'intersection', 'traveling', 'right', 'precrash', 'one', 'as', 'from', 'two', 'were', 'by', 'had', 'reason', 'his', 'side', 'is', 'front', 'her', 'traffic', 'an', 'it', 'speed', 'stated']</code></pre>
</div>
</div>
</section>
<section id="the-tf-idf-vectors" class="slide level2">
<h2>The TF-IDF vectors</h2>
<div class="cell" data-execution_count="39">
<div class="sourceCode cell-code" id="cb52"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb52-1"><a href="#cb52-1"></a>pd.DataFrame(X_train_txt, columns<span class="op">=</span>vocab, index<span class="op">=</span>X_train.index)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="38">

<div>
<style scoped="">
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>[UNK]</th>
      <th>the</th>
      <th>was</th>
      <th>a</th>
      <th>to</th>
      <th>of</th>
      <th>and</th>
      <th>vehicle</th>
      <th>v1</th>
      <th>in</th>
      <th>...</th>
      <th>am</th>
      <th>2lane</th>
      <th>tractor</th>
      <th>start</th>
      <th>react</th>
      <th>lighted</th>
      <th>hyundai</th>
      <th>encroaching</th>
      <th>closely</th>
      <th>ordinarily</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>2532</th>
      <td>85.163025</td>
      <td>42.274662</td>
      <td>10.395409</td>
      <td>10.395409</td>
      <td>11.785541</td>
      <td>8.323526</td>
      <td>8.323526</td>
      <td>11.099958</td>
      <td>6.713714</td>
      <td>9.775118</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>6209</th>
      <td>23.226280</td>
      <td>17.325682</td>
      <td>10.395409</td>
      <td>5.544218</td>
      <td>4.159603</td>
      <td>5.549018</td>
      <td>7.629900</td>
      <td>4.856232</td>
      <td>5.221777</td>
      <td>4.887559</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>2561</th>
      <td>59.356049</td>
      <td>30.493198</td>
      <td>15.246599</td>
      <td>11.088436</td>
      <td>9.012472</td>
      <td>7.629900</td>
      <td>8.323526</td>
      <td>4.162484</td>
      <td>8.951618</td>
      <td>2.792891</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>...</th>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
      <td>...</td>
    </tr>
    <tr>
      <th>6882</th>
      <td>54.194653</td>
      <td>20.790817</td>
      <td>4.851191</td>
      <td>7.623300</td>
      <td>9.012472</td>
      <td>4.855391</td>
      <td>4.161763</td>
      <td>2.774990</td>
      <td>5.221777</td>
      <td>2.094668</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>3.61771</td>
    </tr>
    <tr>
      <th>206</th>
      <td>72.259537</td>
      <td>27.028063</td>
      <td>13.167518</td>
      <td>6.237246</td>
      <td>8.319205</td>
      <td>4.855391</td>
      <td>6.242645</td>
      <td>5.549979</td>
      <td>8.205649</td>
      <td>2.094668</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
    </tr>
    <tr>
      <th>6356</th>
      <td>33.549072</td>
      <td>15.246599</td>
      <td>9.702381</td>
      <td>8.316327</td>
      <td>7.625938</td>
      <td>5.549018</td>
      <td>7.629900</td>
      <td>5.549979</td>
      <td>5.967745</td>
      <td>8.378673</td>
      <td>...</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.0</td>
      <td>0.00000</td>
    </tr>
  </tbody>
</table>
<p>4169 rows × 1000 columns</p>
</div>
</div>
</div>
</section>
<section id="feed-tf-idf-into-an-ann" class="slide level2">
<h2>Feed TF-IDF into an ANN</h2>
<div class="cell" data-execution_count="40">
<div class="sourceCode cell-code" id="cb53"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb53-1"><a href="#cb53-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb53-2"><a href="#cb53-2"></a>tfidfModel <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb53-3"><a href="#cb53-3"></a>    layers.Dense(<span class="dv">250</span>, <span class="st">"relu"</span>, input_dim<span class="op">=</span>X_train_txt.shape[<span class="dv">1</span>]),</span>
<span id="cb53-4"><a href="#cb53-4"></a>    layers.Dense(<span class="dv">1</span>, <span class="st">"sigmoid"</span>)</span>
<span id="cb53-5"><a href="#cb53-5"></a>])</span>
<span id="cb53-6"><a href="#cb53-6"></a></span>
<span id="cb53-7"><a href="#cb53-7"></a>tfidfModel.<span class="bu">compile</span>(<span class="st">"adam"</span>, <span class="st">"BinaryCrossentropy"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb53-8"><a href="#cb53-8"></a>tfidfModel.summary(print_fn<span class="op">=</span>skip_empty)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
dense (Dense)               (None, 250)               250250
dense_1 (Dense)             (None, 1)                 251
=================================================================
Total params: 250,501
Trainable params: 250,501
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
</section>
<section id="fit-evaluate" class="slide level2">
<h2>Fit &amp; evaluate</h2>
<div class="cell" data-execution_count="41">
<div class="sourceCode cell-code" id="cb55"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb55-1"><a href="#cb55-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">3</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb55-2"><a href="#cb55-2"></a>    monitor<span class="op">=</span><span class="st">"val_accuracy"</span>, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb55-3"><a href="#cb55-3"></a></span>
<span id="cb55-4"><a href="#cb55-4"></a><span class="cf">if</span> <span class="kw">not</span> Path(<span class="st">"tfidfModel.h5"</span>).exists():</span>
<span id="cb55-5"><a href="#cb55-5"></a>    tfidfModel.fit(X_train_txt, y_train, epochs<span class="op">=</span><span class="dv">1_000</span>, callbacks<span class="op">=</span>es,</span>
<span id="cb55-6"><a href="#cb55-6"></a>        validation_data<span class="op">=</span>(X_val_txt, y_val), verbose<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb55-7"><a href="#cb55-7"></a>    tfidfModel.save(<span class="st">"tfidfModel.h5"</span>)</span>
<span id="cb55-8"><a href="#cb55-8"></a><span class="cf">else</span>:</span>
<span id="cb55-9"><a href="#cb55-9"></a>    tfidfModel <span class="op">=</span> keras.models.load_model(<span class="st">"tfidfModel.h5"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="42">
<div class="sourceCode cell-code" id="cb56"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb56-1"><a href="#cb56-1"></a>tfidfModel.evaluate(X_train_txt, y_train, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="41">
<pre><code>[0.0011804004898294806, 1.0]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="43">
<div class="sourceCode cell-code" id="cb58"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb58-1"><a href="#cb58-1"></a>tfidfModel.evaluate(X_val_txt, y_val, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="42">
<pre><code>[0.4174775183200836, 0.8992805480957031]</code></pre>
</div>
</div>
</section>
<section id="keep-text-as-sequence-of-tokens" class="slide level2">
<h2>Keep text as sequence of tokens</h2>
<div class="cell" data-execution_count="44">
<div class="sourceCode cell-code" id="cb60"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb60-1"><a href="#cb60-1"></a>max_length <span class="op">=</span> <span class="dv">800</span> </span>
<span id="cb60-2"><a href="#cb60-2"></a>max_tokens <span class="op">=</span> <span class="dv">1_000</span></span>
<span id="cb60-3"><a href="#cb60-3"></a>vect <span class="op">=</span> layers.TextVectorization(</span>
<span id="cb60-4"><a href="#cb60-4"></a>    max_tokens<span class="op">=</span>max_tokens,</span>
<span id="cb60-5"><a href="#cb60-5"></a>    output_sequence_length<span class="op">=</span>max_length,    </span>
<span id="cb60-6"><a href="#cb60-6"></a>)</span>
<span id="cb60-7"><a href="#cb60-7"></a></span>
<span id="cb60-8"><a href="#cb60-8"></a>vect.adapt(X_train)</span>
<span id="cb60-9"><a href="#cb60-9"></a>vocab <span class="op">=</span> vect.get_vocabulary()</span>
<span id="cb60-10"><a href="#cb60-10"></a></span>
<span id="cb60-11"><a href="#cb60-11"></a>X_train_txt <span class="op">=</span> vect(X_train)</span>
<span id="cb60-12"><a href="#cb60-12"></a>X_val_txt <span class="op">=</span> vect(X_val)</span>
<span id="cb60-13"><a href="#cb60-13"></a>X_test_txt <span class="op">=</span> vect(X_test)</span>
<span id="cb60-14"><a href="#cb60-14"></a></span>
<span id="cb60-15"><a href="#cb60-15"></a><span class="bu">print</span>(vocab[:<span class="dv">50</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>['', '[UNK]', 'the', 'was', 'a', 'to', 'of', 'and', 'vehicle', 'v1', 'in', 'driver', 'for', 'this', 'critical', 'v2', 'lane', 'he', 'on', 'with', 'that', 'left', 'roadway', 'coded', 'she', 'event', 'crash', 'not', 'at', 'intersection', 'traveling', 'right', 'precrash', 'one', 'as', 'from', 'two', 'were', 'by', 'had', 'reason', 'his', 'side', 'is', 'front', 'her', 'traffic', 'an', 'it', 'speed']</code></pre>
</div>
</div>
</section>
<section id="a-sequence-of-integers" class="slide level2">
<h2>A sequence of integers</h2>
<div class="cell" data-execution_count="45">
<div class="sourceCode cell-code" id="cb62"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb62-1"><a href="#cb62-1"></a>X_train_txt[<span class="dv">0</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="44">
<pre><code>&lt;tf.Tensor: shape=(800,), dtype=int64, numpy=
array([ 13,  26,  51,  10,   2, 258, 224,   6,   4, 170,  10,   2, 414,
         6,   4, 571, 978,  29,   2,  29, 575,   6,   4, 196,   1,  36,
        53, 212,  67, 240,  56,  16,  22, 874,  38,  47, 188,   1,  36,
        53, 212,  67, 240,  56,  16,  22, 183,  38,   4, 683,   1,  46,
       242,   2, 158, 196,  22,   3, 112,   7,  77,  19,   4, 619, 447,
       556,   2,  90,  49,   3, 211,  65, 190,  57,   2,  46, 248,   3,
       404,   7,  60,  37,  52, 177, 256,  86,  28,   2,  62,   6,   2,
        26,   9,   4, 406, 977,   1,   1,   3,  70,  28,   2,  29,  99,
       123,  10,  16,  95, 316,  12,   2, 242,   5, 427, 274,  48, 159,
        56,  21,  15,   4, 314, 346,   1,   3,  81,  10,  16,  36, 164,
         2, 126,  29, 195,  48, 605,   5, 330,  77,  72,   2, 110, 194,
       236,   9, 246,  83,  21,  34,   9, 197,   2,  56,  83,  11, 139,
         4, 179,   8,  19,   1, 394,   1, 164,   2,  29,  35,   2, 124,
         9,  70,  10,   2, 414,   6,   2,  29,  10,   1,   5,   2, 164,
       179,   8,  15, 173,   2,  29,   7,  71,   2,  44,   6,   9,  19,
        83,  44,  21, 251,  75,  85,  66,   5, 134,  58,  10,   2,  29,
         7,  37,  75,  73,  59,   5,  84,   2,  11,   6,   9,   4,   1,
        61, 386,   5, 118,  10, 282, 263,   1, 322, 935, 290,  12, 791,
       300, 468, 489,   7,   1,  17,   3,  18,  41, 117,   5, 684, 150,
         1,  28,   2,  62,   6,   2,  26,  17,  50,  20,  72,   2, 110,
       435,  17,  39, 454,   1,   5, 500,  41,  56,  64,  70,  27,   1,
        37,   5, 330,  72,  17, 139,   2, 179, 237, 410,  17, 345, 139,
        15, 698,   2,  29,   7,  17,   1,  12,  97,  17,   3,  27, 221,
        10,   2,  26,   2,  14,  32,  25,  12,   9,   3,  23,  13,   8,
        30,  78,   2,  16, 135,  21,  42,   6, 111,  16,   2,  14,  40,
         3,  23,  34,   4,  11,  93, 185,   1, 142,   1,   2,  89, 103,
        23,   5,   9, 291,  47,   1, 518, 576,  17, 782, 145,   1,   2,
        29,   7,  27,  70,  34, 189,  34,   2, 164, 179,   8,   1,   2,
        46,   1,   2,  11,   6,  15,   4,   1,  61,  10, 282, 263,   3,
       496,  41, 760, 551,  12,   4, 982, 318,  28,   2,  62,   6,   2,
        26,  17,   3,  18,  41, 117, 115,  35, 156,  72,   2,  26,  51,
        17,  50,  17,   3,  81,  10,  16, 100,  34,   2,  46, 110, 194,
       236,  17, 654,   2,   8,  10,   2,  21,  99, 123,  39,   1,   5,
        56,  21,   7, 146,   2,  29,  17,   1,  34,   2,   8, 351,  83,
        56,   7,  92,  10,   2, 414,   6,   2,  29,  17, 510,  64, 159,
        27, 149,   9,  17, 162, 139,   2, 179,   8,  19,  83, 394,   7,
         1,  18, 116,  17, 173,   2,  29,  17, 595, 334, 122,   7,   3,
       168,   5, 118, 954, 180,  28,   4, 650,   1,   2,  14,  32,  25,
        12,  15,   3,  23,  54,   8, 132,  35, 186,  67,  78,  21,  16,
       135,   2,  14,  40,   3,  27,  23,   5,  13,   8,   2,  89, 103,
        23,   5,  13,   8, 291, 218, 231,  17, 209,   5, 138,   2, 164,
       179,   8,  34, 189,  34,   4, 509, 483,  17, 539,   2,  54,  11,
       281,  56, 335, 672,   2,   1,  10,   2, 164,   2,  29,   1,  34,
         4, 333,   7,  50,  20,   2,  79,   8,  39,   2,   1,   5, 239,
        41,  56,  64,  70,   1,  72,  17, 452,  41, 179,   8, 164,  17,
       454,  62,   5, 500,  41,  56, 108,  41,   8, 821,   2,  29,  64,
        70,   1,  17, 109,  50,  20,   2,  81,   8,  76,  27,   1,   2,
       179,   8, 302, 116, 134,  58,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,
         0,   0,   0,   0,   0,   0,   0])&gt;</code></pre>
</div>
</div>
</section>
<section id="feed-lstm-a-sequence-of-one-hots" class="slide level2">
<h2>Feed LSTM a sequence of one-hots</h2>
<div class="cell" data-execution_count="46">
<div class="sourceCode cell-code" id="cb64"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb64-1"><a href="#cb64-1"></a>tf.random.set_seed(<span class="dv">42</span>)</span>
<span id="cb64-2"><a href="#cb64-2"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(max_length,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb64-3"><a href="#cb64-3"></a>onehot <span class="op">=</span> tf.one_hot(inputs, depth<span class="op">=</span>max_tokens)</span>
<span id="cb64-4"><a href="#cb64-4"></a>x <span class="op">=</span> layers.Bidirectional(layers.LSTM(<span class="dv">32</span>))(onehot)</span>
<span id="cb64-5"><a href="#cb64-5"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x) </span>
<span id="cb64-6"><a href="#cb64-6"></a>outputs <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)(x)    </span>
<span id="cb64-7"><a href="#cb64-7"></a>oneHotModel <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb64-8"><a href="#cb64-8"></a>oneHotModel.<span class="bu">compile</span>(optimizer<span class="op">=</span><span class="st">"rmsprop"</span>,</span>
<span id="cb64-9"><a href="#cb64-9"></a>    loss<span class="op">=</span><span class="st">"binary_crossentropy"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb64-10"><a href="#cb64-10"></a>oneHotModel.summary(print_fn<span class="op">=</span>skip_empty)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
input_7 (InputLayer)        [(None, 800)]             0
tf.one_hot (TFOpLambda)     (None, 800, 1000)         0
bidirectional (Bidirectiona  (None, 64)               264448
l)
dropout (Dropout)           (None, 64)                0
dense_2 (Dense)             (None, 1)                 65
=================================================================
Total params: 264,513
Trainable params: 264,513
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
</section>
<section id="fit-evaluate-1" class="slide level2">
<h2>Fit &amp; evaluate</h2>
<div class="cell" data-execution_count="47">
<div class="sourceCode cell-code" id="cb66"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb66-1"><a href="#cb66-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">3</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb66-2"><a href="#cb66-2"></a>    monitor<span class="op">=</span><span class="st">"val_accuracy"</span>, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb66-3"><a href="#cb66-3"></a></span>
<span id="cb66-4"><a href="#cb66-4"></a><span class="cf">if</span> <span class="kw">not</span> Path(<span class="st">"oneHotModel.h5"</span>).exists():</span>
<span id="cb66-5"><a href="#cb66-5"></a>    oneHotModel.fit(X_train_txt, y_train, epochs<span class="op">=</span><span class="dv">1_000</span>, callbacks<span class="op">=</span>es,</span>
<span id="cb66-6"><a href="#cb66-6"></a>        validation_data<span class="op">=</span>(X_val_txt, y_val), verbose<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb66-7"><a href="#cb66-7"></a>    oneHotModel.save(<span class="st">"oneHotModel.h5"</span>)</span>
<span id="cb66-8"><a href="#cb66-8"></a><span class="cf">else</span>:</span>
<span id="cb66-9"><a href="#cb66-9"></a>    oneHotModel <span class="op">=</span> keras.models.load_model(<span class="st">"oneHotModel.h5"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="48">
<div class="sourceCode cell-code" id="cb67"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb67-1"><a href="#cb67-1"></a>oneHotModel.evaluate(X_train_txt, y_train, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="47">
<pre><code>[0.32058975100517273, 0.8882225751876831]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="49">
<div class="sourceCode cell-code" id="cb69"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb69-1"><a href="#cb69-1"></a>oneHotModel.evaluate(X_val_txt, y_val, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="48">
<pre><code>[0.3253844976425171, 0.8870503306388855]</code></pre>
</div>
</div>
</section>
<section id="custom-embeddings" class="slide level2">
<h2>Custom embeddings</h2>
<div class="cell" data-execution_count="50">
<div class="sourceCode cell-code" id="cb71"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb71-1"><a href="#cb71-1"></a>inputs <span class="op">=</span> keras.Input(shape<span class="op">=</span>(max_length,), dtype<span class="op">=</span><span class="st">"int64"</span>)</span>
<span id="cb71-2"><a href="#cb71-2"></a>embedded <span class="op">=</span> layers.Embedding(input_dim<span class="op">=</span>max_tokens, output_dim<span class="op">=</span><span class="dv">32</span>,</span>
<span id="cb71-3"><a href="#cb71-3"></a>        mask_zero<span class="op">=</span><span class="va">True</span>)(inputs)</span>
<span id="cb71-4"><a href="#cb71-4"></a>x <span class="op">=</span> layers.Bidirectional(layers.LSTM(<span class="dv">32</span>))(embedded)</span>
<span id="cb71-5"><a href="#cb71-5"></a>x <span class="op">=</span> layers.Dropout(<span class="fl">0.5</span>)(x)</span>
<span id="cb71-6"><a href="#cb71-6"></a>outputs <span class="op">=</span> layers.Dense(<span class="dv">1</span>, activation<span class="op">=</span><span class="st">"sigmoid"</span>)(x)</span>
<span id="cb71-7"><a href="#cb71-7"></a>embedLSTM <span class="op">=</span> keras.Model(inputs, outputs)</span>
<span id="cb71-8"><a href="#cb71-8"></a>embedLSTM.<span class="bu">compile</span>(<span class="st">"rmsprop"</span>, <span class="st">"binary_crossentropy"</span>, metrics<span class="op">=</span>[<span class="st">"accuracy"</span>])</span>
<span id="cb71-9"><a href="#cb71-9"></a>embedLSTM.summary(print_fn<span class="op">=</span>skip_empty)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "model_1"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
input_8 (InputLayer)        [(None, 800)]             0
embedding (Embedding)       (None, 800, 32)           32000
bidirectional_1 (Bidirectio  (None, 64)               16640
nal)
dropout_1 (Dropout)         (None, 64)                0
dense_3 (Dense)             (None, 1)                 65
=================================================================
Total params: 48,705
Trainable params: 48,705
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
</section>
<section id="fit-evaluate-2" class="slide level2">
<h2>Fit &amp; evaluate</h2>
<div class="cell" data-execution_count="51">
<div class="sourceCode cell-code" id="cb73"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb73-1"><a href="#cb73-1"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(patience<span class="op">=</span><span class="dv">3</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>,</span>
<span id="cb73-2"><a href="#cb73-2"></a>    monitor<span class="op">=</span><span class="st">"val_accuracy"</span>, verbose<span class="op">=</span><span class="dv">2</span>)</span>
<span id="cb73-3"><a href="#cb73-3"></a></span>
<span id="cb73-4"><a href="#cb73-4"></a><span class="cf">if</span> <span class="kw">not</span> Path(<span class="st">"embedLSTM.h5"</span>).exists():</span>
<span id="cb73-5"><a href="#cb73-5"></a>    embedLSTM.fit(X_train_txt, y_train, epochs<span class="op">=</span><span class="dv">1_000</span>, callbacks<span class="op">=</span>es,</span>
<span id="cb73-6"><a href="#cb73-6"></a>        validation_data<span class="op">=</span>(X_val_txt, y_val), verbose<span class="op">=</span><span class="dv">0</span>)<span class="op">;</span></span>
<span id="cb73-7"><a href="#cb73-7"></a>    embedLSTM.save(<span class="st">"embedLSTM.h5"</span>)</span>
<span id="cb73-8"><a href="#cb73-8"></a><span class="cf">else</span>:</span>
<span id="cb73-9"><a href="#cb73-9"></a>    embedLSTM <span class="op">=</span> keras.models.load_model(<span class="st">"embedLSTM.h5"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="cell" data-execution_count="52">
<div class="sourceCode cell-code" id="cb74"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb74-1"><a href="#cb74-1"></a>embedLSTM.evaluate(X_train_txt, y_train, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="51">
<pre><code>[0.26118603348731995, 0.9143679738044739]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="53">
<div class="sourceCode cell-code" id="cb76"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb76-1"><a href="#cb76-1"></a>embedLSTM.evaluate(X_val_txt, y_val, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="52">
<pre><code>[0.24710741639137268, 0.9201439023017883]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="54">
<div class="sourceCode cell-code" id="cb78"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb78-1"><a href="#cb78-1"></a>embedLSTM.evaluate(X_test_txt, y_test, verbose<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="53">
<pre><code>[0.24779456853866577, 0.9187050461769104]</code></pre>
</div>
</div>
</section></section>
<section>
<section id="word-embeddings" class="title-slide slide level1 center" data-background-image="unsw-yellow-shape.png" data-visibility="uncounted">
<h1>Word Embeddings</h1>

</section>
<section id="overview" class="slide level2">
<h2>Overview</h2>
<aside class="notes">
<p>In order for deep learning models to process language, we need to supply that language to the model in a way it can digest, i.e.&nbsp;a <strong>quantitative representation</strong> such as a 2-D matrix of numerical values.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="columns">
<div class="column" style="width:60%;">
<p>Popular methods for converting text into numbers include:</p>
<ul>
<li>One-hot encoding</li>
<li>Bag of words</li>
<li>TF-IDF</li>
<li>Word vectors (<em>transfer learning</em>)</li>
</ul>
</div><div class="column" style="width:40%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="xkcd-assigning_numbers_2x.png"></p>
<p></p><figcaption>Assigning Numbers</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Source: Randall Munroe (2022), <a href="https://xkcd.com/2610/">xkcd #2610: Assigning Numbers</a>.</p>
</div>
</section>
<section id="word-vectors" class="slide level2">
<h2>Word Vectors</h2>
<ul>
<li>One-hot representations capture word ‘existence’ only, whereas word vectors capture information about word meaning as well as location.</li>
<li>This enables deep learning NLP models to automatically learn linguistic features.</li>
<li><strong>Word2Vec</strong> &amp; <strong>GloVe</strong> are popular algorithms for generating word embeddings (i.e.&nbsp;word vectors).</li>
</ul>
</section>
<section id="word-vectors-1" class="slide level2">
<h2>Word Vectors</h2>

<img data-src="krohn_f02_06-blur.png" class="r-stretch quarto-figure-center"><p class="caption">Illustrative word vectors.</p><aside class="notes">
<ul>
<li>Overarching concept is to assign each word within a corpus to a particular, meaningful location within a multidimensional space called the vector space.</li>
<li>Initially each word is assigned to a random location.</li>
<li>BUT by considering the words that tend to be used around a given word within the corpus, the locations of the words shift.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="footer">
<p>Source: Krohn (2019), <em>Deep Learning Illustrated</em>, Figure 2-6 (<strong>redacted</strong>).</p>
</div>
</section>
<section id="remember-this-diagram" class="slide level2">
<h2>Remember this diagram?</h2>

<img data-src="Geron-mls2_1304-blur.png" class="r-stretch quarto-figure-center"><p class="caption">Embeddings will gradually improve during training.</p><div class="footer">
<p>Source: Aurélien Géron (2019), <em>Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow</em>, 2nd Edition, Figure 13-4 (<strong>redacted</strong>).</p>
</div>
</section>
<section id="word2vec" class="slide level2">
<h2>Word2Vec</h2>
<p><strong>Key idea</strong>: You’re known by the company you keep.</p>
<p>Two algorithms are used to calculate embeddings:</p>
<ul>
<li><em>Continuous bag of words</em>: uses the context words to predict the target word</li>
<li><em>Skip-gram</em>: uses the target word to predict the context words</li>
</ul>
<p>Predictions are made using a neural network with one hidden layer. Through backpropagation, we update a set of “weights” which become the word vectors.</p>
<div class="footer">
<p>Paper: Mikolov et al.&nbsp;(2013), <a href="https://arxiv.org/pdf/1301.3781.pdf"><em>Efficient estimation of word representations in vector space</em></a>, arXiv:1301.3781.</p>
</div>
</section>
<section id="word2vec-training-methods" class="slide level2">
<h2>Word2Vec training methods</h2>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="Chaudhary-nlp-ssl-center-word-prediction.gif"></p>
<p></p><figcaption>Continuous bag of words is a <em>center word prediction</em> task</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="Chaudhary-nlp-ssl-neighbor-word-prediction.gif"></p>
<p></p><figcaption>Skip-gram is a <em>neighbour word prediction</em> task</figcaption><p></p>
</figure>
</div>
<div class="callout callout-tip callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Suggested viewing</strong></p>
</div>
<div class="callout-content">
<p>Computerphile (2019), <a href="https://youtu.be/gQddtTdmG_8">Vectoring Words (Word Embeddings)</a>, YouTube (16 mins).</p>
</div>
</div>
</div>
<div class="footer">
<p>Source: Amit Chaudhary (2020), <a href="https://amitness.com/2020/05/self-supervised-learning-nlp/">Self Supervised Representation Learning in NLP</a>.</p>
</div>
</section>
<section id="the-skip-gram-network" class="slide level2">
<h2>The skip-gram network</h2>

<img data-src="lilianweng-word2vec-skip-gram.png" class="r-stretch quarto-figure-center"><p class="caption">The skip-gram model. Both the input vector <span class="math inline">\(\boldsymbol{x}\)</span> and the output <span class="math inline">\(\boldsymbol{y}\)</span> are one-hot encoded word representations. The hidden layer is the word embedding of size <span class="math inline">\(N\)</span>.</p><div class="footer">
<p>Source: Lilian Weng (2017), <a href="https://lilianweng.github.io/posts/2017-10-15-word-embedding/">Learning Word Embedding</a>, Blog post, Figure 1.</p>
</div>
</section>
<section id="glove" class="slide level2 smaller">
<h2>GloVe</h2>
<p>GloVe (Global Vectors for Word Representation) is an unsupervised learning algorithm for obtaining word vector representations, developed by Stanford University in 2014.</p>
<p>GloVe captures contextual information about words by comparing co-occurrence probability ratios</p>
<p><em>A co-occurrence probability</em> is the probability that word <span class="math inline">\(k\)</span> is present in the corpus if word <span class="math inline">\(j\)</span> is present.</p>

<img data-src="co-occurrence.png" class="r-stretch quarto-figure-center"><p class="caption">Example co-occurrence probabilities</p><div class="footer">
<p>Source: Pennington et al.&nbsp;(2014), <a href="https://nlp.stanford.edu/projects/glove/">GloVe: Global Vectors for Word Representation</a>, Project webpage.</p>
</div>
</section>
<section id="word-vector-arithmetic" class="slide level2">
<h2>Word Vector Arithmetic</h2>
<div class="columns">
<div class="column">
<p>Relationships between words becomes vector math.</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="vectors-Figure_03_02_09.jpeg"></p>
<p></p><figcaption>You remember vectors, right?</figcaption><p></p>
</figure>
</div>
<aside class="notes">
<ul>
<li>E.g., if we calculate the direction and distance between the coordinates of the words <em>Paris</em> and <em>France</em>, and trace this direction and distance from <em>London</em>, we should be close to the word <em>England</em>.</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</div><div class="column">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="krohn_f02_07-blur.png"></p>
<p></p><figcaption>Illustrative word vector arithmetic</figcaption><p></p>
</figure>
</div>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="krohn_f02_08-blur.png"></p>
<p></p><figcaption>Screenshot from <a href="https://lamyiowce.github.io/word2viz/">Word2viz</a></figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Sources: PressBooks, <a href="https://pressbooks.bccampus.ca/collegephysics/chapter/vector-addition-and-subtraction-graphical-methods/">College Physics: OpenStax</a>, Chapter 17 Figure 9, and Krohn (2019), <em>Deep Learning Illustrated</em>, Figures 2-7 &amp; 2-8 (<strong>redacted</strong>).</p>
</div>
</section></section>
<section>
<section id="section" class="title-slide slide level1 center">
<h1></h1>
<h2>
Pretrained word embeddings
</h2>
<p>Install <code>gensim</code> library:</p>
<div class="cell" data-execution_count="55">
<div class="sourceCode cell-code" id="cb80"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb80-1"><a href="#cb80-1"></a><span class="op">!</span>pip install gensim <span class="op">&gt;</span> <span class="op">/</span>dev<span class="op">/</span>null</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>Load word2vec embeddings trained on Google News:</p>
<div class="cell" data-execution_count="56">
<div class="sourceCode cell-code" id="cb81"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb81-1"><a href="#cb81-1"></a><span class="im">import</span> gensim.downloader <span class="im">as</span> api</span>
<span id="cb81-2"><a href="#cb81-2"></a>wv <span class="op">=</span> api.load(<span class="st">'word2vec-google-news-300'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>When run for the first time, that downloads a huge file:</p>
<div class="cell" data-execution_count="57">
<div class="sourceCode cell-code" id="cb82"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb82-1"><a href="#cb82-1"></a><span class="op">!</span>ls <span class="op">~/</span>gensim<span class="op">-</span>data<span class="op">/</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>information.json         word2vec-google-news-300</code></pre>
</div>
</div>
<div class="cell" data-execution_count="58">
<div class="sourceCode cell-code" id="cb84"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb84-1"><a href="#cb84-1"></a><span class="op">!</span>ls <span class="op">-</span>lh <span class="op">~/</span>gensim<span class="op">-</span>data<span class="op">/</span>word2vec<span class="op">-</span>google<span class="op">-</span>news<span class="op">-</span><span class="dv">300</span><span class="op">/</span>word2vec<span class="op">-</span>google<span class="op">-</span>news<span class="op">-</span><span class="fl">300.</span><span class="er">gz</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>-rw-r--r--  1 plaub  staff   1.6G 21 Jul 10:47 /Users/plaub/gensim-data/word2vec-google-news-300/word2vec-google-news-300.gz</code></pre>
</div>
</div>
<div class="cell" data-execution_count="59">
<div class="sourceCode cell-code" id="cb86"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb86-1"><a href="#cb86-1"></a><span class="bu">len</span>(wv)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="58">
<pre><code>3000000</code></pre>
</div>
</div>
</section>
<section id="treat-wv-like-a-dictionary" class="slide level2">
<h2>Treat <code>wv</code> like a dictionary</h2>
<div class="cell" data-execution_count="60">
<div class="sourceCode cell-code" id="cb88"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb88-1"><a href="#cb88-1"></a>wv[<span class="st">"pizza"</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="59">
<pre><code>array([-1.26e-01,  2.54e-02,  1.67e-01,  5.51e-01, -7.67e-02,  1.29e-01,
        1.03e-01, -3.95e-04,  1.22e-01,  4.32e-02,  1.73e-01, -6.84e-02,
        3.42e-01,  8.40e-02,  6.69e-02,  2.68e-01, -3.71e-02, -5.57e-02,
        1.81e-01,  1.90e-02, -5.08e-02,  9.03e-03,  1.77e-01,  6.49e-02,
       -6.25e-02, -9.42e-02, -9.72e-02,  4.00e-01,  1.15e-01,  1.03e-01,
       -1.87e-02, -2.70e-01,  1.81e-01,  1.25e-01, -3.17e-02, -5.49e-02,
        3.46e-01, -1.57e-02,  1.82e-05,  2.07e-01, -1.26e-01, -2.83e-01,
        2.00e-01,  8.35e-02, -4.74e-02, -3.11e-02, -2.62e-01,  1.70e-01,
       -2.03e-02,  1.53e-01, -1.21e-01,  3.75e-01, -5.69e-02, -4.76e-03,
       -1.95e-01, -2.03e-01,  3.01e-01, -1.01e-01, -3.18e-01, -9.03e-02,
       -1.19e-01,  1.95e-01, -8.79e-02,  1.58e-01,  1.52e-02, -1.60e-01,
       -3.30e-01, -4.67e-01,  1.69e-01,  2.23e-02,  1.55e-01,  1.08e-01,
       -3.56e-02,  9.13e-02, -8.69e-02, -1.20e-01, -3.09e-01, -2.61e-02,
       -7.23e-02, -4.80e-01,  3.78e-02, -1.36e-01, -1.03e-01, -2.91e-01,
       -1.93e-01, -4.22e-01, -1.06e-01,  3.55e-01,  1.67e-01, -3.63e-03,
       -7.42e-02, -3.22e-01, -7.52e-02, -8.25e-02, -2.91e-01, -1.26e-01,
        1.68e-02,  5.00e-02,  1.28e-01, -7.42e-02, -1.31e-01, -2.46e-01,
        6.49e-02,  1.53e-01,  2.60e-01, -1.05e-01,  3.57e-01, -4.30e-02,
       -1.58e-01,  8.20e-02, -5.98e-02, -2.34e-01, -3.22e-01, -1.26e-01,
        5.40e-02, -1.88e-01,  1.36e-01, -6.59e-02,  8.36e-03, -1.85e-01,
       -2.97e-01, -1.85e-01, -4.74e-02, -1.06e-01, -6.93e-02,  3.83e-02,
       -3.20e-02,  3.64e-02, -1.20e-01,  1.77e-01, -1.16e-01,  1.99e-02,
        8.64e-02,  6.08e-02, -1.41e-01,  3.30e-01,  1.94e-01, -1.56e-01,
        3.93e-01,  1.81e-03,  7.28e-02, -2.54e-01, -3.54e-02,  2.87e-03,
       -1.73e-01,  9.77e-03, -1.56e-02,  3.23e-03, -1.70e-01,  1.55e-01,
        7.18e-02,  4.10e-01, -2.11e-01,  1.32e-01,  7.63e-03,  4.79e-02,
       -4.54e-02,  7.32e-02, -4.06e-01, -2.06e-02, -4.04e-01, -1.01e-01,
       -2.03e-01,  1.55e-01, -1.89e-01,  6.59e-02,  6.54e-02, -2.05e-01,
        5.47e-02, -3.06e-02, -1.54e-01, -2.62e-01,  3.81e-03, -8.20e-02,
       -3.20e-01,  2.84e-02,  2.70e-01,  1.74e-01, -1.67e-01,  2.23e-01,
        6.35e-02, -1.96e-01,  1.46e-01, -1.56e-02,  2.60e-02, -6.30e-02,
        2.94e-02,  3.28e-01, -4.69e-02, -1.52e-01,  6.98e-02,  3.18e-01,
       -1.08e-01,  3.66e-02, -1.99e-01,  1.64e-03,  6.41e-03, -1.47e-01,
       -6.25e-02, -4.36e-03, -2.75e-01,  8.54e-02, -5.00e-02, -3.12e-01,
       -1.34e-01, -1.99e-01,  5.18e-02, -9.28e-02, -2.40e-01, -7.86e-02,
       -1.54e-01, -6.64e-02, -1.97e-01,  1.77e-01, -1.57e-01, -1.63e-01,
        6.01e-02, -5.86e-02, -2.23e-01, -6.59e-02, -9.38e-02, -4.14e-01,
        2.56e-01, -1.77e-01,  2.52e-01,  1.48e-01, -1.04e-01, -8.61e-03,
       -1.23e-01, -9.23e-02,  4.42e-02, -1.71e-01, -1.98e-01,  1.92e-01,
        2.85e-01, -4.35e-02,  1.08e-01, -5.37e-02, -2.10e-02,  1.46e-01,
        3.83e-01,  2.32e-02, -8.84e-02,  7.32e-02, -1.01e-01, -1.06e-01,
        4.12e-01,  2.11e-01,  2.79e-01, -2.09e-02,  2.07e-01,  9.81e-02,
        2.39e-01,  7.67e-02,  2.02e-01, -6.08e-02, -2.64e-03, -1.84e-01,
       -1.57e-02, -3.20e-01,  9.03e-02,  1.02e-01, -4.96e-01, -9.72e-02,
       -8.11e-02, -1.81e-01, -1.46e-01,  8.64e-02, -2.04e-01, -2.02e-01,
       -5.47e-02,  2.54e-01,  2.09e-02, -1.16e-01,  2.02e-01, -8.06e-02,
       -1.05e-01, -7.96e-02,  1.97e-02, -2.49e-01,  1.31e-01,  2.89e-01,
       -2.26e-01,  4.55e-01, -2.73e-01, -2.58e-01, -3.15e-02,  4.04e-01,
       -2.68e-01,  2.89e-01, -1.84e-01, -1.48e-01, -1.07e-01,  1.28e-01,
        5.47e-01, -8.69e-02, -1.48e-02,  6.98e-02, -8.50e-02, -1.55e-01],
      dtype=float32)</code></pre>
</div>
</div>
<div class="cell" data-execution_count="61">
<div class="sourceCode cell-code" id="cb90"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb90-1"><a href="#cb90-1"></a><span class="bu">len</span>(wv[<span class="st">"pizza"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="60">
<pre><code>300</code></pre>
</div>
</div>
</section>
<section id="find-nearby-word-vectors" class="slide level2">
<h2>Find nearby word vectors</h2>
<div class="cell" data-execution_count="62">
<div class="sourceCode cell-code" id="cb92"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb92-1"><a href="#cb92-1"></a>wv.most_similar(<span class="st">"Python"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>[('Jython', 0.6152505874633789),
 ('Perl_Python', 0.5710949897766113),
 ('IronPython', 0.5704679489135742),
 ('scripting_languages', 0.5695091485977173),
 ('PHP_Perl', 0.5687724947929382),
 ('Java_Python', 0.5681070685386658),
 ('PHP', 0.5660915374755859),
 ('Python_Ruby', 0.5632461905479431),
 ('Visual_Basic', 0.5603479743003845),
 ('Perl', 0.5530891418457031)]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="63">
<div class="sourceCode cell-code" id="cb94"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb94-1"><a href="#cb94-1"></a>wv.similarity(<span class="st">"Python"</span>, <span class="st">"Java"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="62">
<pre><code>0.46189713</code></pre>
</div>
</div>
<div class="cell" data-execution_count="64">
<div class="sourceCode cell-code" id="cb96"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb96-1"><a href="#cb96-1"></a>wv.similarity(<span class="st">"Python"</span>, <span class="st">"sport"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="63">
<pre><code>0.08406469</code></pre>
</div>
</div>
<div class="cell" data-execution_count="65">
<div class="sourceCode cell-code" id="cb98"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb98-1"><a href="#cb98-1"></a>wv.similarity(<span class="st">"Python"</span>, <span class="st">"R"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="64">
<pre><code>0.06695429</code></pre>
</div>
</div>
<div class="footer">
<p>Fun fact: Gensim’s <code>most_similar</code> uses Spotify’s <code>annoy</code> library (“Approximate Nearest Neighbors Oh Yeah”)</p>
</div>
</section>
<section id="what-does-similarity-mean" class="slide level2">
<h2>What does ‘similarity’ mean?</h2>
<p>The ‘similarity’ scores</p>
<div class="cell" data-execution_count="66">
<div class="sourceCode cell-code" id="cb100"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb100-1"><a href="#cb100-1"></a>wv.similarity(<span class="st">"Sydney"</span>, <span class="st">"Melbourne"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="65">
<pre><code>0.8613987</code></pre>
</div>
</div>
<p>are normally based on cosine distance.</p>
<div class="cell" data-execution_count="67">
<div class="sourceCode cell-code" id="cb102"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb102-1"><a href="#cb102-1"></a>x <span class="op">=</span> wv[<span class="st">"Sydney"</span>]</span>
<span id="cb102-2"><a href="#cb102-2"></a>y <span class="op">=</span> wv[<span class="st">"Melbourne"</span>]</span>
<span id="cb102-3"><a href="#cb102-3"></a>x.dot(y) <span class="op">/</span> (np.linalg.norm(x) <span class="op">*</span> np.linalg.norm(y))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="66">
<pre><code>0.8613986</code></pre>
</div>
</div>
<div class="cell" data-execution_count="68">
<div class="sourceCode cell-code" id="cb104"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb104-1"><a href="#cb104-1"></a>wv.similarity(<span class="st">"Sydney"</span>, <span class="st">"Aarhus"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="67">
<pre><code>0.19079602</code></pre>
</div>
</div>
</section>
<section id="wengs-got-word2vec" class="slide level2">
<h2>Weng’s GoT Word2Vec</h2>
<p>In the GoT word embedding space, the top similar words to “king” and “queen” are:</p>
<div class="columns">
<div class="column">
<div class="sourceCode" id="cb106"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb106-1"><a href="#cb106-1"></a>model.most_similar(<span class="st">'king'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>('kings', 0.897245) 
('baratheon', 0.809675) 
('son', 0.763614)
('robert', 0.708522)
('lords', 0.698684)
('joffrey', 0.696455)
('prince', 0.695699)
('brother', 0.685239)
('aerys', 0.684527)
('stannis', 0.682932)</code></pre>
</div><div class="column">
<div class="sourceCode" id="cb108"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb108-1"><a href="#cb108-1"></a>model.most_similar(<span class="st">'queen'</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<pre><code>('cersei', 0.942618)
('joffrey', 0.933756)
('margaery', 0.931099)
('sister', 0.928902)
('prince', 0.927364)
('uncle', 0.922507)
('varys', 0.918421)
('ned', 0.917492)
('melisandre', 0.915403)
('robb', 0.915272)</code></pre>
</div>
</div>
<div class="footer">
<p>Source: Lilian Weng (2017), <a href="https://lilianweng.github.io/posts/2017-10-15-word-embedding/">Learning Word Embedding</a>, Blog post.</p>
</div>
</section>
<section id="combining-word-vectors" class="slide level2">
<h2>Combining word vectors</h2>
<p>You can summarise a sentence by averaging the individual word vectors.</p>
<div class="cell" data-execution_count="69">
<div class="sourceCode cell-code" id="cb110"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb110-1"><a href="#cb110-1"></a>sv <span class="op">=</span> (wv[<span class="st">"Melbourne"</span>] <span class="op">+</span> wv[<span class="st">"has"</span>] <span class="op">+</span> wv[<span class="st">"better"</span>] <span class="op">+</span> wv[<span class="st">"coffee"</span>]) <span class="op">/</span> <span class="dv">4</span></span>
<span id="cb110-2"><a href="#cb110-2"></a><span class="bu">len</span>(sv), sv[:<span class="dv">5</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="68">
<pre><code>(300, array([-0.08, -0.11, -0.16,  0.24,  0.06], dtype=float32))</code></pre>
</div>
</div>
<blockquote>
<p>As it turns out, averaging word embeddings is a surprisingly effective way to create word embeddings. It’s not perfect (as you’ll see), but it does a strong job of capturing what you might perceive to be complex relationships between words.</p>
</blockquote>
<div class="footer">
<p>Source: Trask (2019), Grokking Deep Learning, Chapter 12.</p>
</div>
</section>
<section id="recipe-recommender" class="slide level2">
<h2>Recipe recommender</h2>
<div class="columns">
<div class="column" style="width:49%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="duarte-o-carmo-recipe-space-1.png"></p>
<p></p><figcaption>Recipes are the average of the word vectors of the ingredients.</figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:51%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="duarte-o-carmo-recipe-space-2.png"></p>
<p></p><figcaption>Nearest neighbours used to classify new recipes as potentially delicious.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Source: Duarte O.Carmo (2022), <a href="https://duarteocarmo.com/blog/scandinavia-food-python-recommendation-systems">A recipe recommendation system</a>, Blog post.</p>
</div>
</section>
<section id="analogies-with-word-vectors" class="slide level2">
<h2>Analogies with word vectors</h2>
<p>Obama is to America as ___ is to Australia.</p>
<div class="fragment">
<p><span class="math display">\[ \text{Obama} - \text{America} + \text{Australia} = ? \]</span></p>
</div>
<div class="fragment">
<div class="cell" data-execution_count="70">
<div class="sourceCode cell-code" id="cb112"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb112-1"><a href="#cb112-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">"Obama"</span>, <span class="st">"Australia"</span>], negative<span class="op">=</span>[<span class="st">"America"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="69">
<pre><code>[('Mr_Rudd', 0.615142285823822),
 ('Prime_Minister_Julia_Gillard', 0.6045385003089905),
 ('Prime_Minister_Kevin_Rudd', 0.5982581973075867),
 ('Kevin_Rudd', 0.5627648830413818),
 ('Ms_Gillard', 0.5517690181732178),
 ('Opposition_Leader_Kevin_Rudd', 0.5298037528991699),
 ('Mr_Beazley', 0.5259249806404114),
 ('Gillard', 0.5250653028488159),
 ('NARDA_GILMORE', 0.5203536748886108),
 ('Mr_Downer', 0.5150347948074341)]</code></pre>
</div>
</div>
</div>
</section>
<section id="testing-more-associations" class="slide level2">
<h2>Testing more associations</h2>
<div class="cell" data-execution_count="71">
<div class="sourceCode cell-code" id="cb114"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb114-1"><a href="#cb114-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">"France"</span>, <span class="st">"London"</span>], negative<span class="op">=</span>[<span class="st">"Paris"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="70">
<pre><code>[('Britain', 0.7368934750556946),
 ('UK', 0.6637030839920044),
 ('England', 0.6119861602783203),
 ('United_Kingdom', 0.6067784428596497),
 ('Great_Britain', 0.5870823860168457),
 ('Britian', 0.5852951407432556),
 ('Scotland', 0.5410018563270569),
 ('British', 0.5318331718444824),
 ('Europe', 0.5307437181472778),
 ('East_Midlands', 0.5230222344398499)]</code></pre>
</div>
</div>
</section>
<section id="quickly-get-to-bad-associations" class="slide level2">
<h2>Quickly get to bad associations</h2>
<div class="cell" data-execution_count="72">
<div class="sourceCode cell-code" id="cb116"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb116-1"><a href="#cb116-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">"King"</span>, <span class="st">"woman"</span>], negative<span class="op">=</span>[<span class="st">"man"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="71">
<pre><code>[('Queen', 0.5515626072883606),
 ('Oprah_BFF_Gayle', 0.47597548365592957),
 ('Geoffrey_Rush_Exit', 0.46460166573524475),
 ('Princess', 0.4533674418926239),
 ('Yvonne_Stickney', 0.4507041573524475),
 ('L._Bonauto', 0.4422135353088379),
 ('gal_pal_Gayle', 0.4408389925956726),
 ('Alveda_C.', 0.440279096364975),
 ('Tupou_V.', 0.4373863935470581),
 ('K._Letourneau', 0.435103178024292)]</code></pre>
</div>
</div>
<div class="cell" data-execution_count="73">
<div class="sourceCode cell-code" id="cb118"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb118-1"><a href="#cb118-1"></a>wv.most_similar(positive<span class="op">=</span>[<span class="st">"computer_programmer"</span>, <span class="st">"woman"</span>], negative<span class="op">=</span>[<span class="st">"man"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="72">
<pre><code>[('homemaker', 0.5627118945121765),
 ('housewife', 0.5105047225952148),
 ('graphic_designer', 0.5051802396774292),
 ('schoolteacher', 0.49794942140579224),
 ('businesswoman', 0.493489146232605),
 ('paralegal', 0.4925510883331299),
 ('registered_nurse', 0.4907974898815155),
 ('saleswoman', 0.48816272616386414),
 ('electrical_engineer', 0.4797726571559906),
 ('mechanical_engineer', 0.4755399525165558)]</code></pre>
</div>
</div>
</section>
<section id="bias-in-nlp-models" class="slide level2 smaller">
<h2>Bias in NLP models</h2>
<div class="columns">
<div class="column">
<p><img data-src="the-verge-banner-microsoft-tay.jpeg"></p>
<p>The Verge (2016), <a href="https://www.theverge.com/2016/3/24/11297050/tay-microsoft-chatbot-racist">Twitter taught Microsoft’s AI chatbot to be a racist a****** in less than a day</a>.</p>
</div><div class="column">
<blockquote>
<p>… there are serious questions to answer, like how are we going to teach AI using public data without incorporating the worst traits of humanity? If we create bots that mirror their users, do we care if their users are human trash? There are plenty of examples of technology embodying — either accidentally or on purpose — the prejudices of society, and Tay’s adventures on Twitter show that even big corporations like Microsoft forget to take any preventative measures against these problems.</p>
</blockquote>
</div>
</div>
</section>
<section id="the-library-cheats-a-little-bit" class="slide level2">
<h2>The library cheats a little bit</h2>
<div class="cell" data-execution_count="74">
<div class="sourceCode cell-code" id="cb120"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb120-1"><a href="#cb120-1"></a>wv.similar_by_vector(wv[<span class="st">"computer_programmer"</span>]<span class="op">-</span>wv[<span class="st">"man"</span>]<span class="op">+</span>wv[<span class="st">"woman"</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display" data-execution_count="73">
<pre><code>[('computer_programmer', 0.910581111907959),
 ('homemaker', 0.5771315693855286),
 ('schoolteacher', 0.5500192046165466),
 ('graphic_designer', 0.5464698672294617),
 ('mechanical_engineer', 0.539836585521698),
 ('electrical_engineer', 0.5337055325508118),
 ('housewife', 0.5274525284767151),
 ('programmer', 0.5096209049224854),
 ('businesswoman', 0.5029540657997131),
 ('keypunch_operator', 0.4974639415740967)]</code></pre>
</div>
</div>
<p>To get the ‘nice’ analogies, the <code>.most_similar</code> ignores the input words as possible answers.</p>
<div class="cell" data-execution_count="75">
<div class="sourceCode cell-code" id="cb122"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb122-1"><a href="#cb122-1"></a><span class="co"># ignore (don't return) keys from the input</span></span>
<span id="cb122-2"><a href="#cb122-2"></a>result <span class="op">=</span> [</span>
<span id="cb122-3"><a href="#cb122-3"></a>    (<span class="va">self</span>.index_to_key[sim <span class="op">+</span> clip_start], <span class="bu">float</span>(dists[sim]))</span>
<span id="cb122-4"><a href="#cb122-4"></a>    <span class="cf">for</span> sim <span class="kw">in</span> best <span class="cf">if</span> (sim <span class="op">+</span> clip_start) <span class="kw">not</span> <span class="kw">in</span> all_keys</span>
<span id="cb122-5"><a href="#cb122-5"></a>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="footer">
<p>Source: gensim, <a href="https://github.com/RaRe-Technologies/gensim/blob/eeb7e8662d5350efe68fa14db08b02d273735af9/gensim/models/keyedvectors.py#L853">gensim/models/keyedvectors.py</a>, lines 853-857.</p>
</div>
</section></section>
<section>
<section id="story-wall" class="title-slide slide level1 center" data-background-image="unsw-yellow-shape.png" data-visibility="uncounted">
<h1>Story Wall</h1>

</section>
<section id="a-few-comments" class="slide level2">
<h2>A few comments</h2>
<ul>
<li>Expert programmers still use other people’s code</li>
<li>When going through some tutorial notebook, don’t just press ‘run cell’</li>
<li>Try changing hyperparameters (weights in a loss function in particular)</li>
<li>Try commenting out some lines of code</li>
<li>Try removing layers from a network</li>
<li>Keep expectations low when running on your own inputs</li>
</ul>
</section></section>
<section>
<section id="text-generation" class="title-slide slide level1 center" data-background-image="unsw-yellow-shape.png" data-visibility="uncounted">
<h1>Text Generation</h1>

</section>
<section id="generative-deep-learning" class="slide level2">
<h2>Generative deep learning</h2>
<ul>
<li>Using AI as augmented intelligence rather than artificial intelligence.</li>
<li>Use of deep learning to augment creative activities such as writing, music and art, to <em>generate</em> new things.</li>
<li>Some applications: text generation, deep dreaming, neural style transfer, variational autoencoders and generative adversarial networks.</li>
</ul>
</section>
<section id="text-generation-1" class="slide level2">
<h2>Text generation</h2>
<blockquote>
<p>Generating sequential data is the closest computers get to dreaming.</p>
</blockquote>
<ul>
<li>Generate sequence data: Train a model to predict the next token or next few tokens in a sentence, using previous tokens as input.</li>
<li>A network that models the probability of the next tokens given the previous ones is called a <em>language model</em>.</li>
</ul>
<aside class="notes">
<p>GPT-3 is a 175 billion parameter text-generation model trained by the startup OpenAI on a large text corpus of digitally available books, Wikipedia and web crawling. GPT-3 made headlines in 2020 due to its capability to generate plausible-sounding text paragraphs on virtually any topic.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="footer">
<p>Source: Alex Graves (2013), <a href="https://arxiv.org/abs/1308.0850">Generating Sequences With Recurrent Neural Networks</a>.</p>
</div>
</section>
<section id="word-level-language-model" class="slide level2">
<h2>Word-level language model</h2>

<img data-src="chollet-languagemodel-blur.png" class="r-stretch quarto-figure-center"><p class="caption">Diagram of a word-level language model.</p><div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Figure 12.1 (<strong>redacted</strong>).</p>
</div>
</section>
<section id="character-level-language-model" class="slide level2">
<h2>Character-level language model</h2>

<img data-src="tensorflow-text_generation_sampling.png" class="r-stretch quarto-figure-center"><p class="caption">Diagram of a character-level language model (Char-RNN)</p><div class="footer">
<p>Source: Tensorflow tutorial, <a href="https://www.tensorflow.org/text/tutorials/text_generation">Text generation with an RNN</a>.</p>
</div>
</section>
<section id="useful-for-speech-recognition" class="slide level2">
<h2>Useful for speech recognition</h2>
<div id="fig-speech-recognition" class="quarto-figure quarto-figure-center">
<figure>
<table>
<colgroup>
<col style="width: 50%">
<col style="width: 50%">
</colgroup>
<thead>
<tr class="header">
<th>RNN output</th>
<th>Decoded Transcription</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>what is the weather like in bostin right now</td>
<td>what is the weather like in boston right now</td>
</tr>
<tr class="even">
<td>prime miniter nerenr modi</td>
<td>prime minister narendra modi</td>
</tr>
<tr class="odd">
<td>arther n tickets for the game</td>
<td>are there any tickets for the game</td>
</tr>
</tbody>
</table>
<p></p><figcaption>Figure&nbsp;1: Examples of transcriptions directly from the RNN with errors that are fixed by addition of a language model.</figcaption><p></p>
</figure>
</div>
<div class="footer">
<p>Source: Hannun et al.&nbsp;(2014), <a href="https://arxiv.org/pdf/1412.5567.pdf">Deep Speech: Scaling up end-to-end speech recognition</a>, arXiv:1412.5567, Table 1.</p>
</div>
</section>
<section id="generating-shakespeare-i" class="slide level2">
<h2>Generating Shakespeare I</h2>
<blockquote>
<p>ROMEO:<br>
Why, sir, what think you, sir?<br>
<br>
AUTOLYCUS:<br>
A dozen; shall I be deceased.<br>
The enemy is parting with your general,<br>
As bias should still combit them offend<br>
That Montague is as devotions that did satisfied;<br>
But not they are put your pleasure.</p>
</blockquote>
<div class="footer">
<p>Source: Tensorflow tutorial, <a href="https://www.tensorflow.org/text/tutorials/text_generation">Text generation with an RNN</a>.</p>
</div>
</section>
<section id="generating-shakespeare-ii" class="slide level2">
<h2>Generating Shakespeare II</h2>
<blockquote>
<p>DUKE OF YORK:<br>
Peace, sing! do you must be all the law;<br>
And overmuting Mercutio slain;<br>
And stand betide that blows which wretched shame;<br>
Which, I, that have been complaints me older hours.<br>
<br>
LUCENTIO:<br>
What, marry, may shame, the forish priest-lay estimest you, sir,<br>
Whom I will purchase with green limits o’ the commons’ ears!</p>
</blockquote>
<div class="footer">
<p>Source: Tensorflow tutorial, <a href="https://www.tensorflow.org/text/tutorials/text_generation">Text generation with an RNN</a>.</p>
</div>
</section>
<section id="generating-shakespeare-iii" class="slide level2">
<h2>Generating Shakespeare III</h2>
<blockquote>
<p>ANTIGONUS:<br>
To be by oath enjoin’d to this. Farewell!<br>
The day frowns more and more: thou’rt like to have<br>
A lullaby too rough: I never saw<br>
The heavens so dim by day. A savage clamour!<br>
<br>
[Exit, pursued by a bear]</p>
</blockquote>
<div class="footer">
<p>Source: Tensorflow tutorial, <a href="https://www.tensorflow.org/text/tutorials/text_generation">Text generation with an RNN</a>.</p>
</div>
</section>
<section id="sampling-strategy" class="slide level2">
<h2>Sampling strategy</h2>
<ul>
<li><em>Greedy sampling</em> will choose the token with the highest probability. It makes the resulting sentence repetitive and predictable.</li>
<li><em>Stochastic sampling</em>: if a word has probability 0.3 of being next in the sentence according to the model, we’ll choose it 30% of the time. But the result is still not interesting enough and still quite predictable.</li>
<li>Use a <em>softmax temperature</em> to control the randomness. More randomness results in more surprising and creative sentences.</li>
</ul>
</section>
<section id="generating-laub-temp-0.01" class="slide level2">
<h2>Generating Laub (temp = 0.01)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> be different situation. So, next one is what they rective that each commit to be able to learn some relationships from the course, and that is part of the image that it’s very clese and black problems that you’re trying to fit the neural network to do there instead of like a specific though shef series of layers mean about full of the chosen the baseline of car was in the right, but that’s an important facts and it’s a very small summary with very scrort by the beginning of the sentence.</p>
</blockquote>
</section>
<section id="generating-laub-temp-0.25" class="slide level2">
<h2>Generating Laub (temp = 0.25)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> decreas before model that we that we have to think about it, this mightsks better, for chattely the same project, because you might use the test set because it’s to be picked up the things that I wanted to heard of things that I like that even real you and you’re using the same thing again now because we need to understand what it’s doing the same thing but instead of putting it in particular week, and we can say that’s a thing I mainly link it’s three columns.</p>
</blockquote>
</section>
<section id="generating-laub-temp-0.5" class="slide level2">
<h2>Generating Laub (temp = 0.5)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> probably the adw n wait lots of ngobs teulagedation to calculate the gradient and then I’ll be less than one layer the next slide will br input over and over the threshow you ampaigey the one that we want to apply them quickly. So, here this is the screen here the main top kecw onct three thing to told them, and the output is a vertical variables and Marceparase of things that you’re moving the blurring and that just data set is to maybe kind of categorical variants here but there’s more efficiently not basically replace that with respect to the best and be the same thing.</p>
</blockquote>
</section>
<section id="generating-laub-temp-1" class="slide level2">
<h2>Generating Laub (temp = 1)</h2>
<blockquote>
<p><em>In today’s lecture we will</em> put it different shates to touch on last week, so I want to ask what are you object frod current. They don’t have any zero into it, things like that which mistakes. 10 claims that the average version was relden distever ditgs and Python for the whole term wo long right to really. The name of these two options. There are in that seems to be modified version. If you look at when you’re putting numbers into your, that that’s over. And I went backwards, up, if they’rina functional pricing working with.</p>
</blockquote>
</section>
<section id="generating-laub-temp-1.5" class="slide level2">
<h2>Generating Laub (temp = 1.5)</h2>
<blockquote>
<p>In today’s lecture we will put it could be bedinnth. Lowerstoriage nruron. So rochain the everything that I just sGiming. If there was a large. It’s gonua draltionation. Tow many, up, would that black and 53% that’s girter thankAty will get you jast typically stickK thing. But maybe. Anyway, I’m going to work on this libry two, past, at shit citcs jast pleming to memorize overcamples like pre pysing, why wareed to smart a one in this reportbryeccuriay.</p>
</blockquote>
</section>
<section id="generate-the-most-likely-sequence" class="slide level2">
<h2>Generate the most likely sequence</h2>

<img data-src="chatbot.png" class="r-stretch quarto-figure-center"><p class="caption">An example sequence-to-sequence chatbot model.</p><div class="footer">
<p>Source: Payne (2021), <a href="https://www.width.ai/post/what-is-beam-search">What is beam search</a>, Width.ai blog.</p>
</div>
</section>
<section id="beam-search" class="slide level2">
<h2>Beam search</h2>

<img data-src="beam-search.png" class="r-stretch quarto-figure-center"><p class="caption">Illustration of a beam search.</p><div class="footer">
<p>Source: Doshi (2021), <a href="https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24">Foundations of NLP Explained Visually: Beam Search, How It Works</a>, towardsdatascience.com.</p>
</div>
</section></section>
<section>
<section id="image-generation" class="title-slide slide level1 center" data-background-image="unsw-yellow-shape.png" data-visibility="uncounted">
<h1>Image Generation</h1>

</section>
<section id="reverse-engineering-a-cnn" class="slide level2">
<h2>Reverse-engineering a CNN</h2>
<p>A CNN is a function <span class="math inline">\(f_{\boldsymbol{\theta}}(\mathbf{x})\)</span> that takes a vector (image) <span class="math inline">\(\mathbf{x}\)</span> and returns a vector (distribution) <span class="math inline">\(\widehat{\mathbf{y}}\)</span>.</p>
<p>Normally, we train it by modifying <span class="math inline">\(\boldsymbol{\theta}\)</span> so that</p>
<p><span class="math display">\[ \boldsymbol{\theta}^*\ =\  \underset{\boldsymbol{\theta}}{\mathrm{argmin}} \,\, \text{Loss} \bigl( f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y} \bigr). \]</span></p>
<p>However, it is possible to <em>not train</em> the network but to modify <span class="math inline">\(\mathbf{x}\)</span>, like</p>
<p><span class="math display">\[ \mathbf{x}^*\ =\  \underset{\mathbf{x}}{\mathrm{argmin}} \,\, \text{Loss} \bigl( f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y} \bigr). \]</span></p>
<p>This is very slow as we do a lot more gradient descent.</p>
</section>
<section id="adversarial-examples" class="slide level2">
<h2>Adversarial examples</h2>

<img data-src="adversarial-example.png" class="r-stretch quarto-figure-center"><p class="caption">A demonstration of fast adversarial example generation applied to GoogLeNet on ImageNet. By adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, we can change GoogLeNet’s classification of the image.</p><div class="footer">
<p>Source: Goodfellow et al.&nbsp;(2015), <a href="https://arxiv.org/pdf/1412.6572.pdf">Explaining and Harnessing Adversarial Examples</a>, ICLR.</p>
</div>
</section>
<section id="adversarial-stickers" class="slide level2">
<h2>Adversarial stickers</h2>

<img data-src="the-verge-adversarial_patch_.0.gif" class="r-stretch quarto-figure-center"><p class="caption">Adversarial stickers.</p><div class="footer">
<p>Source: The Verge (2018), <a href="https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google">These stickers make computer vision software hallucinate things that aren’t there</a>.</p>
</div>
</section>
<section id="deep-dream" class="slide level2">
<h2>Deep Dream</h2>

<img data-src="deep-dream.jpeg" class="r-stretch quarto-figure-center"><p class="caption">Deep Dream is an image-modification program released by Google in 2015.</p><div class="footer">
<p>Source: Wikipedia, <a href="https://commons.wikimedia.org/wiki/File:Aurelia-aurita-3-0009.jpg">DeepDream page</a>.</p>
</div>
</section>
<section id="deepdream" class="slide level2">
<h2>DeepDream</h2>
<ul>
<li>Even though many deep learning models are black boxes, convnets are quite interpretable via visualization. Some visualization techniques are: visualizing convnet outputs shows how convnet layers transform the input, visualizing convnet filters shows what visual patterns or concept each filter is receptive to, etc.</li>
<li>The output of a layer is often called its activation, the output of the activation function.</li>
<li>The activations of the first few layers of the network carries more information about the visual contents, while deeper layers encode higher, more abstract concepts.</li>
</ul>
</section>
<section id="deepdream-1" class="slide level2">
<h2>DeepDream</h2>
<ul>
<li>Each filter is receptive to a visual pattern. To visualize a convnet filter, gradient ascent is used to maximize the response of the filter. Gradient ascent maximize a loss function and moves the image in a direction that activate the filter more strongly to enhance its reading of the visual pattern.</li>
<li>DeepDream maximizes the activation of the entire convnet layer rather than that of a specific filter, thus mixing together many visual patterns all at once.</li>
<li>DeepDream starts with an existing image, latches on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.</li>
</ul>
</section>
<section id="many-passes-over-the-image" class="slide level2">
<h2>Many passes over the image</h2>

<img data-src="chollet-deepdream2-blur.png" class="r-stretch quarto-figure-center"><p class="caption">Input images are processed at different scales (called octaves), which further improve the quality of the visualization.</p><div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Figure 12.6 (<strong>redacted</strong>).</p>
</div>
</section>
<section id="original" class="slide level2">
<h2>Original</h2>

<img data-src="deep-dream-melbourne-original.jpg" class="r-stretch quarto-figure-center"><p class="caption">A sunny day on the Mornington peninsula.</p></section>
<section id="transformed" class="slide level2">
<h2>Transformed</h2>

<img data-src="deep-dream-melbourne.png" class="r-stretch quarto-figure-center"><p class="caption">Deep-dreaming version.</p><div class="footer">
<p>Generated by <a href="https://keras.io/examples/generative/deep_dream/">Keras’ Deep Dream tutorial</a>.</p>
</div>
</section></section>
<section>
<section id="section-1" class="title-slide slide level1 center">
<h1></h1>
<h2>
Neural style transfer
</h2>
<p>Applying the style of a reference image to a target image while conserving the content of the target image.</p>

<img data-src="neuralstyletransfer.png" class="r-stretch quarto-figure-center"><p class="caption">An example neural style transfer.</p><aside class="notes">
<ul>
<li>Style: textures, colors, visual patterns (blue-and-yellow circular brushstrokes in Vincent Van Gogh’s Starry Night)</li>
<li>Content: the higher-level macrostructure of the image (buildings in the Tübingen photograph).</li>
</ul>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
<div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Figure 12.9.</p>
</div>
</section>
<section id="goal-of-nst" class="slide level2">
<h2>Goal of NST</h2>
<p>What the model does:</p>
<ul>
<li><p>Preserve content by maintaining similar deeper layer activations between the original image and the generated image. The convnet should “see” both the original image and the generated image as containing the same things.</p></li>
<li><p>Preserve style by maintaining similar correlations within activations for both low level layers and high-level layers. Feature correlations within a layer capture textures: the generated image and the style-reference image should share the same textures at different spatial scales.</p></li>
</ul>
</section>
<section id="a-wanderer-in-greenland" class="slide level2">
<h2>A wanderer in Greenland</h2>
<div class="columns">
<div class="column" style="width:50%;">
<p>Content</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="ninja.jpg"></p>
<p></p><figcaption>Some striking young hiker in Greenland.</figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:50%;">
<p>Style</p>
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="wanderer.jpg"></p>
<p></p><figcaption><em>Wanderer above the Sea of Fog</em> by Caspar David Friedrich.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="a-wanderer-in-greenland-ii" class="slide level2">
<h2>A wanderer in Greenland II</h2>
<div class="columns">
<div class="column" style="width:45%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="ninja.gif"></p>
<p></p><figcaption>Animation of NST in progress.</figcaption><p></p>
</figure>
</div>
</div><div class="column" style="width:55%;">
<div class="quarto-figure quarto-figure-center">
<figure>
<p><img data-src="ninja-wanderer.png"></p>
<p></p><figcaption>One result of NST.</figcaption><p></p>
</figure>
</div>
</div>
</div>
<div class="callout callout-tip callout-captioned callout-style-default">
<div class="callout-body">
<div class="callout-caption">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<p><strong>Question</strong></p>
</div>
<div class="callout-content">
<p>How would you make this faster for one specific style image?</p>
</div>
</div>
</div>
<div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="a-new-style-image" class="slide level2">
<h2>A new style image</h2>

<img data-src="wave.jpg" class="r-stretch quarto-figure-center"><p class="caption">Hokusai’s Great Wave off Kanagawa</p><div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="a-new-content-image" class="slide level2">
<h2>A new content image</h2>

<img data-src="qingdao.jpg" class="r-stretch quarto-figure-center"><p class="caption">The seascape in Qingdao</p><div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="another-neural-style-transfer" class="slide level2">
<h2>Another neural style transfer</h2>

<img data-src="qwave.jpg" class="r-stretch quarto-figure-center"><p class="caption">The seascape in Qingdao in the style of Hokusai’s Great Wave off Kanagawa</p><div class="footer">
<p>Source: Laub (2018), <a href="https://pat-laub.github.io/2018/01/07/neural-style-transfer.html">On Neural Style Transfer</a>, Blog post.</p>
</div>
</section>
<section id="why-is-this-important" class="slide level2">
<h2>Why is this important?</h2>
<p>Taking derivatives with respect to the input image can be a first step toward explainable AI for convolutional networks.</p>
<ul>
<li><a href="https://youtu.be/y8cwyeccuy4">Saliency maps</a></li>
<li><a href="https://youtu.be/xGZfAoh0xKs">Grad-CAM</a></li>
</ul>
</section></section>
<section>
<section id="autoencoders" class="title-slide slide level1 center" data-background-image="unsw-yellow-shape.png" data-visibility="uncounted">
<h1>Autoencoders</h1>

</section>
<section id="autoencoder" class="slide level2">
<h2>Autoencoder</h2>
<p>An autoencoder takes a data/image, maps it to a latent space via en encoder module, then decodes it back to an output with the same dimensions via a decoder module.</p>

<img data-src="chollet-autoencoder-blur.png" class="r-stretch quarto-figure-center"><p class="caption">Schematic of an autoencoder.</p><div class="footer">
<p>Source: François Chollet (2021), <em>Deep Learning with Python</em>, Second Edition, Figure 12.16 (<strong>redacted</strong>).</p>
</div>
</section>
<section id="autoencoder-ii" class="slide level2 smaller">
<h2>Autoencoder II</h2>
<ul>
<li>An autoencoder is trained by using the same image as both the input and the target, meaning an autoencoder learns to reconstruct the original inputs. Therefore it’s <em>not supervised learning</em>, but <em>self-supervised learning</em>.</li>
<li>If we impose constraints on the encoders to be low-dimensional and sparse, <em>the input data will be compressed</em> into fewer bits of information.</li>
<li>Latent space is a place that stores low-dimensional representation of data. It can be used for <em>data compression</em>, where data is compressed to a point in a latent space.</li>
<li>An image can be compressed into a latent representation, which can then be reconstructed back to a <em>slightly different image</em>.</li>
</ul>
<aside class="notes">
<p>For image editing, an image can be projected onto a latent space and moved inside the latent space in a meaningful way (which means we modify its latent representation), before being mapped back to the image space. This will edit the image and allow us to generate images that have never been seen before.</p>
<style type="text/css">
        span.MJX_Assistive_MathML {
          position:absolute!important;
          clip: rect(1px, 1px, 1px, 1px);
          padding: 1px 0 0 0!important;
          border: 0!important;
          height: 1px!important;
          width: 1px!important;
          overflow: hidden!important;
          display:block!important;
      }</style></aside>
</section>
<section id="example-psam" class="slide level2">
<h2>Example: PSAM</h2>
<p>Loading the dataset off-screen (using <a href="https://pat-laub.github.io/DeepLearningMaterials/Lecture-6-Computer-Vision/computer-vision.html#/downloading-the-dataset">Lecture 6 code</a>).</p>
<div class="columns">
<div class="column">
<div class="cell" data-execution_count="77">
<div class="sourceCode cell-code" id="cb123"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb123-1"><a href="#cb123-1"></a>plt.imshow(X_train[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-78-output-1.png"></p>
</div>
</div>
</div><div class="column">
<div class="cell" data-execution_count="78">
<div class="sourceCode cell-code" id="cb124"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb124-1"><a href="#cb124-1"></a>plt.imshow(X_train[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-79-output-1.png"></p>
</div>
</div>
</div>
</div>
</section>
<section id="a-compression-game" class="slide level2">
<h2>A compression game</h2>
<div class="columns">
<div class="column">
<div class="cell" data-execution_count="79">
<div class="sourceCode cell-code" id="cb125"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb125-1"><a href="#cb125-1"></a>plt.imshow(X_train[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span>
<span id="cb125-2"><a href="#cb125-2"></a><span class="bu">print</span>(imgWidth <span class="op">*</span> imgHeight)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>6400</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-80-output-2.png"></p>
</div>
</div>
</div><div class="column">
<blockquote>
<p><em>A 4 with a curly foot, a flat line goes across the middle of the 4, two feet come off the bottom.</em></p>
</blockquote>
<p>96 characters</p>
<blockquote>
<p><em>A Dōng character, rotated counterclockwise 15 degrees.</em></p>
</blockquote>
<p>54 characters</p>
</div>
</div>
</section>
<section id="make-a-basic-autoencoder" class="slide level2">
<h2>Make a basic autoencoder</h2>
<div class="cell" data-execution_count="80">
<div class="sourceCode cell-code" id="cb127"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb127-1"><a href="#cb127-1"></a>numHiddenLayer <span class="op">=</span> <span class="dv">400</span></span>
<span id="cb127-2"><a href="#cb127-2"></a><span class="bu">print</span>(<span class="ss">f"Compress from </span><span class="sc">{</span>imgHeight <span class="op">*</span> imgWidth<span class="sc">}</span><span class="ss"> pixels to </span><span class="sc">{</span>numHiddenLayer<span class="sc">}</span><span class="ss"> latent variables."</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Compress from 6400 pixels to 400 latent variables.</code></pre>
</div>
</div>
<div class="cell" data-execution_count="81">
<div class="sourceCode cell-code" id="cb129"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb129-1"><a href="#cb129-1"></a>tf.random.set_seed(<span class="dv">123</span>)</span>
<span id="cb129-2"><a href="#cb129-2"></a></span>
<span id="cb129-3"><a href="#cb129-3"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb129-4"><a href="#cb129-4"></a>    layers.Rescaling(<span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>, input_shape<span class="op">=</span>(imgHeight, imgWidth, <span class="dv">1</span>)),</span>
<span id="cb129-5"><a href="#cb129-5"></a>    layers.Flatten(),</span>
<span id="cb129-6"><a href="#cb129-6"></a>    layers.Dense(numHiddenLayer, <span class="st">"relu"</span>),</span>
<span id="cb129-7"><a href="#cb129-7"></a>    layers.Dense(imgHeight<span class="op">*</span>imgWidth, <span class="st">"sigmoid"</span>),</span>
<span id="cb129-8"><a href="#cb129-8"></a>    layers.Reshape((imgHeight, imgWidth, <span class="dv">1</span>)),</span>
<span id="cb129-9"><a href="#cb129-9"></a>    layers.Rescaling(<span class="dv">255</span>),</span>
<span id="cb129-10"><a href="#cb129-10"></a>])</span>
<span id="cb129-11"><a href="#cb129-11"></a></span>
<span id="cb129-12"><a href="#cb129-12"></a>model.<span class="bu">compile</span>(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb129-13"><a href="#cb129-13"></a>epochs <span class="op">=</span> <span class="dv">1_000</span></span>
<span id="cb129-14"><a href="#cb129-14"></a>es <span class="op">=</span> keras.callbacks.EarlyStopping(</span>
<span id="cb129-15"><a href="#cb129-15"></a>    patience<span class="op">=</span><span class="dv">5</span>, restore_best_weights<span class="op">=</span><span class="va">True</span>)</span>
<span id="cb129-16"><a href="#cb129-16"></a>model.fit(X_train, X_train, epochs<span class="op">=</span>epochs, verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb129-17"><a href="#cb129-17"></a>    validation_data<span class="op">=</span>(X_val, X_val), callbacks<span class="op">=</span>es)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-model" class="slide level2">
<h2>The model</h2>
<div class="cell" data-execution_count="82">
<div class="sourceCode cell-code" id="cb130"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb130-1"><a href="#cb130-1"></a>model.summary(print_fn<span class="op">=</span>skip_empty)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_1"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
rescaling (Rescaling)       (None, 80, 80, 1)         0
flatten (Flatten)           (None, 6400)              0
dense_4 (Dense)             (None, 400)               2560400
dense_5 (Dense)             (None, 6400)              2566400
reshape (Reshape)           (None, 80, 80, 1)         0
rescaling_1 (Rescaling)     (None, 80, 80, 1)         0
=================================================================
Total params: 5,126,800
Trainable params: 5,126,800
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-execution_count="83">
<div class="sourceCode cell-code" id="cb132"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb132-1"><a href="#cb132-1"></a><span class="bu">print</span>(model.evaluate(X_val, X_val, verbose<span class="op">=</span><span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2257.136962890625</code></pre>
</div>
</div>
</section>
<section id="some-recovered-image" class="slide level2">
<h2>Some recovered image</h2>
<div class="cell" data-execution_count="84">
<div class="sourceCode cell-code" id="cb134"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb134-1"><a href="#cb134-1"></a>X_val_rec <span class="op">=</span> model.predict(X_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div class="cell" data-execution_count="85">
<div class="sourceCode cell-code" id="cb135"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb135-1"><a href="#cb135-1"></a>plt.imshow(X_val[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-86-output-1.png"></p>
</div>
</div>
</div><div class="column">
<div class="cell" data-execution_count="86">
<div class="sourceCode cell-code" id="cb136"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb136-1"><a href="#cb136-1"></a>plt.imshow(X_val_rec[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-87-output-1.png"></p>
</div>
</div>
</div>
</div>
</section>
<section id="invert-the-images" class="slide level2">
<h2>Invert the images</h2>
<div class="columns">
<div class="column">
<div class="cell" data-execution_count="87">
<div class="sourceCode cell-code" id="cb137"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb137-1"><a href="#cb137-1"></a>plt.imshow(<span class="dv">255</span> <span class="op">-</span> X_train[<span class="dv">0</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-88-output-1.png"></p>
</div>
</div>
</div><div class="column">
<div class="cell" data-execution_count="88">
<div class="sourceCode cell-code" id="cb138"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb138-1"><a href="#cb138-1"></a>plt.imshow(<span class="dv">255</span> <span class="op">-</span> X_train[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-89-output-1.png"></p>
</div>
</div>
</div>
</div>
</section>
<section id="try-inverting-the-images" class="slide level2">
<h2>Try inverting the images</h2>
<div class="cell" data-execution_count="89">
<div class="sourceCode cell-code" id="cb139"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb139-1"><a href="#cb139-1"></a>tf.random.set_seed(<span class="dv">123</span>)</span>
<span id="cb139-2"><a href="#cb139-2"></a></span>
<span id="cb139-3"><a href="#cb139-3"></a>model <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb139-4"><a href="#cb139-4"></a>    layers.Rescaling(<span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>, input_shape<span class="op">=</span>(imgHeight, imgWidth, <span class="dv">1</span>)),</span>
<span id="cb139-5"><a href="#cb139-5"></a>    layers.Lambda(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">-</span> x),</span>
<span id="cb139-6"><a href="#cb139-6"></a>    layers.Flatten(),</span>
<span id="cb139-7"><a href="#cb139-7"></a>    layers.Dense(numHiddenLayer, <span class="st">"relu"</span>),</span>
<span id="cb139-8"><a href="#cb139-8"></a>    layers.Dense(imgHeight<span class="op">*</span>imgWidth, <span class="st">"sigmoid"</span>),</span>
<span id="cb139-9"><a href="#cb139-9"></a>    layers.Lambda(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">-</span> x),</span>
<span id="cb139-10"><a href="#cb139-10"></a>    layers.Reshape((imgHeight, imgWidth, <span class="dv">1</span>)),</span>
<span id="cb139-11"><a href="#cb139-11"></a>    layers.Rescaling(<span class="dv">255</span>),</span>
<span id="cb139-12"><a href="#cb139-12"></a>])</span>
<span id="cb139-13"><a href="#cb139-13"></a></span>
<span id="cb139-14"><a href="#cb139-14"></a>model.<span class="bu">compile</span>(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb139-15"><a href="#cb139-15"></a>model.fit(X_train, X_train, epochs<span class="op">=</span>epochs, verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb139-16"><a href="#cb139-16"></a>    validation_data<span class="op">=</span>(X_val, X_val), callbacks<span class="op">=</span>es)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="the-model-1" class="slide level2">
<h2>The model</h2>
<div class="cell" data-execution_count="90">
<div class="sourceCode cell-code" id="cb140"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb140-1"><a href="#cb140-1"></a>model.summary(print_fn<span class="op">=</span>skip_empty)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_2"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
rescaling_2 (Rescaling)     (None, 80, 80, 1)         0
lambda (Lambda)             (None, 80, 80, 1)         0
flatten_1 (Flatten)         (None, 6400)              0
dense_6 (Dense)             (None, 400)               2560400
dense_7 (Dense)             (None, 6400)              2566400
lambda_1 (Lambda)           (None, 6400)              0
reshape_1 (Reshape)         (None, 80, 80, 1)         0
rescaling_3 (Rescaling)     (None, 80, 80, 1)         0
=================================================================
Total params: 5,126,800
Trainable params: 5,126,800
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-execution_count="91">
<div class="sourceCode cell-code" id="cb142"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb142-1"><a href="#cb142-1"></a><span class="bu">print</span>(model.evaluate(X_val, X_val, verbose<span class="op">=</span><span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>2352.100830078125</code></pre>
</div>
</div>
</section>
<section id="some-recovered-image-1" class="slide level2">
<h2>Some recovered image</h2>
<div class="cell" data-execution_count="92">
<div class="sourceCode cell-code" id="cb144"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb144-1"><a href="#cb144-1"></a>X_val_rec <span class="op">=</span> model.predict(X_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div class="cell" data-execution_count="93">
<div class="sourceCode cell-code" id="cb145"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb145-1"><a href="#cb145-1"></a>plt.imshow(X_val[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-94-output-1.png"></p>
</div>
</div>
</div><div class="column">
<div class="cell" data-execution_count="94">
<div class="sourceCode cell-code" id="cb146"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb146-1"><a href="#cb146-1"></a>plt.imshow(X_val_rec[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-95-output-1.png"></p>
</div>
</div>
</div>
</div>
</section>
<section id="cnn-enhanced-encoder" class="slide level2">
<h2>CNN-enhanced encoder</h2>
<div class="cell" data-execution_count="95">
<div class="sourceCode cell-code" id="cb147"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb147-1"><a href="#cb147-1"></a>tf.random.set_seed(<span class="dv">123</span>)</span>
<span id="cb147-2"><a href="#cb147-2"></a></span>
<span id="cb147-3"><a href="#cb147-3"></a>encoder <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb147-4"><a href="#cb147-4"></a>    layers.Rescaling(<span class="fl">1.</span><span class="op">/</span><span class="dv">255</span>, input_shape<span class="op">=</span>(imgHeight, imgWidth, <span class="dv">1</span>)),</span>
<span id="cb147-5"><a href="#cb147-5"></a>    layers.Lambda(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">-</span> x),</span>
<span id="cb147-6"><a href="#cb147-6"></a>    layers.Conv2D(<span class="dv">16</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb147-7"><a href="#cb147-7"></a>    layers.MaxPooling2D(),</span>
<span id="cb147-8"><a href="#cb147-8"></a>    layers.Conv2D(<span class="dv">32</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb147-9"><a href="#cb147-9"></a>    layers.MaxPooling2D(),</span>
<span id="cb147-10"><a href="#cb147-10"></a>    layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb147-11"><a href="#cb147-11"></a>    layers.MaxPooling2D(),</span>
<span id="cb147-12"><a href="#cb147-12"></a>    layers.Flatten(),</span>
<span id="cb147-13"><a href="#cb147-13"></a>    layers.Dense(numHiddenLayer, <span class="st">"relu"</span>)</span>
<span id="cb147-14"><a href="#cb147-14"></a>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="cnn-enhanced-decoder" class="slide level2">
<h2>CNN-enhanced decoder</h2>
<div class="cell" data-execution_count="96">
<div class="sourceCode cell-code" id="cb148"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb148-1"><a href="#cb148-1"></a>decoder <span class="op">=</span> keras.models.Sequential([</span>
<span id="cb148-2"><a href="#cb148-2"></a>    keras.Input(shape<span class="op">=</span>(numHiddenLayer,)),</span>
<span id="cb148-3"><a href="#cb148-3"></a>    layers.Dense(<span class="dv">20</span><span class="op">*</span><span class="dv">20</span>),</span>
<span id="cb148-4"><a href="#cb148-4"></a>    layers.Reshape((<span class="dv">20</span>, <span class="dv">20</span>, <span class="dv">1</span>)),</span>
<span id="cb148-5"><a href="#cb148-5"></a>    layers.Conv2D(<span class="dv">128</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb148-6"><a href="#cb148-6"></a>    layers.UpSampling2D(),</span>
<span id="cb148-7"><a href="#cb148-7"></a>    layers.Conv2D(<span class="dv">64</span>, <span class="dv">3</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb148-8"><a href="#cb148-8"></a>    layers.UpSampling2D(),</span>
<span id="cb148-9"><a href="#cb148-9"></a>    layers.Conv2D(<span class="dv">1</span>, <span class="dv">1</span>, padding<span class="op">=</span><span class="st">"same"</span>, activation<span class="op">=</span><span class="st">"relu"</span>),</span>
<span id="cb148-10"><a href="#cb148-10"></a>    layers.Lambda(<span class="kw">lambda</span> x: <span class="dv">1</span> <span class="op">-</span> x),</span>
<span id="cb148-11"><a href="#cb148-11"></a>    layers.Rescaling(<span class="dv">255</span>),</span>
<span id="cb148-12"><a href="#cb148-12"></a>])</span>
<span id="cb148-13"><a href="#cb148-13"></a></span>
<span id="cb148-14"><a href="#cb148-14"></a>model <span class="op">=</span> keras.models.Sequential([encoder, decoder])</span>
<span id="cb148-15"><a href="#cb148-15"></a>model.<span class="bu">compile</span>(<span class="st">"adam"</span>, <span class="st">"mse"</span>)</span>
<span id="cb148-16"><a href="#cb148-16"></a>model.fit(X_train, X_train, epochs<span class="op">=</span>epochs, verbose<span class="op">=</span><span class="dv">0</span>,</span>
<span id="cb148-17"><a href="#cb148-17"></a>    validation_data<span class="op">=</span>(X_val, X_val), callbacks<span class="op">=</span>es)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="encoder-summary" class="slide level2">
<h2>Encoder summary</h2>
<div class="cell" data-execution_count="97">
<div class="sourceCode cell-code" id="cb149"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb149-1"><a href="#cb149-1"></a>encoder.summary(print_fn<span class="op">=</span>skip_empty)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_3"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
rescaling_4 (Rescaling)     (None, 80, 80, 1)         0
lambda_2 (Lambda)           (None, 80, 80, 1)         0
conv2d_188 (Conv2D)         (None, 80, 80, 16)        160
max_pooling2d_8 (MaxPooling  (None, 40, 40, 16)       0
2D)
conv2d_189 (Conv2D)         (None, 40, 40, 32)        4640
max_pooling2d_9 (MaxPooling  (None, 20, 20, 32)       0
2D)
conv2d_190 (Conv2D)         (None, 20, 20, 64)        18496
max_pooling2d_10 (MaxPoolin  (None, 10, 10, 64)       0
g2D)
flatten_2 (Flatten)         (None, 6400)              0
dense_8 (Dense)             (None, 400)               2560400
=================================================================
Total params: 2,583,696
Trainable params: 2,583,696
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
</section>
<section id="decoder-summary" class="slide level2">
<h2>Decoder summary</h2>
<div class="cell" data-execution_count="98">
<div class="sourceCode cell-code" id="cb151"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb151-1"><a href="#cb151-1"></a>decoder.summary(print_fn<span class="op">=</span>skip_empty)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Model: "sequential_4"
_________________________________________________________________
Layer (type)                Output Shape              Param #
=================================================================
dense_9 (Dense)             (None, 400)               160400
reshape_2 (Reshape)         (None, 20, 20, 1)         0
conv2d_191 (Conv2D)         (None, 20, 20, 128)       1280
up_sampling2d (UpSampling2D  (None, 40, 40, 128)      0
)
conv2d_192 (Conv2D)         (None, 40, 40, 64)        73792
up_sampling2d_1 (UpSampling  (None, 80, 80, 64)       0
2D)
conv2d_193 (Conv2D)         (None, 80, 80, 1)         65
lambda_3 (Lambda)           (None, 80, 80, 1)         0
rescaling_5 (Rescaling)     (None, 80, 80, 1)         0
=================================================================
Total params: 235,537
Trainable params: 235,537
Non-trainable params: 0
_________________________________________________________________</code></pre>
</div>
</div>
<div class="cell" data-execution_count="99">
<div class="sourceCode cell-code" id="cb153"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb153-1"><a href="#cb153-1"></a><span class="bu">print</span>(model.evaluate(X_val, X_val, verbose<span class="op">=</span><span class="dv">0</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>1849.020263671875</code></pre>
</div>
</div>
</section>
<section id="some-recovered-image-2" class="slide level2">
<h2>Some recovered image</h2>
<div class="cell" data-execution_count="100">
<div class="sourceCode cell-code" id="cb155"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb155-1"><a href="#cb155-1"></a>X_val_rec <span class="op">=</span> model.predict(X_val)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<div class="columns">
<div class="column">
<div class="cell" data-execution_count="101">
<div class="sourceCode cell-code" id="cb156"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb156-1"><a href="#cb156-1"></a>plt.imshow(X_val[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-102-output-1.png"></p>
</div>
</div>
</div><div class="column">
<div class="cell" data-execution_count="102">
<div class="sourceCode cell-code" id="cb157"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb157-1"><a href="#cb157-1"></a>plt.imshow(X_val_rec[<span class="dv">42</span>], cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-103-output-1.png"></p>
</div>
</div>
</div>
</div>
</section>
<section id="latent-space-vs-word-embedding" class="slide level2 smaller">
<h2>Latent space vs word embedding</h2>
<ul>
<li>We revisit the concept of word embedding, where words in the vocabulary are mapped into vector representations. Words with similar meaning should lie close to one another in the word-embedding space.</li>
<li>Latent space contains low-dimensional representation of data. Data/Images that are similar should lie close in the latent space.</li>
<li>There are pre-trained word-embedding spaces such as those for English-language movie review, German-language legal documents, etc. Semantic relationships between words differ for different tasks. Similarly, the structure of latent spaces for different data sets (humans faces, animals, etc) are different.</li>
</ul>
</section>
<section id="latent-space-vs-word-embedding-1" class="slide level2">
<h2>Latent space vs word embedding</h2>
<ul>
<li>Given a latent space of representations, or an embedding space, certain directions in the space may encode interesting axes of variation in the original data.</li>
<li>A <strong>concept vector</strong> is a direction of variation in the data. For example there may be a smile vector such that if <span class="math inline">\(z\)</span> is the latent representation of a face, then <span class="math inline">\(z+s\)</span> is the representation of the same face, smiling. We can generate an image of the person smiling from this latent representation.</li>
</ul>
</section>
<section id="intentionally-add-noise-to-inputs" class="slide level2">
<h2>Intentionally add noise to inputs</h2>
<div class="columns">
<div class="column">
<div class="cell" data-execution_count="103">
<div class="sourceCode cell-code" id="cb158"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb158-1"><a href="#cb158-1"></a>mask <span class="op">=</span> rnd.random(size<span class="op">=</span>X_train.shape[<span class="dv">1</span>:]) <span class="op">&lt;</span> <span class="fl">0.5</span></span>
<span id="cb158-2"><a href="#cb158-2"></a>plt.imshow(mask <span class="op">*</span> (<span class="dv">255</span> <span class="op">-</span> X_train[<span class="dv">0</span>]), cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-104-output-1.png"></p>
</div>
</div>
</div><div class="column">
<div class="cell" data-execution_count="104">
<div class="sourceCode cell-code" id="cb159"><pre class="sourceCode numberSource python number-lines code-with-copy"><code class="sourceCode python"><span id="cb159-1"><a href="#cb159-1"></a>mask <span class="op">=</span> rnd.random(size<span class="op">=</span>X_train.shape[<span class="dv">1</span>:]) <span class="op">&lt;</span> <span class="fl">0.5</span></span>
<span id="cb159-2"><a href="#cb159-2"></a>plt.imshow(mask <span class="op">*</span> (<span class="dv">255</span> <span class="op">-</span> X_train[<span class="dv">42</span>]) <span class="op">*</span> mask, cmap<span class="op">=</span><span class="st">"gray"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<p><img data-src="generative-networks_files/figure-revealjs/cell-105-output-1.png"></p>
</div>
</div>
</div>
</div>
</section>
<section id="denoising-autoencoder" class="slide level2">
<h2>Denoising autoencoder</h2>
<p>Can be used to do <a href="https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629">feature engineering for supervised learning problems</a></p>
<blockquote>
<p>It is also possible to include input variables as outputs to infer missing values or just help the model “understand” the features – in fact the winning solution of a claims prediction Kaggle competition heavily used denoising autoencoders together with model stacking and ensembling – read more here.</p>
</blockquote>
<p>Jacky Poon</p>
<div class="footer">
<p>Source: Poon (2021), <a href="https://actuariesinstitute.github.io/cookbook/docs/multitasking_risk_pricing.html"><em>Multitasking Risk Pricing Using Deep Learning</em></a>, Actuaries’ Analytical Cookbook.</p>
</div>
</section></section>
<section id="section-2" class="title-slide slide level1 center" data-visibility="uncounted">
<h1></h1>
<h2>
Glossary
</h2>
<div class="columns">
<div class="column">
<ul>
<li>autoencoder</li>
<li>bias</li>
<li>DeepDream</li>
<li>greedy sampling</li>
<li>GloVe</li>
<li>Grad-CAM</li>
<li>language model</li>
</ul>
</div><div class="column">
<ul>
<li>latent space</li>
<li>neural style transfer</li>
<li>softmax temperature</li>
<li>stochastic sampling</li>
<li>word embeddings/vectors</li>
<li>word2vec</li>
</ul>
</div>
</div>
<script defer="">
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>

<img src="unsw-logo.svg" class="slide-logo r-stretch"><div class="footer footer-default">
<p>Slides: <a href="https://pat-laub.github.io">Dr Patrick Laub</a> (<span class="citation" data-cites="PatrickLaub">@PatrickLaub</span>).</p>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="generative-networks_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="generative-networks_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="generative-networks_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="generative-networks_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="generative-networks_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="generative-networks_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="generative-networks_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="generative-networks_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="generative-networks_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="generative-networks_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="generative-networks_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': false,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":true,"boardmarkerWidth":6,"grid":false,"background":["rgba(255,255,255,0.0)","https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"]},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: true,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: true,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.2,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          // use id or data attribute instead here
          let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    <script>registerRevealCallbacks();</script>
    

</body></html>