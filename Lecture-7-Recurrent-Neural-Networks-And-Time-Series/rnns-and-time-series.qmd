---
title: Recurrent Neural Networks
---

```{python}
#| echo: false
#| warning: false
import os
os.environ["KERAS_BACKEND"] = "torch"
os.environ["CUDA_VISIBLE_DEVICES"] = ""

import torch
torch.set_num_threads(1)

import matplotlib

import cycler
colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
matplotlib.pyplot.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (2.0, 2.0)
  # matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 3.0)
  # matplotlib.pyplot.rcParams['figure.dpi'] = 350

def set_rectangular_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 2.0)

set_rectangular_figures()
matplotlib.pyplot.rcParams['figure.dpi'] = 350
matplotlib.pyplot.rcParams['savefig.bbox'] = "tight"
matplotlib.pyplot.rcParams['font.family'] = "serif"

matplotlib.pyplot.rcParams['axes.spines.right'] = False
matplotlib.pyplot.rcParams['axes.spines.top'] = False

def square_fig():
  return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()

def add_diagonal_line():
  xl = matplotlib.pyplot.xlim()
  yl = matplotlib.pyplot.ylim()

  min_left = min(xl[0], yl[0])
  max_right = max(xl[1], yl[1])
  matplotlib.pyplot.plot([min_left, max_right], [min_left, max_right], color="black", linestyle="--")
  matplotlib.pyplot.xlim([min_left, max_right])
  matplotlib.pyplot.ylim([min_left, max_right])

import pandas
pandas.options.display.max_rows = 4

import numpy
numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import keras
keras.utils.set_random_seed(1)
```

::: {.content-visible unless-format="revealjs"}

```{python}
#| code-fold: true
#| code-summary: Show the package imports
import random
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from keras.models import Sequential
from keras.layers import Dense, Input
from keras.callbacks import EarlyStopping
```

:::

# Tensors & Time Series {visibility="uncounted"}

## Shapes of data

![Illustration of tensors of different rank.](medium-tensor-rank.png)

::: footer
Source: Paras Patidar (2019), [Tensors â€” Representation of Data In Neural Networks](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b), Medium article.
:::


## The `axis` argument in numpy

Starting with a $(3, 4)$-shaped matrix:
```{python}
X = np.arange(12).reshape(3,4)
X
```

::: {.content-visible unless-format="revealjs"}
The above code creates an array with values from 0 to 11 and converts that array into a matrix with 3 rows and 4 columns.
:::

::: columns
::: column
`axis=0`: $(3, 4) \leadsto (4,)$.
```{python}
X.sum(axis=0)
```

::: {.content-visible unless-format="revealjs"}
The above code returns the column sum. This changes the shape of the matrix from $(3,4)$ to $(4,)$. Similarly, `X.sum(axis=1)` returns row sums and will change the shape of the matrix from $(3,4)$ to $(3,)$.
:::

:::
::: column
`axis=1`: $(3, 4) \leadsto (3,)$.
```{python}
X.prod(axis=1)
```
:::
:::

The return value's rank is one less than the input's rank.

::: {.callout-important}
The `axis` parameter tells us which dimension is removed.
:::

## Using `axis` & `keepdims`

With `keepdims=True`, the rank doesn't change.

```{python}
X = np.arange(12).reshape(3,4)
X
```

::: columns
::: column
`axis=0`: $(3, 4) \leadsto (1, 4)$.
```{python}
X.sum(axis=0, keepdims=True)
```
:::
::: column
`axis=1`: $(3, 4) \leadsto (3, 1)$.
```{python}
X.prod(axis=1, keepdims=True)
```
:::
:::

::: columns
::: column
```{python}
#| error: true
X / X.sum(axis=1)
```
:::
::: column
```{python}
X / X.sum(axis=1, keepdims=True)
```
:::
:::

## The rank of a time series

Say we had $n$ observations of a time series $x_1, x_2, \dots, x_n$. 

This $\mathbf{x} = (x_1, \dots, x_n)$ would have shape $(n,)$ & rank 1.

If instead we had a batch of $b$ time series'

$$
\mathbf{X} = \begin{pmatrix}
x_7 & x_8 & \dots & x_{7+n-1} \\
x_2 & x_3 & \dots & x_{2+n-1} \\
\vdots & \vdots & \ddots & \vdots \\
x_3 & x_4 & \dots & x_{3+n-1} \\
\end{pmatrix}  \,,
$$

the batch $\mathbf{X}$ would have shape $(b, n)$ & rank 2.

## Multivariate time series

::: {.content-visible unless-format="revealjs"}
Multivariate time series consists of more than 1 variable observation at a given time point. Following example has two variables `x` and `y`. 
:::

::: columns
::: {.column width="35%"}

<center>

```{python}
#| echo: false
from IPython.display import Markdown
t = range(4)
x = [f"$x_{i}$" for i in t]
y = [f"$y_{i}$" for i in t]
df = pandas.DataFrame({"$x$": x, "$y$": y})
df.index.name='$t$'
Markdown(df.to_markdown())
```

</center>

:::
::: {.column width="65%"}
Say $n$ observations of the $m$ time series, would be a shape $(n, m)$ matrix of rank 2.

In Keras, a batch of $b$ of these time series has shape $(b, n, m)$ and has rank 3.
:::
:::

::: {.callout-note}
Use $\mathbf{x}_t \in \mathbb{R}^{1 \times m}$ to denote the vector of all time series at time $t$.
Here, $\mathbf{x}_t = (x_t, y_t)$.
:::


# Some Recurrent Structures {visibility="uncounted"}

## Recurrence relation

> A recurrence relation is an equation that expresses each element of a sequence as a function of the preceding ones. More precisely, in the case where only the immediately preceding element is involved, a recurrence relation has the form
> 
> $$ u_n = \psi(n, u_{n-1}) \quad \text{ for } \quad n > 0.$$

__Example__: Factorial $n! = n (n-1)!$ for $n > 0$ given $0! = 1$.

::: footer
Source: Wikipedia, [Recurrence relation](https://en.wikipedia.org/wiki/Recurrence_relation#Definition). 
:::

## Diagram of an RNN cell

The RNN processes each data in the sequence one by one, while keeping memory of what came before.

::: {.content-visible unless-format="revealjs"}
The following figure shows how the recurrent neural network combines an input `X_l` with a preprocessed state of the process `A_l` to produce the output `O_l`. RNNs have a cyclic information processing structure that enables them to pass information sequentially from previous inputs. RNNs can capture dependencies and patterns in sequential data, making them useful for analysing time series data.
:::

![Schematic of a simple recurrent neural network. E.g. SimpleRNN, LSTM, or GRU.](ISLR2-10_12.svg)

::: footer
Source: James et al (2022), [An Introduction to Statistical Learning](https://www.statlearning.com/), 2nd edition, Figure 10.12.
:::

## A SimpleRNN cell.

![Diagram of a SimpleRNN cell.](colah-LSTM3-SimpleRNN.png)

All the outputs before the final one are often discarded.

::: footer
Source: Christopher Olah (2015), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs), Colah's Blog.
:::

## SimpleRNN

Say each prediction is a vector of size $d$, so $\mathbf{y}_t \in \mathbb{R}^{1 \times d}$.

Then the main equation of a SimpleRNN, given $\mathbf{y}_0 = \mathbf{0}$, is

$$ \mathbf{y}_t = \psi\bigl( \mathbf{x}_t \mathbf{W}_x + \mathbf{y}_{t-1} \mathbf{W}_y + \mathbf{b} \bigr) . $$

Here,
$$
\begin{aligned}
&\mathbf{x}_t \in \mathbb{R}^{1 \times m}, \mathbf{W}_x \in \mathbb{R}^{m \times d}, \\
&\mathbf{y}_{t-1} \in \mathbb{R}^{1 \times d}, \mathbf{W}_y \in \mathbb{R}^{d \times d}, \text{ and } \mathbf{b} \in \mathbb{R}^{d}.
\end{aligned}
$$

::: {.content-visible unless-format="revealjs"}
At each time step, a simple Recurrent Neural Network (RNN) takes an input vector `x_t`, incorporate it with the information from the previous hidden state `{y}_{t-1}` and produces an output vector at each time step `y_t`. The hidden state helps the network remember the context of the previous words, enabling it to make informed predictions about what comes next in the sequence. In a simple RNN, the output at time `(t-1)` is the same as the hidden state at time `t`.
:::

## SimpleRNN (in batches)

::: {.content-visible unless-format="revealjs"}
The difference between RNN and RNNs with batch processing lies in the way how the neural network handles sequences of input data. With batch processing, the model processes multiple ($b$) input sequences simultaneously. The training data is grouped into batches, and the weights are updated based on the average error across the entire batch. Batch processing often results in more stable weight updates, as the model learns from a diverse set of examples in each batch, reducing the impact of noise in individual sequences. 
:::

Say we operate on batches of size $b$, then $\mathbf{Y}_t \in \mathbb{R}^{b \times d}$.

The main equation of a SimpleRNN, given $\mathbf{Y}_0 = \mathbf{0}$, is
$$ \mathbf{Y}_t = \psi\bigl( \mathbf{X}_t \mathbf{W}_x + \mathbf{Y}_{t-1} \mathbf{W}_y + \mathbf{b} \bigr) . $$
Here,
$$
\begin{aligned}
&\mathbf{X}_t \in \mathbb{R}^{b \times m}, \mathbf{W}_x \in \mathbb{R}^{m \times d}, \\
&\mathbf{Y}_{t-1} \in \mathbb{R}^{b \times d}, \mathbf{W}_y \in \mathbb{R}^{d \times d}, \text{ and } \mathbf{b} \in \mathbb{R}^{d}.
\end{aligned}
$$

::: footer
Remember, $\mathbf{X} \in \mathbb{R}^{b \times n \times m}$, $\mathbf{Y} \in \mathbb{R}^{b \times d}$, and $\mathbf{X}_t$ is equivalent to `X[:, t, :]`.
:::

## Simple Keras demo

```{python}
num_obs = 4                                                                     #<1>
num_time_steps = 3                                                              #<2>
num_time_series = 2                                                             #<3>

X = np.arange(num_obs*num_time_steps*num_time_series).astype(np.float32) \
        .reshape([num_obs, num_time_steps, num_time_series])                    #<4>

output_size = 1                                                                 
y = np.array([0, 0, 1, 1])
```
1. Defines the number of observations
2. Defines the number of time steps
3. Defines the number of time series
4. Reshapes the array to a range 3 tensor (4,3,2)

::: columns
::: column
```{python}
X[:2]               #<1>
```
1. Selects the first two slices along the first dimension. Since the tensor of dimensions (4,3,2), `X[:2]` selects the first two slices (0 and 1) along the first dimension, and returns a sub-tensor of shape (2,3,2). 
:::
::: column
```{python}
X[2:]              #<1>
```
1. Selects the last two slices along the first dimension. The first dimension (axis=0) has size 4. Therefore, `X[2:]`  selects the last two slices (2 and 3) along the first dimension, and returns a sub-tensor of shape (2,3,2). 
:::
:::

## Keras' SimpleRNN

As usual, the `SimpleRNN` is just a layer in Keras. 

```{python}
from keras.layers import SimpleRNN                       #<1>

random.seed(1234)                                                   #<2>
model = Sequential([
  SimpleRNN(output_size, activation="sigmoid")                      #<3>
])
model.compile(loss="binary_crossentropy", metrics=["accuracy"])     #<4>

hist = model.fit(X, y, epochs=500, verbose=False)                   #<5>
model.evaluate(X, y, verbose=False)                                 #<6>
```

1. Imports the SimpleRNN layer from the Keras library
2. Sets the seed for the random number generator to ensure reproducibility
3. Defines a simple RNN with one output node and sigmoid activation function
4. Specifies binary crossentropy as the loss function (usually used in classification problems), and specifies "accuracy" as the metric to be monitored during training
5. Trains the model for 500 epochs and saves output as `hist`
6. Evaluates the model to obtain a value for the loss and accuracy


The predicted probabilities on the training set are:

```{python}
model.predict(X, verbose=0)
```

## SimpleRNN weights

::: {.content-visible unless-format="revealjs"}
To verify the results of predicted probabilities, we can obtain the weights of the fitted model and calculate the outcome manually as follows.
:::

```{python}
model.get_weights()
```

```{python}
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

W_x, W_y, b = model.get_weights()

Y = np.zeros((num_obs, output_size), dtype=np.float32)
for t in range(num_time_steps):
    X_t = X[:, t, :]
    z = X_t @ W_x + Y @ W_y + b
    Y = sigmoid(z)

Y
```


## LSTM internals

::: {.content-visible unless-format="revealjs"}
Simple RNN structures encounter vanishing gradient problems, hence, struggle with learning long term dependencies. LSTM are designed to overcome this problem. LSTMs have a more complex network structure (contains more memory cells and gating mechanisms) and can better regulate the information flow. 
:::

![Diagram of an LSTM cell.](colah-LSTM3-chain.png)
![Notation for the diagram.](colah-LSTM2-notation.png)

::: footer
Source: Christopher Olah (2015), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs), Colah's Blog.
:::

## GRU internals

::: {.content-visible unless-format="revealjs"}
GRUs are simpler compared to LSTM, hence, computationally more efficient than LSTMs. 
:::

![Diagram of a GRU cell.](colah-LSTM3-var-GRU.png)

::: footer
Source: Christopher Olah (2015), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs), Colah's Blog.
:::

# Recurrent Neural Networks {visibility="uncounted"}

## Basic facts of RNNs

- A recurrent neural network is a type of neural network that is designed to process sequences of data (e.g. time series, sentences).
- A recurrent neural network is any network that contains a recurrent layer.
- A recurrent layer is a layer that processes data in a sequence.
- An RNN can have one or more recurrent layers.
- Weights are shared over time; this allows the model to be used on arbitrary-length sequences.

## Applications

- Forecasting: revenue forecast, weather forecast, predict disease rate from medical history, etc. 
- Classification: given a time series of the activities of a visitor on a website, classify whether the visitor is a bot or a human.
- Event detection: given a continuous data stream, identify the occurrence of a specific event. Example: Detect utterances like "Hey Alexa" from an audio stream.
- Anomaly detection: given a continuous data stream, detect anything unusual happening. Example: Detect unusual activity on the corporate network.

## Input and output sequences

![Categories of recurrent neural networks: sequence to sequence, sequence to vector, vector to sequence, encoder-decoder network.](Geron-rnnType.png)

::: footer
Source: AurÃ©lien GÃ©ron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Chapter 15.
:::


## Input and output sequences

- Sequence to sequence: Useful for predicting time series such as using prices over the last $N$ days to output the prices shifted one day into the future (i.e. from $N-1$ days ago to tomorrow.)
- Sequence to vector: ignore all outputs in the previous time steps except for the last one. Example: give a sentiment score to a sequence of words corresponding to a movie review.

## Input and output sequences {.smaller}

- Vector to sequence: feed the network the same input vector over and over at each time step and let it output a sequence. Example: given that the input is an image, find a caption for it. The image is treated as an input vector (pixels in an image do not follow a sequence). The caption is a sequence of textual description of the image. A dataset containing images and their descriptions is the input of the RNN.
- The Encoder-Decoder: The encoder is a sequence-to-vector network. The decoder is a vector-to-sequence network. Example: Feed the network a sequence in one language. Use the encoder to convert the sentence into a single vector representation. The decoder decodes this vector into the translation of the sentence in another language.

## Recurrent layers can be stacked.

![_Deep RNN_ unrolled through time.](Geron-recurrentneurondeep.png)


::: footer
Source: AurÃ©lien GÃ©ron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Chapter 15.
:::

# CoreLogic Hedonic Home Value Index {visibility="uncounted"}

## Australian House Price Indices

```{python}
#|echo: false
house_prices = pd.read_csv("sa3-capitals-house-price-index.csv", index_col=0)
house_prices.index = pd.to_datetime(house_prices.index)
house_prices.index.name = "Date"
house_prices = house_prices[house_prices.index > pd.to_datetime("1990")]
house_prices.columns = ["Brisbane", "East_Bris", "North_Bris", "West_Bris",
  "Melbourne", "North_Syd", "Sydney"]
house_prices.plot(legend=False);
```

## Percentage changes {.smaller}

```{python}
changes = house_prices.pct_change().dropna()
changes.round(2)
```

```{python}
#| echo: false
pandas.options.display.max_rows = 7
``` 

## Percentage changes 


```{python}
#| eval: false
changes.plot();
```
```{python}
#| echo: false
changes.plot(lw=1);
matplotlib.pyplot.legend(bbox_to_anchor=(0.5, 1.0), loc="lower center", frameon=False, ncol=3);
```

## The size of the changes

::: columns
::: column
```{python}
changes.mean()
```

```{python}
changes *= 100
```

```{python}
changes.mean()
```
:::
::: column
```{python}
#| echo: false
# set_square_figures() 
```
```{python}
changes.plot(legend=False);
```
:::
:::

# Splitting time series data {visibility="uncounted"}

## Split _without_ shuffling

```{python}
num_train = int(0.6 * len(changes))
num_val = int(0.2 * len(changes))
num_test = len(changes) - num_train - num_val
print(f"# Train: {num_train}, # Val: {num_val}, # Test: {num_test}")
```
```{python}
#| echo: false
changes.iloc[:num_train,0].plot(c=colors[0], lw=1, alpha=0.5);
changes.iloc[num_train:(num_train+num_val),0].plot(c=colors[1], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[(num_train+num_val):,0].plot(c=colors[2], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[:num_train,1:].plot(c=colors[0], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[num_train:(num_train+num_val),1:].plot(c=colors[1], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[(num_train+num_val):,1:].plot(c=colors[2], ax=plt.gca(), lw=1, alpha=0.5);
plt.legend(["Train", "Val", "Test"], frameon=False, ncol=3);
```

```{python}
#| echo: false
set_rectangular_figures() 
```

## Subsequences of a time series

Keras has a built-in method for converting a time series into subsequences/chunks.

```{python}
#| warning: false
from keras.utils import timeseries_dataset_from_array

integers = range(10)                                
dummy_dataset = timeseries_dataset_from_array(
    data=integers[:-3],                                 
    targets=integers[3:],                               
    sequence_length=3,                                      
    batch_size=2,                                           
)

for inputs, targets in dummy_dataset:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))
```

::: footer
Source: Code snippet in Chapter 10 of Chollet.
:::


 
## On time series splits

If you have a lot of time series data, then use:

```{python}
from keras.utils import timeseries_dataset_from_array
data = range(20); seq = 3; ts = data[:-seq]; target = data[seq:]
nTrain = int(0.5 * len(ts)); nVal = int(0.25 * len(ts))
nTest = len(ts) - nTrain - nVal
print(f"# Train: {nTrain}, # Val: {nVal}, # Test: {nTest}")
```

::: columns
::: {.column width="33%"}

```{python}
trainDS = \
  timeseries_dataset_from_array(
    ts, target, seq,
    end_index=nTrain)
```

:::
::: {.column width="33%"}

```{python}
valDS = \
  timeseries_dataset_from_array(
    ts, target, seq,
    start_index=nTrain,
    end_index=nTrain+nVal)
```

:::
::: {.column width="33%"}

```{python}
testDS = \
  timeseries_dataset_from_array(
    ts, target, seq,
    start_index=nTrain+nVal)
```

:::
:::

::: columns
::: {.column width="33%"}

```{python}
#| echo: false
print("Training dataset")
for inputs, targets in trainDS:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))
```

:::
::: {.column width="33%"}

```{python}
#| echo: false
print("Validation dataset")
for inputs, targets in valDS:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))
```

:::
::: {.column width="33%"}

```{python}
#| echo: false
print("Test dataset")
for inputs, targets in testDS:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))
```

:::
:::

::: footer
Adapted from: FranÃ§ois Chollet (2021), _Deep Learning with Python_, Second Edition, Listing 10.7.
:::

## On time series splits II

If you _don't_ have a lot of time series data, consider:

```{python}
X = []; y = []
for i in range(len(data)-seq):
    X.append(data[i:i+seq])
    y.append(data[i+seq])
X = np.array(X); y = np.array(y);
```

::: columns
::: {.column width="33%"}

```{python}
nTrain = int(0.5 * X.shape[0])
X_train = X[:nTrain]
y_train = y[:nTrain]
```

:::
::: {.column width="33%"}

```{python}
nVal = int(np.ceil(0.25 * X.shape[0]))
X_val = X[nTrain:nTrain+nVal]
y_val = y[nTrain:nTrain+nVal]
```

:::
::: {.column width="33%"}

```{python}
nTest = X.shape[0] - nTrain - nVal
X_test = X[nTrain+nVal:]
y_test = y[nTrain+nVal:]
```

:::
:::

::: columns
::: {.column width="33%"}

```{python}
#| echo: false
print("Training dataset")
for i in range(X_train.shape[0]):
    print([int(x) for x in X_train[i]], int(y_train[i]))
```

:::
::: {.column width="33%"}

```{python}
#| echo: false
print("Validation dataset")
for i in range(X_val.shape[0]):
    print([int(x) for x in X_val[i]], int(y_val[i]))
```

:::
::: {.column width="33%"}

```{python}
#| echo: false
print("Test dataset")
for i in range(X_test.shape[0]):
    print([int(x) for x in X_test[i]], int(y_test[i]))
```

:::
:::

# Predicting Sydney House Prices {visibility="uncounted"}

## Creating dataset objects

::: columns
::: column
```{python}
# Num. of input time series.
num_ts = changes.shape[1]

# How many prev. months to use.
seq_length = 6

# Predict the next month ahead.
ahead = 1

# The index of the first target.
delay = (seq_length+ahead-1)
```
:::
::: column
```{python}
# Which suburb to predict.
target_suburb = changes["Sydney"]

train_ds = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=target_suburb[delay:],
    sequence_length=seq_length,
    end_index=num_train)
```
:::
:::

::: columns
::: column
```{python}
val_ds = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=target_suburb[delay:],
    sequence_length=seq_length,
    start_index=num_train,
    end_index=num_train+num_val)
```
:::
::: column
```{python}
test_ds = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=target_suburb[delay:],
    sequence_length=seq_length,
    start_index=num_train+num_val)
```
:::
:::

## Converting `Dataset` to numpy

The `Dataset` object can be handed to Keras directly, but if we really need a numpy array, we can run:
```{python}
X_train = np.concatenate(list(train_ds.map(lambda x, y: x)))
y_train = np.concatenate(list(train_ds.map(lambda x, y: y)))
```

The shape of our training set is now:

```{python}
X_train.shape
```

```{python}
y_train.shape
```

Later, we need the targets as numpy arrays:

```{python}
y_train = np.concatenate(list(train_ds.map(lambda x, y: y)))
y_val = np.concatenate(list(val_ds.map(lambda x, y: y)))
y_test = np.concatenate(list(test_ds.map(lambda x, y: y)))
```

## A dense network

```{python}
from keras.layers import Input, Flatten
random.seed(1)
model_dense = Sequential([
    Input((seq_length, num_ts)),
    Flatten(),
    Dense(50, activation="leaky_relu"),
    Dense(20, activation="leaky_relu"),
    Dense(1, activation="linear")
])
model_dense.compile(loss="mse", optimizer="adam")
print(f"This model has {model_dense.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = model_dense.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0);
```

## Plot the model
```{python}
from keras.utils import plot_model
plot_model(model_dense, show_shapes=True)
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_dense.evaluate(val_ds, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = model_dense.predict(val_ds, verbose=0)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="Dense")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

## A `SimpleRNN` layer

```{python}
random.seed(1)

model_simple = Sequential([
    Input((seq_length, num_ts)),
    SimpleRNN(50),
    Dense(1, activation="linear")
])
model_simple.compile(loss="mse", optimizer="adam")
print(f"This model has {model_simple.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = model_simple.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0);
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_simple.evaluate(val_ds, verbose=0)
```

## Plot the model
```{python}
plot_model(model_simple, show_shapes=True)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = model_simple.predict(val_ds, verbose=0)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="SimpleRNN")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

## A `LSTM` layer

```{python}
from keras.layers import LSTM

random.seed(1)

model_lstm = Sequential([
    Input((seq_length, num_ts)),
    LSTM(50),
    Dense(1, activation="linear")
])

model_lstm.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = model_lstm.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0);
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_lstm.evaluate(val_ds, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = model_lstm.predict(val_ds, verbose=0)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="LSTM")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

## A `GRU` layer

```{python}
from keras.layers import GRU

random.seed(1)

model_gru = Sequential([
    Input((seq_length, num_ts)),
    GRU(50),
    Dense(1, activation="linear")
])

model_gru.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = model_gru.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_gru.evaluate(val_ds, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = model_gru.predict(val_ds, verbose=0)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="GRU")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

## Two `GRU` layers

```{python}
random.seed(1)

model_two_grus = Sequential([
    Input((seq_length, num_ts)),
    GRU(50, return_sequences=True),
    GRU(50),
    Dense(1, activation="linear")
])

model_two_grus.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = model_two_grus.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_two_grus.evaluate(val_ds, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = model_two_grus.predict(val_ds, verbose=0)
# plt.scatter(y_val, y_pred)
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="2 GRUs")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

## Compare the models

```{python}
#| echo: false
models = [model_dense, model_simple, model_lstm, model_gru, model_two_grus]
model_names = ["Dense", "SimpleRNN", "LSTM", "GRU", "2 GRUs"]
mse_val = {name: model.evaluate(val_ds, verbose=0) for name, model in zip(model_names, models)}
val_results = pd.DataFrame({
    "Model": mse_val.keys(), "MSE": mse_val.values()
})
val_results.sort_values("MSE", ascending=False)
```

The network with two GRU layers is the best. 

```{python}
model_two_grus.evaluate(test_ds, verbose=0)
```

## Test set

```{python}
#| echo: false
y_pred = model_two_grus.predict(test_ds, verbose=0)
# plt.scatter(y_test, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_test, label="Sydney")
plt.plot(y_pred, label="2 GRU")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

# Predicting Multiple Time Series {visibility="uncounted"}

## Creating dataset objects

::: columns
::: column
Change the `targets` argument to include all the suburbs.
:::
::: column
```{python}
train_ds = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seq_length,
    end_index=num_train)
```
:::
:::

::: columns
::: column
```{python}
val_ds = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seq_length,
    start_index=num_train,
    end_index=num_train+num_val)
```
:::
::: column
```{python}
test_ds = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seq_length,
    start_index=num_train+num_val)
```
:::
:::

## Converting `Dataset` to numpy

The shape of our training set is now:

```{python}
X_train = np.concatenate(list(train_ds.map(lambda x, y: x)))
X_train.shape
```

```{python}
Y_train = np.concatenate(list(train_ds.map(lambda x, y: y)))
Y_train.shape
```

Later, we need the targets as numpy arrays:

```{python}
Y_train = np.concatenate(list(train_ds.map(lambda x, y: y)))
Y_val = np.concatenate(list(val_ds.map(lambda x, y: y)))
Y_test = np.concatenate(list(test_ds.map(lambda x, y: y)))
```

## A dense network

```{python}
random.seed(1)
model_dense = Sequential([
    Input((seq_length, num_ts)),
    Flatten(),
    Dense(50, activation="leaky_relu"),
    Dense(20, activation="leaky_relu"),
    Dense(num_ts, activation="linear")
])
model_dense.compile(loss="mse", optimizer="adam")
print(f"This model has {model_dense.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = model_dense.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0);
```

## Plot the model
```{python}
plot_model(model_dense, show_shapes=True)
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_dense.evaluate(val_ds, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = model_dense.predict(val_ds, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="Dense")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="Dense")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="Dense")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```
:::
:::



## A `SimpleRNN` layer

```{python}
random.seed(1)

model_simple = Sequential([
    Input((seq_length, num_ts)),
    SimpleRNN(50),
    Dense(num_ts, activation="linear")
])
model_simple.compile(loss="mse", optimizer="adam")
print(f"This model has {model_simple.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = model_simple.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0);
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_simple.evaluate(val_ds, verbose=0)
```

## Plot the model
```{python}
plot_model(model_simple, show_shapes=True)
```


## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = model_simple.predict(val_ds, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="SimpleRNN")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="SimpleRNN")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="SimpleRNN")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```
:::
:::


## A `LSTM` layer

```{python}
random.seed(1)

model_lstm = Sequential([
    Input((seq_length, num_ts)),
    LSTM(50),
    Dense(num_ts, activation="linear")
])

model_lstm.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = model_lstm.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0);
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_lstm.evaluate(val_ds, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = model_lstm.predict(val_ds, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="LSTM")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="LSTM")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="LSTM")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```
:::
:::

## A `GRU` layer

```{python}
random.seed(1)

model_gru = Sequential([
    Input((seq_length, num_ts)),
    GRU(50),
    Dense(num_ts, activation="linear")
])

model_gru.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = model_gru.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_gru.evaluate(val_ds, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = model_gru.predict(val_ds, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="GRU")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="GRU")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="GRU")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```
:::
:::

## Two `GRU` layers

```{python}
random.seed(1)

model_two_grus = Sequential([
    Input((seq_length, num_ts)),
    GRU(50, return_sequences=True),
    GRU(50),
    Dense(num_ts, activation="linear")
])

model_two_grus.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = model_two_grus.fit(train_ds, epochs=1_000, \
  validation_data=val_ds, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.xlabel("Epoch")
plt.ylabel("Loss")
plt.legend(frameon=False);
```

```{python}
model_two_grus.evaluate(val_ds, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = model_two_grus.predict(val_ds, verbose=0)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="2 GRUs")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="2 GRUs")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="2 GRUs")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```
:::
:::

## Compare the models

```{python}
#| echo: false
models = [model_dense, model_simple, model_lstm, model_gru, model_two_grus]
model_names = ["Dense", "SimpleRNN", "LSTM", "GRU", "2 GRUs"]
mse_val = {name: model.evaluate(val_ds, verbose=0) for name, model in zip(model_names, models)}
val_results = pd.DataFrame({
    "Model": mse_val.keys(), "MSE": mse_val.values()
})
val_results.sort_values("MSE", ascending=False)
```

The network with an LSTM layer is the best. 

```{python}
model_lstm.evaluate(test_ds, verbose=0)
```

## Test set

::: columns
::: column
```{python}
#| echo: false
Y_pred = model_lstm.predict(test_ds, verbose=0)
plt.scatter(Y_test, Y_pred)
add_diagonal_line()
plt.xlabel("Actual")
plt.ylabel("Predicted")
plt.show()

plt.plot(Y_test[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="GRU")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_test[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="GRU")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_test[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="GRU")
plt.xlabel("Time")
plt.ylabel("Change in HPI (%)")
plt.legend(frameon=False);
```
:::
:::

# Appendix {data-visibility="uncounted"}

## Package Versions {data-visibility="uncounted"}

```{python}
from watermark import watermark
print(watermark(python=True, packages="keras,matplotlib,numpy,pandas,seaborn,scipy,torch,tensorflow,tf_keras"))
```

## Glossary {data-visibility="uncounted"}

- GRU
- LSTM
- recurrent neural networks
- SimpleRNN

```{python}
#| echo: false
from pathlib import Path
Path("model.png").unlink()
```