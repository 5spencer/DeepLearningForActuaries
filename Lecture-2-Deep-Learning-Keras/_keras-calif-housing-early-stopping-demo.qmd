# Validation Sets {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

```{python}
#| echo: false
from sklearn.metrics import mean_squared_error
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.datasets import fetch_california_housing
features, target = fetch_california_housing(as_frame=True, return_X_y=True)

mseTrainPrev = {'Linear Regression': 0.5205522163645129,
 'Basic ANN': 0.8213997980052227,
 'Long run ANN': 0.6608583727274803,
 'Exp ANN': 0.3220443619821428}

mseTestPrev = {'Linear Regression': 0.5411287478470694,
 'Basic ANN': 0.7963672291024868,
 'Long run ANN': 0.6279579206481324,
 'Exp ANN': 0.3295336955821915}
```

## California housing dataset {data-visibility="uncounted"}

```{python}
features.sample(3, random_state=4)
```

```{python}
target.sample(3, random_state=4)
```

## Questions to answer in ML project

<br>

You fit a few models to the training set, then ask:

<br>

1. __(Selection)__ Which of these models is the best?
2. __(Future Performance)__ How good should we expect the final model to be on unseen data?

## Basic ML workflow

![Splitting the data.](wiki-ML_dataset_training_validation_test_sets.png)

1. For each model, fit it to the _training set_.
2. Compute the error for each model on the _validation set_.
3. Select the model with the lowest validation error.
4. Compute the error of the final model on the _test set_.

::: footer
Source: [Wikipedia](https://commons.wikimedia.org/wiki/File:ML_dataset_training_validation_test_sets.png#filelinks).
:::

## _Diviser en trois_ (split three ways)

<br>

```{python}
# Thanks https://datascience.stackexchange.com/a/15136
X_main, X_test, y_main, y_test = \
    train_test_split(features, target, test_size=0.2, random_state=1)

# As 0.25 x 0.8 = 0.2
X_train, X_val, y_train, y_val = \
    train_test_split(X_main, y_main, test_size=0.25, random_state=1)

X_train.shape, X_val.shape, X_test.shape
```

```{python}
#| echo: false
NUM_FEATURES = len(features.columns)

from sklearn.linear_model import LinearRegression
lr = LinearRegression()
lr.fit(X_train, y_train);

mseTrain = {"Linear Regression": mean_squared_error(y_train, lr.predict(X_train))}
mseVal = {"Linear Regression": mean_squared_error(y_val, lr.predict(X_val))}

random.seed(123)

model = Sequential([
    Dense(30, activation="relu"),
    Dense(1)
])
model.compile("adam", "mse")

model.fit(X_train, y_train, epochs=5, verbose=False)

mseTrain["Basic ANN"] = model.evaluate(X_train, y_train, verbose=False)
mseVal["Basic ANN"] = model.evaluate(X_val, y_val, verbose=False)

random.seed(123)

model = Sequential([
    Dense(30, activation="relu"),
    Dense(1)
])
model.compile("adam", "mse")

model.fit(X_train, y_train, \
    epochs=50, verbose=False)

mseTrain["Long run ANN"] = model.evaluate(X_train, y_train, verbose=False)
mseVal["Long run ANN"] = model.evaluate(X_val, y_val, verbose=False)
```

## Retrain last week's models

... on the new train set (just showing the last one here).

```{python}
sc = StandardScaler()
sc.fit(X_train)
X_train_sc = sc.transform(X_train)
X_val_sc = sc.transform(X_val)
X_test_sc = sc.transform(X_test)

random.seed(123)
model = Sequential([
    Dense(30, activation="relu"),
    Dense(1, activation="exponential")
])
model.compile("adam", "mse")
%time hist = model.fit(X_train_sc, y_train, epochs=50, verbose=False)

mseTrain["Exp ANN"] = mean_squared_error(y_train, model.predict(X_train_sc, verbose=0))
mseVal["Exp ANN"] = mean_squared_error(y_val, model.predict(X_val_sc, verbose=0))
```

## Comparing on Week 2's ~~test~~ val set {data-visibility="uncounted"}


```{python}
#| echo: false
testResults = pd.DataFrame({"Model": mseTestPrev.keys(), "MSE": mseTestPrev.values()})
testResults.sort_values("MSE", ascending=False)
```

## Comparing on validation set {data-visibility="uncounted"}


```{python}
#| echo: false
valResults = pd.DataFrame({"Model": mseVal.keys(), "MSE": mseVal.values()})
valResults.sort_values("MSE", ascending=False)
```

::: fragment
Evaluate _only the final/selected model_ on the test set.

```{python}
mean_squared_error(y_test, model.predict(X_test_sc, verbose=0))
```

```{python}
model.evaluate(X_test_sc, y_test, verbose=False)
```
:::

## Why not use test set for both?

_Thought experiment_: have $m$ classifiers: $f_1(\mathbf{x})$, $\dots$, $f_m(\mathbf{x})$.

They are just as good as each other in the long run
$$
\mathbb{P}(\, f_i(\mathbf{X}) = Y \,)\ =\ 90\% , \quad \text{for } i=1,\dots,m .
$$

::: columns
::: {.column width="40%"}
Evaluate each model on the test set, some will be better than others.

:::
::: {.column width="60%"}
```{python}
#| echo: false
# set_square_figures()
np.random.seed(123)
m = 50
x = np.random.normal(loc=0.9, scale=0.03, size=m)
sns.distplot(x)
plt.scatter(x, np.zeros_like(x))
plt.xlabel("Accuracy of each model on test set")
plt.axvline(0.9, ls='--', c='k');
plt.axvline(np.max(x), ls='--', c='r');
plt.tight_layout()
# set_rectangular_figures()
```
:::
:::

Take the best, you'd think it has $\approx 98\%$ accuracy!

# Early Stopping {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Choosing when to stop training

![Illustrative loss curves over time.](heaton-error-over-time.png)

::: footer
Source: Heaton (2022), [Applications of Deep Learning](https://github.com/jeffheaton/t81_558_deep_learning/blob/master/t81_558_class_03_4_early_stop.ipynb), Part 3.4: Early Stopping.
:::

## Try early stopping

Hinton calls it a "beautiful free lunch"

```{python}
#| code-line-numbers: "|1,10,13"
from tensorflow.keras.callbacks import EarlyStopping

random.seed(123)
model = Sequential([
    Dense(30, activation="relu"),
    Dense(1, activation="exponential")
])
model.compile("adam", "mse")

es = EarlyStopping(restore_best_weights=True, patience=10)

%time hist = model.fit(X_train_sc, y_train, epochs=1_000, \
    callbacks=[es], validation_data=(X_val_sc, y_val), verbose=False)
print(f"Keeping model at epoch #{len(hist.history['loss'])-10}.")
```

## Loss curve

```{python}
plt.plot(hist.history["loss"])
plt.plot(hist.history["val_loss"])
plt.legend(["Training", "Validation"]);
```

## Predictions

```{python}
#| echo: false
set_square_figures()
```

::: columns
::: column
```{python}
#| echo: false
y_pred = model.predict(X_train_sc, verbose=0)
plt.scatter(y_pred, y_train)
plt.xlabel("Predictions")
plt.ylabel("True values")
plt.title("Training set")
add_diagonal_line()
```
:::
::: column
```{python}
#| echo: false
y_pred = model.predict(X_val_sc, verbose=0)
plt.scatter(y_pred, y_val)
plt.xlabel("Predictions")
plt.ylabel("True values")
plt.title("Validation set")
add_diagonal_line()
```
:::
:::

```{python}
#| echo: false
set_rectangular_figures()
```

```{python}
#| echo: false
# Store the results
mseTrain["Early stop ANN"] = model.evaluate(X_train_sc, y_train, verbose=False)
mseVal["Early stop ANN"] = model.evaluate(X_val_sc, y_val, verbose=False)
```

## Comparing models (validation set) {data-visibility="uncounted"}

<br>

```{python}
#| echo: false
valResults = pd.DataFrame({"Model": mseVal.keys(), "MSE": mseVal.values()})
valResults.sort_values("MSE", ascending=False)
```

## Other callbacks

```{python}
from tensorflow.keras.callbacks import ModelCheckpoint

random.seed(123)
model = Sequential([
    Dense(30, activation="relu"),
    Dense(1, activation="exponential")
])
model.compile("adam", "mse")

# On Colab, save models to Google Drive.
mc = ModelCheckpoint("best-model.h5", monitor="val_loss",
        save_best_only=True)
es = EarlyStopping(restore_best_weights=True, patience=5)

hist = model.fit(X_train_sc, y_train, epochs=100, \
    validation_split=0.1, callbacks=[mc, es], verbose=False)

from pathlib import Path
Path("best-model.h5").stat().st_size
```

```{python}
#| echo: false
Path("best-model.h5").unlink()
```