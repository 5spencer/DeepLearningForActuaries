---
title: Deep Learning with Keras
subtitle: ACTL3143 & ACTL5111 Deep Learning for Actuaries
author: Dr Patrick Laub
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    footer: "Slides: [Dr Patrick Laub](https://pat-laub.github.io) (@PatrickLaub)."
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    transition: none
    slide-number: c/t
    strip-comments: true
    preview-links: false
    margin: 0.12
    width: 1000
    chalkboard:
      boardmarker-width: 6
      grid: false
      background:
        - "rgba(255,255,255,0.0)"
        - "https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"
    include-before: <div class="line right"></div>
    include-after: <script>registerRevealCallbacks();</script>
highlight-style: breeze
jupyter: python3
execute:
  keep-ipynb: true
  echo: true
---

```{python}
#| echo: false
import matplotlib
import matplotlib.pyplot as plt
import cycler
colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  plt.rcParams['figure.figsize'] = (2.0, 2.0)

def set_rectangular_figures():
  plt.rcParams['figure.figsize'] = (5.0, 2.0)

set_rectangular_figures()
plt.rcParams['figure.dpi'] = 350
plt.rcParams['savefig.bbox'] = "tight"
plt.rcParams['font.family'] = "serif"

plt.rcParams['axes.spines.right'] = False
plt.rcParams['axes.spines.top'] = False

import pandas as pandas
pandas.options.display.max_rows = 6

import tensorflow
tensorflow.random.set_seed(1)
tensorflow.get_logger().setLevel('ERROR')
tensorflow.config.set_visible_devices([], 'GPU')

def squareFig():
    return plt.figure(figsize=(2, 2), dpi=350).gca()

def add_diagonal_line():
    xl = plt.xlim()
    yl = plt.ylim()
    shortestSide = min(xl[1], yl[1])
    plt.plot([0, shortestSide], [0, shortestSide], color="black", linestyle="--")


import pandas as pd
pd.options.display.max_rows = 6

from tensorflow.random import set_seed

set_seed(1)

import tensorflow as tf
tf.get_logger().setLevel('ERROR')

tf.config.set_visible_devices([], 'GPU')
```

# Deep Learning {background-image="unsw-yellow-shape.png" background-size="85%"}

![Meditation + books = Deep learning.](woman-sitting-lotus-position-books-clouds_53562-9473.jpeg)

::: footer
Source: Freepik, [Woman sitting in lotus position on the books above the clouds](https://img.freepik.com/free-vector/woman-sitting-lotus-position-books-clouds_53562-9473.jpg?w=1480).
:::

## How do real neurons work?

<iframe width="1000" height="600" src="https://www.youtube.com/embed/6qS83wD29PY" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen></iframe>

## A neuron 'firing'

```{python}
#| echo: false
import numpy as np

x = [-5, -1e-10, 0, 5]
y = [x_i >= 0 for x_i in x]
    
plt.plot(x, y)

# Annotate the top-left corner of the plot with the name
# of the activation function
plt.annotate("Step function", xy=(0.2, 1), xycoords='axes fraction',
            xytext=(-5, -5), textcoords='offset points',
            ha='left', va='top')

plt.xlabel("Input")
plt.ylabel("Output");
```

## An artificial neuron 

![A neuron in a neural network with step function activation.](Geron-mls2_1004-blur.png)

::: footer
Source: Aur√©lien G√©ron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Fig. 10-04
:::

## One neuron

::: columns
::: column
$$ \begin{aligned}
  z~=~&x_1 \times w_1 + \\
    &x_2 \times w_2 + \\
    &x_3 \times w_3 . 
  \end{aligned}
$$

$$ \begin{aligned}
  a &= \text{step}(z) \\
    &= \begin{cases}
    1 & \text{if } z > 0 \\
    0 & \text{if } z \leq 0
    \end{cases}
  \end{aligned}
$$
:::
::: column
![A neuron in a neural network with step function activation.](Geron-mls2_1004-blur.png)
:::
:::

Here, $x_1$, $x_2$, $x_3$ is just some fixed data.

The weights $w_1$, $w_2$, $w_3$ should be 'learned'.

::: footer
Source: Aur√©lien G√©ron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Fig. 10-04.
:::

## One neuron with bias

::: columns
::: column
$$ \begin{aligned}
  z~=~&x_1 \times w_1 + \\
    &x_2 \times w_2 + \\
    &x_3 \times w_3 + b .
  \end{aligned}
$$

$$ \begin{aligned}
  a &= \text{step}(z) \\
    &= \begin{cases}
    1 & \text{if } z > 0 \\
    0 & \text{if } z \leq 0
    \end{cases}
  \end{aligned}
$$
:::
::: column
![A neuron in a neural network with step function activation.](Geron-mls2_1004-blur.png)
:::
:::

See [interactive plot](bias-activation-function.html).

The weights $w_1$, $w_2$, $w_3$ and bias $b$ should be 'learned'.

::: footer
Source: Aur√©lien G√©ron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Fig. 10-04.
:::

## A basic neural network

![A basic fully-connected/dense network with one hidden layer.](Geron-mls2_1007-blur.png)

::: footer
Source: Aur√©lien G√©ron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Fig. 10-07
:::

## Step-function activation

### Perceptrons

Brains and computers are binary, so make a perceptron with binary data.
Seemed like a reasonable idea.

### Modern neural network

Replace binary state with continuous state.
Invent fast training algorithm.
Still slow and tedious to create without open source libraries.


## Try different activation functions

<br>

```{python}
#| echo: false
import numpy as np

def plot_activation(x, y, ax, name, hideX=False):
    ax.plot(x, y)
    if y.min() < 0:
        ax.axhline(0, ls='--', c='black', lw=0.5)

    # Annotate the top-left corner of the subplot with the name
    # of the activation function
    ax.annotate(name, xy=(0.2, 1), xycoords='axes fraction',
                xytext=(-5, -5), textcoords='offset points',
                ha='left', va='top')
    
    if hideX:
        ax.xaxis.set_visible(False)

x = np.linspace(-5, 5, 500)

fig, axs = plt.subplots(2, 2)
y = x
plot_activation(x, y, axs[0,0], "Linear", hideX=True)
y = x > 0
plot_activation(x, y, axs[0,1], "Step", hideX=True)
y = np.tanh(x)
plot_activation(x, y, axs[1,0], "tanh")
y = x * (x > 0)
plot_activation(x, y, axs[1,1], "ReLU")

axs[0,0].set(ylabel="Output")
axs[1,0].set(xlabel="Input", ylabel="Output")
axs[1,1].set(xlabel="Input");
```

## Flexible

![](universal-approximator.png)

::: footer
Source: Shan-Hung Wu (2022), [_CS565600 Deep Learning Lecture Slides_](https://nthu-datalab.github.io/ml/slides/10_NN_Design.pdf), Lecture 10 Slide 44.
:::

## Feature engineering

::: columns
::: column
![](krohn_f02_01-blur.png)
:::
::: column
![](krohn_f01_12-blur.png)
:::
:::

::: footer
Source: Krohn (2019), _Deep Learning Illustrated: A Visual, Interactive Guide to Artificial Intelligence_.
:::

## Example: Facial recognition

![Example of manual feature engineering.](facial-recognition.jpeg)

::: footer
Source: Fenjiro (2019), [_Face Id: Deep Learning for Face Recognition_](https://medium.com/@fenjiro/face-id-deep-learning-for-face-recognition-324b50d916d1), Medium.
:::

## What are Keras and TensorFlow?

Keras is common way of specifying, training, and using neural networks.
It gives a simple interface to _various backend_ libraries, including Tensorflow. 

![Keras as a independent interface, and Keras as part of Tensorflow.](Geron-mls2_1010-blur.png)

:::footer
Source: Aur√©lien G√©ron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 10-10.
:::

## The deep learning hammer

Deep learning is not always the answer!

![The map of data science.](data-science-map.png)

::: footer
Source: Serge Masis (2022), [LinkedIn post](https://www.serg.ai).
:::

## Quiz

In this ANN, how many of the following are there:

::: columns
::: {.column width="32%"}

- features,
- targets,
- weights,
- biases, and
- parameters?

What is the depth?

:::
::: {.column width="68%"}
![An artificial neural network.](neural-network-circles.png)
:::
:::

::: footer
Source: Dertat (2017), [_Applied Deep Learning - Part 1: Artificial Neural Networks_](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6), Medium.
:::

{{< include _keras-calif-housing-regression-demo.qmd >}}

{{< include _keras-calif-housing-early-stopping-demo.qmd >}}

{{< include _keras-iris-classification-demo.qmd >}}


# Quiz {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

```{python}
#| echo: false
import numpy

numpy.random.seed(123)
x = numpy.random.rand(100)
y = 1_000 * numpy.random.rand(100)
z = 2*x + 0.001 * y + 1 + 1 * numpy.random.rand(100)

# X = np.c_[x, y]
# features = X
features = pandas.DataFrame({"$x_1$": x, "$x_2$": y})
targets = z
```

## Critique this üí© regression code

```{python}
#| echo: false
from tensorflow.keras.callbacks import EarlyStopping
tf.get_logger().setLevel('ERROR')
```

```{python}
X_train = features[:80]; X_test = features[81:]
y_train = targets[:80]; y_test = targets[81:]
```
```{python}
model = Sequential([
  Dense(32, activation='ReLU', input_dim=2),
   Dense(32, activation='ReLU', input_dim=2),
  Dense(1, activation='sigmoid')
])
model.compile(optimizer="adam", loss='mse')
es = EarlyStopping(patience=10)
fitted_model = model.fit(X_train, y_train, epochs=5,
  callbacks=[es], verbose=False)
```
```{python}
trainMAE = model.evaluate(X_train, y_train, verbose=False)
hist = model.fit(X_test, y_test, epochs=5,
  callbacks=[es], verbose=False)
hist.history["loss"]
testMAE = model.evaluate(X_test, y_test, verbose=False)
```

```{python}
f"Train MAE: {testMAE:.2f} Test MAE: {trainMAE:.2f}"
```

## The data

```{python}
#| echo: false
set_square_figures()
```

::: columns
::: column
```{python}
sns.scatterplot(
  x="$x_1$", y="$x_2$",
  c=targets, data=features);
```
:::
::: column
```{python}
sns.distplot(targets);
```

:::
:::

```{python}
#| echo: false
set_rectangular_figures()
```

## With warnings enabled

```{python}
#| echo: false
tf.get_logger().setLevel("WARN")
```

```{python}
model.fit(X_train, y_train, epochs=5,
  callbacks=[es], verbose=False);
```

```{python}
es = EarlyStopping(restore_best_weights=True, patience=10)
model.fit(X_train, y_train, epochs=5, validation_split=0.1,
  callbacks=[es], verbose=False);
```

## Later `input_dim` ignored

```{python}
#| echo: false
def skip_empty(line):
  if line.strip() != "":
    print(line.strip())
```

```{python}
model = Sequential([
  Dense(32, activation='ReLU', input_dim=2),
  Dense(32, activation='ReLU', input_dim=2),
  Dense(1, activation='ReLU')
])

model.compile(optimizer='adam', loss='mse')
model.summary(print_fn=skip_empty)
```

## Later `input_dim` ignored {data-visibility="uncounted"}

```{python}
model = Sequential([
  Dense(32, activation='ReLU', input_dim=2),
  Dense(32, activation='ReLU'),
  Dense(1, activation='ReLU')
])

model.compile(optimizer='adam', loss='mse')
model.summary(print_fn=skip_empty)
```

## Later `input_dim` ignored {data-visibility="uncounted"}

```{python}
model = Sequential([
  Dense(32, activation='ReLU'),
  Dense(32, activation='ReLU'),
  Dense(1, activation='ReLU')
])
model.compile(optimizer='adam', loss='mse')
model.fit(X_train, y_train, epochs=5, verbose=False)
model.summary(print_fn=skip_empty)
```

{{< include _sw2-poisson-regression.qmd >}}

{{< include _stroke-prediction.qmd >}}

# {.smaller data-visibility="uncounted"}

<h2>Glossary</h2>

::: columns
::: column
- activations, activation function
- artificial intelligence vs machine learning
- artificial neural network
- biases (in neurons)
- callbacks
- classification problem
- cost/loss function
- deep network, network depth
- dense or fully-connected layer
- early stopping
- epoch
- feed-forward neural network
- Keras, Tensorflow, PyTorch
:::
::: column
- labelled/unlabelled data
- machine learning
- matplotlib, seaborn
- neural network architecture
- perceptron
- ReLU
- representation learning
- sigmoid activation function
- targets
- training/test split
- weights (in a neuron)
- validation set
:::
:::

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
