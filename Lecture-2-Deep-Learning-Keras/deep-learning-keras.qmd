---
title: Deep Learning with Keras
---

```{python}
#| echo: false
import matplotlib

# TDOO: Update following section
import matplotlib.pyplot as plt
import cycler

colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)


def set_square_figures():
    plt.rcParams["figure.figsize"] = (2.0, 2.0)


def set_rectangular_figures():
    plt.rcParams["figure.figsize"] = (5.0, 2.0)


set_rectangular_figures()
plt.rcParams["figure.dpi"] = 350
plt.rcParams["savefig.bbox"] = "tight"
plt.rcParams["font.family"] = "serif"

plt.rcParams["axes.spines.right"] = False
plt.rcParams["axes.spines.top"] = False


def square_fig():
    return plt.figure(figsize=(2, 2), dpi=350).gca()


def add_diagonal_line():
    xl = plt.xlim()
    yl = plt.ylim()
    shortestSide = min(xl[1], yl[1])
    plt.plot(
        [0, shortestSide],
        [0, shortestSide],
        color="black",
        linestyle="--",
    )


import pandas as pandas

pandas.options.display.max_rows = 6

import random

random.seed(1234)

import tensorflow

tensorflow.random.set_seed(1)
tensorflow.get_logger().setLevel("ERROR")
tensorflow.config.set_visible_devices([], "GPU")
```

## Lecture Outline

- Neural networks
- Regression demo with Keras

# Deep Learning {background-size="85%"}

![Meditation + books = Deep learning.](woman-sitting-lotus-position-books-clouds_53562-9473.jpeg)

::: footer
Source: Freepik, [Woman sitting in lotus position on the books above the clouds](https://img.freepik.com/free-vector/woman-sitting-lotus-position-books-clouds_53562-9473.jpg?w=1480).
:::

## How do real neurons work?

{{< video https://www.youtube.com/embed/6qS83wD29PY width="100%" height="80%" >}}

## A neuron 'firing'

```{python}
#| echo: false
import numpy as np

x = [-5, -1e-10, 0, 5]
y = [x_i >= 0 for x_i in x]

plt.plot(x, y)

# Annotate the top-left corner of the plot with the name
# of the activation function
plt.annotate(
    "Step function",
    xy=(0.2, 1),
    xycoords="axes fraction",
    xytext=(-5, -5),
    textcoords="offset points",
    ha="left",
    va="top",
)

plt.xlabel("Input")
plt.ylabel("Output");
```

## An artificial neuron 

![A neuron in a neural network with a ReLU activation.](single-neuron.png)

::: footer
Source: Marcus Lautier (2022).
:::

## One neuron

::: columns
::: {.column width="55%"}
$$ \begin{aligned}
  z~=~&x_1 \times w_1 + \\
    &x_2 \times w_2 + \\
    &x_3 \times w_3 . 
  \end{aligned}
$$

$$
  a = \begin{cases}
    z & \text{if } z > 0 \\
    0 & \text{if } z \leq 0
    \end{cases}
$$

Here, $x_1$, $x_2$, $x_3$ is just some fixed data.

:::
::: {.column width="45%"}
![A neuron in a neural network with a ReLU activation.](single-neuron.png)
:::
:::

The weights $w_1$, $w_2$, $w_3$ should be 'learned'.

::: footer
Source: Marcus Lautier (2022).
:::

## One neuron with bias

::: columns
::: {.column width="55%"}
$$ \begin{aligned}
  z~=~&x_1 \times w_1 + \\
    &x_2 \times w_2 + \\
    &x_3 \times w_3 + b .
  \end{aligned}
$$

$$
  a = \begin{cases}
    z & \text{if } z > 0 \\
    0 & \text{if } z \leq 0
    \end{cases}
$$

The weights $w_1$, $w_2$, $w_3$ and bias $b$ should be 'learned'.
:::
::: {.column width="45%"}

::: {.panel-tabset}

### Bias = -4

```{python}
#| echo: false
def plot_relu_with_bias(bias):
    xs = np.linspace(-10, 10, 1_000)
    xs_with_bias = xs + bias
    ys = xs_with_bias * (xs_with_bias >= 0)

    square_fig().plot(xs, ys)
    plt.xlim([-5, 5])
    plt.ylim([-1, 7])
    plt.xlabel("Weighted Sum Input")
    plt.ylabel("Output")


plot_relu_with_bias(-4)
```

### 0

```{python}
#| echo: false
plot_relu_with_bias(0) 
```

### 4

```{python}
#| echo: false
plot_relu_with_bias(4) 
```

:::
:::
:::

## A basic neural network

![A basic fully-connected/dense network.](basic-neural-network.png)

::: footer
Source: Marcus Lautier (2022).
:::

## Step-function activation

### Perceptrons

Brains and computers are binary, so make a perceptron with binary data.
Seemed reasonable, impossible to train.

### Modern neural network

Replace binary state with continuous state.
Still rather slow to train.

::: {.callout-note}
It's a neur**al** network made of neur**on**s, not a "neuron network".
:::

## Try different activation functions

```{python}
#| echo: false
import numpy as np


def plot_activation(x, y, ax, name, hideX=False):
    ax.plot(x, y)
    if y.min() < 0:
        ax.axhline(0, ls="--", c="black", lw=0.5)

    # Annotate the top-left corner of the subplot with the name
    # of the activation function
    ax.annotate(
        name,
        xy=(0.2, 1),
        xycoords="axes fraction",
        xytext=(-5, -5),
        textcoords="offset points",
        ha="left",
        va="top",
    )

    if hideX:
        ax.xaxis.set_visible(False)


x = np.linspace(-5, 5, 500)

fig, axs = plt.subplots(2, 2)
y = x
plot_activation(x, y, axs[0, 0], "Linear", hideX=True)
y = x > 0
plot_activation(x, y, axs[0, 1], "Step", hideX=True)
y = np.tanh(x)
plot_activation(x, y, axs[1, 0], "tanh")
y = x * (x > 0)
plot_activation(x, y, axs[1, 1], "ReLU")

axs[0, 0].set(ylabel="Output")
axs[1, 0].set(xlabel="Input", ylabel="Output")
axs[1, 1].set(xlabel="Input");
```

## Flexible

> One can show that an MLP is a **universal approximator**, meaning 
> it can model any suitably smooth function, given enough hidden units,
> to any desired level of accuracy (Hornik 1991). One can either make
> the model be "wide" or "deep"; the latter has some advantages...

::: footer
Source: Murphy (2012), Machine Learning: A Probabilistic Perspective, 1st Ed, p. 566.
:::

## Feature engineering

::: columns
::: {.column width="55%"}
![](feature-engineering.png)
:::
::: {.column width="45%"}
![](facial-recognition.jpeg)
![](modelling-ratio.png)
:::
:::

::: footer
Sources: Marcus Lautier (2022) & Fenjiro (2019), [_Face Id: Deep Learning for Face Recognition_](https://medium.com/@fenjiro/face-id-deep-learning-for-face-recognition-324b50d916d1), Medium.
:::

## The deep learning hammer

Deep learning is not always the answer!

![The map of data science.](data-science-map.png)

::: footer
Source: Serge Masis (2022), [LinkedIn post](https://www.serg.ai).
:::

## Quiz

In this ANN, how many of the following are there:

::: columns
::: {.column width="32%"}

- features,
- targets,
- weights,
- biases, and
- parameters?

What is the depth?

:::
::: {.column width="68%"}
![An artificial neural network.](neural-network-circles.png)
:::
:::

::: footer
Source: Dertat (2017), [_Applied Deep Learning - Part 1: Artificial Neural Networks_](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6), Medium.
:::

{{< include _keras-calif-housing-regression-demo.qmd >}}

{{< include _regression-quiz.qmd >}}

# {.smaller data-visibility="uncounted"}

<h2>Glossary</h2>


::: columns
::: column
- activations, activation function
- artificial neural network
- biases (in neurons)
- callbacks
- classification problem
- cost/loss function
- deep network, network depth
- dense or fully-connected layer
- early stopping
- epoch
- feed-forward neural network
- Keras, Tensorflow, PyTorch
:::
::: column
- labelled/unlabelled data
- machine learning
- matplotlib, seaborn
- neural network architecture
- perceptron
- ReLU
- representation learning
- sigmoid activation function
- targets
- training/test split
- weights (in a neuron)
- validation set
:::
:::

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })
</script>
