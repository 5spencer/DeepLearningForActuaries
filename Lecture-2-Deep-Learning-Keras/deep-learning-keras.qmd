---
title: Deep Learning with Keras
---

```{python}
#| echo: false
import matplotlib

# TDOO: Update following section
import matplotlib.pyplot as plt
import cycler

colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)


def set_square_figures():
    plt.rcParams["figure.figsize"] = (2.0, 2.0)


def set_rectangular_figures():
    plt.rcParams["figure.figsize"] = (5.0, 2.0)


set_rectangular_figures()
plt.rcParams["figure.dpi"] = 350
plt.rcParams["savefig.bbox"] = "tight"
plt.rcParams["font.family"] = "serif"

plt.rcParams["axes.spines.right"] = False
plt.rcParams["axes.spines.top"] = False


def square_fig():
    return plt.figure(figsize=(2, 2), dpi=350).gca()


def add_diagonal_line():
    xl = plt.xlim()
    yl = plt.ylim()
    shortestSide = min(xl[1], yl[1])
    plt.plot(
        [0, shortestSide],
        [0, shortestSide],
        color="black",
        linestyle="--",
    )


import pandas as pandas

pandas.options.display.max_rows = 6

import random

random.seed(1234)

import tensorflow

tensorflow.random.set_seed(1)
tensorflow.get_logger().setLevel("ERROR")
tensorflow.config.set_visible_devices([], "GPU")
```

## Lecture Outline

- Neural networks
- Regression demo with Keras

# Deep Learning {background-size="85%"}

![Meditation + books = Deep learning.](woman-sitting-lotus-position-books-clouds_53562-9473.jpeg)

::: footer
Source: Freepik, [Woman sitting in lotus position on the books above the clouds](https://img.freepik.com/free-vector/woman-sitting-lotus-position-books-clouds_53562-9473.jpg?w=1480).
:::

## How do real neurons work?

{{< video https://www.youtube.com/embed/6qS83wD29PY width="100%" height="80%" >}}

## A neuron 'firing'

::: {.content-visible unless-format="revealjs"}
Similar to a biological neuron, an artificial neuron 'fires' when the combined input information exceeds a certain threshold. This activation can be seen as a step function. The difference is that the artificial neuron uses mathematical rules (e.g. weighted sum) to 'fire' whereas 'firing' in the biological neurons is far more complex and dynamic.
:::

```{python}
#| echo: false
import numpy as np

x = [-5, -1e-10, 0, 5]
y = [x_i >= 0 for x_i in x]

plt.plot(x, y)

# Annotate the top-left corner of the plot with the name
# of the activation function
plt.annotate(
    "Step function",
    xy=(0.2, 1),
    xycoords="axes fraction",
    xytext=(-5, -5),
    textcoords="offset points",
    ha="left",
    va="top",
)

plt.xlabel("Input")
plt.ylabel("Output");
```

## An artificial neuron 

![A neuron in a neural network with a ReLU activation.](single-neuron.png)

::: {.content-visible unless-format="revealjs"}
The figure shows how we first compute the weighted sum of inputs, and then evaluate the summation using the step function. If the weighted sum is greater than the pre-set threshold, the neuron `fires'. 
:::

::: footer
Source: Marcus Lautier (2022).
:::

## One neuron

::: columns
::: {.column width="55%"}
$$ \begin{aligned}
  z~=~&x_1 \times w_1 + \\
    &x_2 \times w_2 + \\
    &x_3 \times w_3 . 
  \end{aligned}
$$

$$
  a = \begin{cases}
    z & \text{if } z > 0 \\
    0 & \text{if } z \leq 0
    \end{cases}
$$

Here, $x_1$, $x_2$, $x_3$ is just some fixed data.

:::
::: {.column width="45%"}
![A neuron in a neural network with a ReLU activation.](single-neuron.png)
:::
:::

The weights $w_1$, $w_2$, $w_3$ should be 'learned'.

::: footer
Source: Marcus Lautier (2022).
:::

## One neuron with bias

::: {.content-visible unless-format="revealjs"} 
The bias is a constant term added to the product of inputs and weights. It helps in shifting the entire activation function to either the negative or positive side. This shifting can either accelerate or delay the activation. For example, if the bias is negative, it will shift the entire curve to the right, making the activation harder. This is similar to delaying the activation.
::: 


::: columns
::: {.column width="55%"}
$$ \begin{aligned}
  z~=~&x_1 \times w_1 + \\
    &x_2 \times w_2 + \\
    &x_3 \times w_3 + b .
  \end{aligned}
$$

$$
  a = \begin{cases}
    z & \text{if } z > 0 \\
    0 & \text{if } z \leq 0
    \end{cases}
$$

The weights $w_1$, $w_2$, $w_3$ and bias $b$ should be 'learned'.
:::
::: {.column width="45%"}

::: {.panel-tabset}

### Bias = -4

```{python}
#| echo: false
def plot_relu_with_bias(bias):
    xs = np.linspace(-10, 10, 1_000)
    xs_with_bias = xs + bias
    ys = xs_with_bias * (xs_with_bias >= 0)

    square_fig().plot(xs, ys)
    plt.xlim([-5, 5])
    plt.ylim([-1, 7])
    plt.xlabel("Weighted Sum Input")
    plt.ylabel("Output")


plot_relu_with_bias(-4)
```

### 0

```{python}
#| echo: false
plot_relu_with_bias(0) 
```

### 4

```{python}
#| echo: false
plot_relu_with_bias(4) 
```

:::
:::
:::

## A basic neural network

![A basic fully-connected/dense network.](basic-neural-network.png)

::: {.content-visible unless-format="revealjs"}
This neural network consists of an input layer with 2 neurons ($x_1, x_2$), an output layer with 3 neurons, and 1 hidden layer with 4 neurons. Since every neuron is linked to every other neuron, this is called a fully connected neural network. Since we have 2 inputs and 1 bias in the input layer, each neuron in the hidden layer has 2+1=3 parameters to learn. Similarly, there are 4 neurons and 1 bias in the hidden layer. Hence, each neuron in the output layer has 4+1=5 parameters to learn.
:::
 
::: footer
Source: Marcus Lautier (2022).
:::

## Step-function activation

### Perceptrons

Brains and computers are binary, so make a perceptron with binary data.
Seemed reasonable, impossible to train.

### Modern neural network

Replace binary state with continuous state.
Still rather slow to train.

::: {.callout-note}
It's a neur**al** network made of neur**on**s, not a "neuron network".
:::

## Try different activation functions

```{python}
#| echo: false
import numpy as np


def plot_activation(x, y, ax, name, hideX=False):
    ax.plot(x, y)
    if y.min() < 0:
        ax.axhline(0, ls="--", c="black", lw=0.5)

    # Annotate the top-left corner of the subplot with the name
    # of the activation function
    ax.annotate(
        name,
        xy=(0.2, 1),
        xycoords="axes fraction",
        xytext=(-5, -5),
        textcoords="offset points",
        ha="left",
        va="top",
    )

    if hideX:
        ax.xaxis.set_visible(False)


x = np.linspace(-5, 5, 500)

fig, axs = plt.subplots(2, 2)
y = x
plot_activation(x, y, axs[0, 0], "Linear", hideX=True)
y = x > 0
plot_activation(x, y, axs[0, 1], "Step", hideX=True)
y = np.tanh(x)
plot_activation(x, y, axs[1, 0], "tanh")
y = x * (x > 0)
plot_activation(x, y, axs[1, 1], "ReLU")

axs[0, 0].set(ylabel="Output")
axs[1, 0].set(xlabel="Input", ylabel="Output")
axs[1, 1].set(xlabel="Input");
```

::: {.content-visible unless-format="revealjs"}
Activation functions are essential for a neural network design. They provide the mathematical rule for 'firing' the neuron. There are many activation functions, and the choice of the activation function depends on the problem we are trying to solve. Note: If we use the 'linear' activation function at every neuron, then the regression learning problem becomes a simple linear regression. But if we use 'ReLu', 'tanh', or any other non-linear function, then, we can introduce non-linearity into the model so that the model can learn complex non-linear patterns in the data. There are activation functions in both the hidden layers and the output layer. The activation function in the hidden layer controls how the neural network learns complex non-linear patterns in the training data. The choice of activation function in the output layer determines the type of predictions we get.  
:::

## Flexible

> One can show that an MLP is a **universal approximator**, meaning 
> it can model any suitably smooth function, given enough hidden units,
> to any desired level of accuracy (Hornik 1991). One can either make
> the model be "wide" or "deep"; the latter has some advantages...

::: footer
Source: Murphy (2012), Machine Learning: A Probabilistic Perspective, 1st Ed, p. 566.
:::

## Feature engineering

::: columns
::: {.column width="55%"}
![](feature-engineering.png)
:::
::: {.column width="45%"}
![](facial-recognition.jpeg)
![](modelling-ratio.png)
:::
:::

::: {.content-visible unless-format="revealjs"}
A major part of traditional machine learning (TML) involves conducting feature engineering to extract relevant features manually. In contrast, representational learning does not involve heavy manual feature engineering, rather, it learns relevant features automatically from data during the task. Therefore, the effort spent on feature engineering in representational learning is minimal compared to TML.
:::

::: footer
Sources: Marcus Lautier (2022) & Fenjiro (2019), [_Face Id: Deep Learning for Face Recognition_](https://medium.com/@fenjiro/face-id-deep-learning-for-face-recognition-324b50d916d1), Medium.
:::

## The deep learning hammer

Deep learning is not always the answer!

![The map of data science.](data-science-map.png)

::: footer
Source: Serge Masis (2022), [LinkedIn post](https://www.serg.ai).
:::

## Quiz

In this ANN, how many of the following are there:

::: columns
::: {.column width="32%"}

- features,
- targets,
- weights,
- biases, and
- parameters?

What is the depth?

::: {.content-visible unless-format="revealjs"}
There are three inputs, hence, three features. There is one neuron in the output layer, hence, one target. There are $3 \times 4 + 4 \times 4 + 4\times 1 = 32$ arrows, hence, there are 32 weights in total. Since there is 1 bias for each neuron, there are 9 biases in total. The number of total parameters to learn equals to the sum of weights and biases, hence, there are $32+9=41$ parameters in total. 
:::

:::
::: {.column width="68%"}
![An artificial neural network.](neural-network-circles.png)
:::
:::

::: footer
Source: Dertat (2017), [_Applied Deep Learning - Part 1: Artificial Neural Networks_](https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6), Medium.
:::

{{< include _keras-calif-housing-regression-demo.qmd >}}

{{< include _regression-quiz.qmd >}}

# {.smaller data-visibility="uncounted"}

<h2>Glossary</h2>


::: columns
::: column
- activations, activation function
- artificial neural network
- biases (in neurons)
- callbacks
- classification problem
- cost/loss function
- deep network, network depth
- dense or fully-connected layer
- early stopping
- epoch
- feed-forward neural network
- Keras, Tensorflow, PyTorch
:::
::: column
- labelled/unlabelled data
- machine learning
- matplotlib, seaborn
- neural network architecture
- perceptron
- ReLU
- representation learning
- sigmoid activation function
- targets
- training/test split
- weights (in a neuron)
- validation set
:::
:::

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })
</script>
