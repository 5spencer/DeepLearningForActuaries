
# Poisson Regression, Deviance & Loss {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## French motor dataset

```{python}
#| echo: false
from sklearn.datasets import fetch_openml

# Download the dataset if it hasn't already been downloaded.
from pathlib import Path
cd = Path("NotInGit")
if (cd / "freq_data.csv").exists():
    freq = pandas.read_csv("NotInGit/freq_data.csv")
else:
    print("Downloading dataset...")
    from sklearn.datasets import fetch_openml
    freq = fetch_openml(data_id=41214, as_frame=True).frame
    cd.mkdir(exist_ok=True)	
    freq.to_csv("NotInGit/freq_data.csv", index=False)

# Remove the column named 'IDpol'.
freq = freq.drop("IDpol", axis=1)

# Convert categorical variables to numeric.
freq = pandas.get_dummies(freq, columns=["VehGas", "Area", "VehBrand", "Region"])
```

```{python}
freq.drop("ClaimNb", axis=1).sample(3, random_state=6)
```

```{python}
freq["ClaimNb"].sample(3, random_state=6)
```

## Where are things defined?

String options in Keras are just conveniences.

```{python}
model = Sequential([
    Dense(30, activation="relu"),
    Dense(1, activation="exponential")
])
```

is the same as

```{python}
from tensorflow.keras.activations import relu, exponential

model = Sequential([
    Dense(30, activation=relu),
    Dense(1, activation=exponential)
])
```

```{python}
x = [-1.0, 0.0, 1.0]
print(relu(x))
print(exponential(x))
```

## String arguments to `.compile`

When we run

```{python}
model.compile(optimizer="adam", loss="poisson")
```

it is equivalent to

```{python}
from tensorflow.keras.losses import poisson
from tensorflow.keras.optimizers import Adam

model.compile(optimizer=Adam(), loss=poisson)
```

Why do this manually? To adjust the object:

```{python}
optimizer = Adam(learning_rate=0.01)
model.compile(optimizer=optimizer, loss="poisson")
```

or to get help.

<!-- Comment on one-hot encodings for the categorical variables! -->

## Asked to use the "poisson" loss

```{python}
help(tf.keras.losses.poisson)
```

## The model

Have $\{ (\mathbf{x}_i, y_i) \}_{i=1, \dots, n}$ for $\mathbf{x}_i \in \mathbb{R}^{47}$ and $y_i \in \mathbb{N}_0$.

Assume the distribution
$$
Y_i \sim \mathsf{Poisson}(\lambda(\mathbf{x}_i))
$$

We have $\mathbb{E} Y_i = \lambda(\mathbf{x}_i)$. 
The NN takes $\mathbf{x}_i$ and predicts $\mathbb{E} Y_i$.

::: {.callout-note}
For insurance, _this is a bit weird_.
The exposures are different for each policy.

$\lambda(\mathbf{x}_i)$ is the expected number of claims for the duration of policy $i$'s contract.

Normally, $\text{Exposure}_i \not\in \mathbf{x}_i$, and $\lambda(\mathbf{x}_i)$ is the expected rate _per year_, then
$$
Y_i \sim \mathsf{Poisson}(\text{Exposure}_i \times \lambda(\mathbf{x}_i)).
$$
:::

## Poisson probabilities

Since the PMF of the $N \sim \mathsf{Poisson}(\lambda)$ distribution is
$\mathbb{P}(N = k) = \frac{\lambda^k \mathrm{e}^{-\lambda}}{k!}$
then the PMF of $Y_i \sim \mathsf{Poisson}(\lambda(\mathbf{x}_i))$ is

$$
\mathbb{P}(Y_i = y_i) = \frac{ \lambda(\mathbf{x}_i)^{y_i} \, \mathrm{e}^{-\lambda(\mathbf{x}_i)} }{y_i!}
$$

The likelihood of a sample is then
$$
\mathbb{P}(Y_1 = y_1, \dots, Y_n = y_n) = \prod_{i=1}^n \mathbb{P}(Y_i = y_i).
$$

## Log-likelihood

Therefore, the likelihood of $\{ (\mathbf{x}_i, y_i) \}_{i=1, \dots, n}$ is

$$
L = \prod_{i=1}^n \frac{ \lambda(\mathbf{x}_i)^{y_i} \, \mathrm{e}^{-\lambda(\mathbf{x}_i)} }{y_i!}
$$

so the log-likelihood is

$$
\begin{aligned}
\ell
&= \sum_{i=1}^n \log \bigl( \frac{ \lambda(\mathbf{x}_i)^{y_i} \, \mathrm{e}^{-\lambda(\mathbf{x}_i)} }{y_i!} \bigr) \\
&= \sum_{i=1}^n y_i \log \bigl( \lambda(\mathbf{x}_i) \bigr) - \lambda(\mathbf{x}_i) - \log(y_i!) .
\end{aligned}
$$

## Maximising the likelihood

Want to find the best NN $\lambda^*$ such that:
$$
\begin{aligned}
\lambda^* 
&= \arg\max_{\lambda} \sum_{i=1}^n y_i \log \bigl( \lambda(\mathbf{x}_i) \bigr) - \lambda(\mathbf{x}_i) - \log(y_i!) \\
&= \arg\max_{\lambda} \sum_{i=1}^n y_i \log \bigl( \lambda(\mathbf{x}_i) \bigr) - \lambda(\mathbf{x}_i) \\
&= \arg\min_{\lambda} \sum_{i=1}^n \lambda(\mathbf{x}_i) - y_i \log \bigl( \lambda(\mathbf{x}_i)\bigr) \\
&= \arg\min_{\lambda} \frac{1}{n} \sum_{i=1}^n \lambda(\mathbf{x}_i) - y_i \log \bigl( \lambda(\mathbf{x}_i)\bigr) .
\end{aligned}
$$

## Keras' "poisson" loss again

```{python}
#| eval: false
help(poisson)
```

```{python}
#| echo: false
print("""Help on function poisson in module keras.losses:

poisson(y_true, y_pred)
    Computes the Poisson loss between y_true and y_pred.
    
    The Poisson loss is the mean of the elements of the `Tensor`
    `y_pred - y_true * log(y_pred)`.
  
...
""")
```

In other words,
$$
\text{PoissonLoss} = \frac{1}{n} \sum_{i=1}^n \lambda(\mathbf{x}_i) - y_i \log \bigl( \lambda(\mathbf{x}_i) \bigr) .
$$

## Poisson deviance

$$
D = 2 \sum_{i=1}^n y_i \log\bigl( \frac{y_i}{\lambda(\mathbf{x}_i)} \bigr) - \bigl( y_i - \lambda(\mathbf{x}_i) \bigr) .
$$

```{python}
from sklearn.metrics import mean_poisson_deviance
y_true = [0, 2, 1]
y_pred = [0.1, 0.9, 0.8]
mean_poisson_deviance(y_true, y_pred)
```

```{python}
deviance = 0
for y_i, yhat_i in zip(y_true, y_pred):
  firstTerm = y_i * np.log(y_i / yhat_i) if y_i > 0 else 0
  deviance += 2 * (firstTerm - (y_i - yhat_i))
meanDeviance = deviance / len(y_true)
deviance, meanDeviance
```

## Poisson deviance as a loss function

Want to find the best NN $\lambda^*$ such that:
$$
\begin{aligned}
\lambda^* 
&= \arg\min_{\lambda} \, 2 \sum_{i=1}^n y_i \log\bigl( \frac{y_i}{\lambda(\mathbf{x}_i)} \bigr) - \bigl( y_i - \lambda(\mathbf{x}_i) \bigr) \\
&= \arg\min_{\lambda} \sum_{i=1}^n y_i \log( y_i ) - y_i \log\bigl( \lambda(\mathbf{x}_i)  \bigr) - y_i + \lambda(\mathbf{x}_i) \\
&= \arg\min_{\lambda} \sum_{i=1}^n - y_i \log\bigl( \lambda(\mathbf{x}_i) \bigr) + \lambda(\mathbf{x}_i) \\
&= \arg\min_{\lambda} \sum_{i=1}^n \lambda(\mathbf{x}_i) - y_i \log\bigl( \lambda(\mathbf{x}_i) \bigr) .
\end{aligned}
$$

## Sklearn/Keras' `.evaluate`

```{python}
#| echo: false
from sklearn.datasets import fetch_openml

# Download the dataset if it hasn't already been downloaded.
from pathlib import Path
cd = Path("NotInGit")
if (cd / "freq_data.csv").exists():
    freq = pd.read_csv("NotInGit/freq_data.csv")
else:
    print("Downloading dataset...")
    from sklearn.datasets import fetch_openml
    freq = fetch_openml(data_id=41214, as_frame=True).frame
    cd.mkdir(exist_ok=True)
    freq.to_csv("NotInGit/freq_data.csv", index=False)

# Remove the column named 'IDpol'.
freq = freq.drop("IDpol", axis=1)

# Convert categorical variables to numeric.
freq = pd.get_dummies(freq, columns=["VehGas", "Area", "VehBrand", "Region"])

freq["ClaimNb"] = np.minimum(freq.ClaimNb, 4)

X = freq.drop("ClaimNb", axis=1)
y = freq["ClaimNb"]

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=2022)
X_train_unsc = X_train
X_test_unsc = X_test

catVars = ['VehGas', 'Area', 'VehBrand', 'Region']
ctsVars = ['Exposure', 'VehPower', 'VehAge', 'DrivAge', 'BonusMalus', 'Density']

from sklearn.compose import ColumnTransformer
from sklearn.preprocessing import StandardScaler

ct = ColumnTransformer([
        ('Normalise', StandardScaler(), ctsVars)
    ], remainder='passthrough')

X_train = ct.fit_transform(X_train_unsc)
X_test = ct.transform(X_test_unsc)

X_train = pd.DataFrame(X_train, columns=X_train_unsc.columns)
X_test = pd.DataFrame(X_test, columns=X_test_unsc.columns)
``` 

Given a model:
```{python}
model = tf.keras.models.load_model("french-motor.h5")
```
we can calculate the loss on some set of data:
```{python}
print(model.evaluate(X_train, y_train, verbose=False))
print(model.evaluate(X_test, y_test, verbose=False))
```
This is a wrapper for:

```{python}
print(tf.keras.losses.poisson(y_train, model.predict(X_train, verbose=0).flatten()))
print(tf.keras.losses.poisson(y_test, model.predict(X_test, verbose=0).flatten()))
```


## Poisson loss & Poisson deviance

Poisson losses:

```{python}
model.evaluate(X_train, y_train, verbose=0)
```
```{python}
model.evaluate(X_test, y_test, verbose=0)
```

Poisson deviance:

```{python}
y_pred = model.predict(X_train, verbose=0)
mean_poisson_deviance(y_train, y_pred), mean_poisson_deviance(y_train, y_pred) * len(y_pred)
```

```{python}
y_pred = model.predict(X_test, verbose=0)
mean_poisson_deviance(y_test, y_pred), mean_poisson_deviance(y_test, y_pred) * len(y_pred)
```