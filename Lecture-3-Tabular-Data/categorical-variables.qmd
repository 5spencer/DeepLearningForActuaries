---
title: Categorical Variables
---

```{python}
#| echo: false
import matplotlib

import cycler
colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
matplotlib.pyplot.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (2.0, 2.0)

def set_rectangular_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 2.0)

set_rectangular_figures()
matplotlib.pyplot.rcParams['figure.dpi'] = 350
matplotlib.pyplot.rcParams['savefig.bbox'] = "tight"
matplotlib.pyplot.rcParams['font.family'] = "serif"

matplotlib.pyplot.rcParams['axes.spines.right'] = False
matplotlib.pyplot.rcParams['axes.spines.top'] = False

def square_fig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()

def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()
    shortest_side = min(xl[1], yl[1])
    matplotlib.pyplot.plot([0, shortest_side], [0, shortest_side], color="black", linestyle="--")

import pandas
pandas.options.display.max_rows = 4

import numpy
numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import tensorflow
tensorflow.random.set_seed(1)
tensorflow.config.set_visible_devices([], 'GPU')

def skip_empty(line):
  if line.strip() != "":
    print(line.strip())
```

## Lecture outline

- One-hot embedding
- Ordinal variables
- Entity embeddings

## Load packages {data-visibility="uncounted"}

```{python}
import random
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.impute import SimpleImputer
from sklearn.linear_model import LinearRegression

%load_ext watermark
%watermark -p numpy,pandas,sklearn,tensorflow
```

# Preprocessing {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

<center>

{{< video https://www.youtube.com/watch?v=6pDH66X3ClA width="700px" height="500px" >}}

</center>

## Keras model methods

```{python}
#| echo: false
# Make up some fake data to fit a model to.
numpy.random.seed(42)
Xs = np.random.normal(size=(100, 4))
ys = np.random.poisson(size=(100, 1))

# Convert to a pandas DataFrame.
Xs = pd.DataFrame(Xs, columns=["x1", "x2", "x3", "x4"])
ys = pd.DataFrame(ys, columns=["y"])

X_main, X_test, y_main, y_test = train_test_split(
    Xs, ys, random_state=42)
X_train, X_val, y_train, y_val = train_test_split(
    X_main, y_main, random_state=42)

numpy.random.seed(123)
```

::: columns
::: {.column width="45%"}
- `compile`: specify the loss function and optimiser
- `fit`: learn the parameters of the model
- `predict`: apply the model
- `evaluate`: apply the model and calculate a metric 
:::
::: {.column width="55%"}

<br>

```{python}
random.seed(12)
model = Sequential()
model.add(Dense(1, activation="relu"))
model.compile("adam", "poisson")
model.fit(X_train, y_train, verbose=0)
y_pred = model.predict(X_val, verbose=0)
print(model.evaluate(X_val, y_val, verbose=0))
```
:::
:::

## Scikit-learn model methods

::: columns
::: {.column width="45%"}
- `fit`: learn the parameters of the model
- `predict`: apply the model
- `score`: apply the model and calculate a metric 
:::
::: {.column width="55%"}

<br>

```{python}
model = LinearRegression()
model.fit(X_train, y_train)
y_pred = model.predict(X_val)
print(model.score(X_val, y_val))
```
:::
:::


## Scikit-learn preprocessing methods

::: columns
::: {.column width="45%"}
- `fit`: learn the parameters of the transformation
- `transform`: apply the transformation
- `fit_transform`: learn the parameters and apply the transformation
:::
::: {.column width="55%"}

::: {.panel-tabset}

### `fit`

```{python}
scaler = StandardScaler()
scaler.fit(X_train)
X_train_sc = scaler.transform(X_train)
X_val_sc = scaler.transform(X_val)
X_test_sc = scaler.transform(X_test)

print(X_train_sc.mean(axis=0))
print(X_train_sc.std(axis=0))
print(X_val_sc.mean(axis=0))
print(X_val_sc.std(axis=0))
```

### `fit_transform`

```{python}
scaler = StandardScaler()
X_train_sc = scaler.fit_transform(X_train)
X_val_sc = scaler.transform(X_val)
X_test_sc = scaler.transform(X_test)

print(X_train_sc.mean(axis=0))
print(X_train_sc.std(axis=0))
print(X_val_sc.mean(axis=0))
print(X_val_sc.std(axis=0))
```
:::

:::
:::

## Summary of the splitting

![](Melantha_Wang_ML_workflow.svg)

::: footer
Source: Melantha Wang (2022), ACTL3143 Project.
:::

## Dataframes & arrays

::: columns
::: column
```{python}
X_test.head(3)
```
:::
::: column
```{python}
X_test_sc
```
:::
:::

::: {.callout-note}
By default, when you pass `sklearn` a DataFrame it returns a `numpy` array.
:::

## Keep as a DataFrame

::: columns
::: {.column width="55%"}

<br>

From [scikit-learn 1.2](https://scikit-learn.org/stable/auto_examples/release_highlights/plot_release_highlights_1_2_0.html#pandas-output-with-set-output-api):

```{python}
from sklearn import set_config
set_config(transform_output="pandas")

imp = SimpleImputer()
imp.fit(X_train)
X_train_imp = imp.fit_transform(X_train)
X_val_imp = imp.transform(X_val)
X_test_imp = imp.transform(X_test)
```
:::
::: {.column width="45%"}
```{python}
X_test_imp
```
:::
:::

```{python}
#| echo: false
# set_config(transform_output="default")
pandas.options.display.max_rows = 6
```

# French Motor Claims & Poisson Regression {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## French motor dataset

Download the dataset if we don't have it already.

```{python}
#| output-location: slide
from pathlib import Path
from sklearn.datasets import fetch_openml

if not Path("french-motor.csv").exists():
    freq = fetch_openml(data_id=41214, as_frame=True).frame
    freq.to_csv("french-motor.csv", index=False)
else:
    freq = pd.read_csv("french-motor.csv")

freq
```


## Data dictionary {.smaller}

::: columns
::: column
- `IDpol`: policy number (unique identifier)
- `ClaimNb`: number of claims on the given policy
- `Exposure`: total exposure in yearly units
- `Area`: area code (categorical, ordinal)
- `VehPower`: power of the car (categorical, ordinal)
- `VehAge`: age of the car in years
- `DrivAge`: age of the (most common) driver in years
:::
::: column
- `BonusMalus`: bonus-malus level between 50 and 230 (with reference level 100)
- `VehBrand`: car brand (categorical, nominal)
- `VehGas`: diesel or regular fuel car (binary)
- `Density`: density of inhabitants per km^2^ in the city of the living place of the driver
- `Region`: regions in France (prior to 2016)
:::
:::

::: footer
Source: Nell et al. (2020), [Case Study: French Motor Third-Party Liability Claims](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764), SSRN.
:::

## The model

Have $\{ (\mathbf{x}_i, y_i) \}_{i=1, \dots, n}$ for $\mathbf{x}_i \in \mathbb{R}^{47}$ and $y_i \in \mathbb{N}_0$.

Assume the distribution
$$
Y_i \sim \mathsf{Poisson}(\lambda(\mathbf{x}_i))
$$

We have $\mathbb{E} Y_i = \lambda(\mathbf{x}_i)$. 
The NN takes $\mathbf{x}_i$ & predicts $\mathbb{E} Y_i$.

::: {.callout-note}
For insurance, _this is a bit weird_.
The exposures are different for each policy.

$\lambda(\mathbf{x}_i)$ is the expected number of claims for the duration of policy $i$'s contract.

Normally, $\text{Exposure}_i \not\in \mathbf{x}_i$, and $\lambda(\mathbf{x}_i)$ is the expected rate _per year_, then
$$
Y_i \sim \mathsf{Poisson}(\text{Exposure}_i \times \lambda(\mathbf{x}_i)).
$$
:::

## Where are things defined?

String options in Keras are just conveniences.

```{python}
model = Sequential([
    Dense(30, activation="relu"),
    Dense(1, activation="exponential")
])
```

is the same as

```{python}
from tensorflow.keras.activations import relu, exponential

model = Sequential([
    Dense(30, activation=relu),
    Dense(1, activation=exponential)
])
```

```{python}
x = [-1.0, 0.0, 1.0]
print(relu(x))
print(exponential(x))
```

## String arguments to `.compile`

When we run

```{python}
model.compile(optimizer="adam", loss="poisson")
```

it is equivalent to

```{python}
from tensorflow.keras.losses import poisson
from tensorflow.keras.optimizers import Adam

model.compile(optimizer=Adam(), loss=poisson)
```

Why do this manually? To adjust the object:

```{python}
optimizer = Adam(learning_rate=0.01)
model.compile(optimizer=optimizer, loss="poisson")
```

or to get help.

## Keras' "poisson" loss

```{python}
help(tensorflow.keras.losses.poisson)
```

# Ordinal Variables {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Subsample and split

```{python}
freq = freq.drop("IDpol", axis=1).head(25_000)

X_train, X_test, y_train, y_test = train_test_split(
  freq.drop("ClaimNb", axis=1), freq["ClaimNb"], random_state=2023)

# Reset each index to start at 0 again.
X_train = X_train.reset_index(drop=True) 
X_test = X_test.reset_index(drop=True)
```

## What values do we see in the data?

::: {layout-ncol=2 layout-nrow=2}
```{python}
X_train["Area"].value_counts()
```

```{python}
X_train["VehBrand"].value_counts()
```

```{python}
X_train["VehGas"].value_counts()
```

```{python}
X_train["Region"].value_counts()
```
:::

## Ordinal & binary categories are easy

```{python}
from sklearn.preprocessing import OrdinalEncoder
oe = OrdinalEncoder()
oe.fit(X_train[["Area", "VehGas"]])
oe.categories_
```

```{python}
for i, area in enumerate(oe.categories_[0]):
    print(f"The Area value {area} gets turned into {i}.")
```

```{python}
for i, gas in enumerate(oe.categories_[1]):
    print(f"The VehGas value {gas} gets turned into {i}.")
```

## Ordinal encoded values

```{python}
X_train_ord = oe.transform(X_train[["Area", "VehGas"]])
X_test_ord = oe.transform(X_test[["Area", "VehGas"]])
```

::: columns
::: column
```{python}
X_train[["Area", "VehGas"]].head()
```
:::
::: column
```{python}
X_train_ord.head()
```
:::
:::

## Train on ordinal encoded values

```{python}
random.seed(12)
model = Sequential([
  Dense(1, activation="exponential")
])

model.compile(optimizer="adam", loss="poisson")

es = EarlyStopping(verbose=True)
hist = model.fit(X_train_ord, y_train, epochs=100, verbose=0,
    validation_split=0.2, callbacks=[es])
hist.history["val_loss"][-1]
```

What about adding the continuous variables back in?
Use a sklearn _column transformer_ for that.

## Preprocess ordinal & continuous

```{python}
from sklearn.compose import make_column_transformer

ct = make_column_transformer(
  (OrdinalEncoder(), ["Area", "VehGas"]),
  ("drop", ["VehBrand", "Region"]),
  remainder=StandardScaler()
)

X_train_ct = ct.fit_transform(X_train)
```

::: columns
::: column
```{python}
X_train.head(3)
```
:::
::: column
```{python}
X_train_ct.head(3)
```
:::
:::

## Preprocess ordinal & continuous II

```{python}
from sklearn.compose import make_column_transformer

ct = make_column_transformer(
  (OrdinalEncoder(), ["Area", "VehGas"]),
  ("drop", ["VehBrand", "Region"]),
  remainder=StandardScaler(),
  verbose_feature_names_out=False
)
X_train_ct = ct.fit_transform(X_train)
```

::: columns
::: column
```{python}
X_train.head(3)
```
:::
::: column
```{python}
X_train_ct.head(3)
```
:::
:::


# Categorical Variables & Entity Embeddings {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Region column

![French Administrative Regions](french-regions.png)

::: footer
Source: Nell et al. (2020), [Case Study: French Motor Third-Party Liability Claims](https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3164764), SSRN.
:::

## One-hot encoding

```{python}
oe = OneHotEncoder(sparse_output=False)
X_train_oh = oe.fit_transform(X_train[["Region"]])
X_test_oh = oe.transform(X_test[["Region"]])
print(list(X_train["Region"][:5]))
X_train_oh.head()
```

## Train on one-hot inputs

```{python}
num_regions = len(oe.categories_[0])

random.seed(12)
model = Sequential([
  Dense(2, input_dim=num_regions),
  Dense(1, activation="exponential")
])

model.compile(optimizer="adam", loss="poisson")

es = EarlyStopping(verbose=True)
hist = model.fit(X_train_oh, y_train, epochs=100, verbose=0,
    validation_split=0.2, callbacks=[es])
hist.history["val_loss"][-1]
```

## Consider the first layer {.smaller}

```{python}
every_category = pd.DataFrame(np.eye(num_regions), columns=oe.categories_[0])
every_category.head(3)
```

```{python}
# Put this through the first layer of the model
X = every_category.to_numpy()
model.layers[0](X)
```

## The first layer

```{python}
layer = model.layers[0]
W, b = layer.get_weights()
X.shape, W.shape, b.shape
```

::: columns
::: column
```{python}
X @ W + b
```
:::
::: column
```{python}
W + b
```
:::
:::

## Just a look-up operation

::: columns
::: column
```{python}
display(list(oe.categories_[0]))
```
:::
::: column
```{python}
W + b
```
:::
:::

## Turn the region into an index

```{python}
oe = OrdinalEncoder()
X_train_reg = oe.fit_transform(X_train[["Region"]])
X_test_reg = oe.transform(X_test[["Region"]])

for i, reg in enumerate(oe.categories_[0][:3]):
  print(f"The Region value {reg} gets turned into {i}.")
```

## Embedding

```{python}
from tensorflow.keras.layers import Embedding
num_regions = len(np.unique(X_train[["Region"]]))

random.seed(12)
model = Sequential([
  Embedding(input_dim=num_regions, output_dim=2),
  Dense(1, activation="exponential")
])

model.compile(optimizer="adam", loss="poisson")
```

## Fitting that model

```{python}
es = EarlyStopping(verbose=True)
hist = model.fit(X_train_reg, y_train, epochs=100, verbose=0,
    validation_split=0.2, callbacks=[es])
hist.history["val_loss"][-1]
```

```{python}
model.layers
```


## Keras' Embedding Layer

::: columns
::: column
```{python}
model.layers[0].get_weights()[0]
```
:::
::: column
```{python}
X_train["Region"].head(4)
```
```{python}
X_sample = X_train_reg[:4].to_numpy()
X_sample
```
```{python}
enc_tensor = model.layers[0](X_sample)
enc_tensor.numpy().squeeze()
```
:::
:::

## The learned embeddings

```{python}
points = model.layers[0].get_weights()[0]
plt.scatter(points[:,0], points[:,1])
for i in range(num_regions):
  plt.text(points[i,0]+0.01, points[i,1] , s=oe.categories_[0][i])
```

## Entity embeddings 

![Embeddings will gradually improve during training.](entity-embeddings.png)

::: footer
Source: Marcus Lautier (2022).
:::

## Embeddings & other inputs

![Illustration of a neural network with both continuous and categorical inputs.](nn-with-entity-embedding-diagram.png)

We can't do this with Sequential models...

::: footer
Source: LotusLabs Blog, [Accurate insurance claims prediction with Deep Learning](https://www.lotuslabs.ai/accurate-insurance-claims-prediction-with-deep-learning/).
:::

# Keras' Functional API {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Converting Sequential models

```{python}
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input
```

::: columns
::: column
```{python}
random.seed(12)

model = Sequential([
  Dense(30, "relu"),
  Dense(1, "exponential")
])

model.compile(
  optimizer="adam",
  loss="poisson")

hist = model.fit(
  X_train_ord, y_train,
  epochs=1, verbose=0,
  validation_split=0.2)
hist.history["val_loss"][-1]
```
:::
::: column
```{python}
random.seed(12)

inputs = Input(shape=(2,))
x = Dense(30, "relu")(inputs)
out = Dense(1, "exponential")(x)
model = Model(inputs, out)

model.compile(
  optimizer="adam",
  loss="poisson")

hist = model.fit(
  X_train_ord, y_train,
  epochs=1, verbose=0,
  validation_split=0.2)
hist.history["val_loss"][-1]
```
:::
:::

Cf. [one-length tuples](https://pat-laub.github.io/DeepLearningMaterials/2023/Lecture-1-Artificial-Intelligence/python.html#/one-length-tuples).

## Wide & Deep network

::: columns
::: {.column width="45%"}
![An illustration of the wide & deep network architecture.](wide-and-deep-network.png)
:::
::: {.column width="55%"}
Add a _skip connection_ from input to output layers.

```{python}
from tensorflow.keras.layers \
    import Concatenate

inp = Input(shape=X_train.shape[1:])
hidden1 = Dense(30, "relu")(inp)
hidden2 = Dense(30, "relu")(hidden1)
concat = Concatenate()(
  [inp, hidden2])
output = Dense(1)(concat)
model = Model(
    inputs=[inp],
    outputs=[output])
```
:::
:::

::: footer
Sources: Marcus Lautier (2022) & Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Chapter 10 code snippet.
:::

## Naming the layers

For complex networks, it is often useful to give meaningul names to the layers.

```{python}
input_ = Input(shape=X_train.shape[1:], name="input")
hidden1 = Dense(30, activation="relu", name="hidden1")(input_)
hidden2 = Dense(30, activation="relu", name="hidden2")(hidden1)
concat = Concatenate(name="combined")([input_, hidden2])
output = Dense(1, name="output")(concat)
model = Model(inputs=[input_], outputs=[output])
```

## Inspecting a complex model

```{python}
from tensorflow.keras.utils import plot_model
```

::: columns
::: {.column width="30%"}
```{python}
plot_model(model)
```
:::
::: {.column width="70%"}
::: {.smaller}
```{python}
model.summary(print_fn=skip_empty, line_length=75)
```
:::
:::
:::

# French Motor Dataset with Embeddings {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## The desired architecture

![Illustration of a neural network with both continuous and categorical inputs.](nn-with-entity-embedding-diagram.png)

::: footer
Source: LotusLabs Blog, [Accurate insurance claims prediction with Deep Learning](https://www.lotuslabs.ai/accurate-insurance-claims-prediction-with-deep-learning/).
:::

## Preprocess all French motor inputs

Transform the categorical variables to integers:

```{python}
num_brands, num_regions = X_train.nunique()[["VehBrand", "Region"]]

ct = make_column_transformer(
  (OrdinalEncoder(), ["VehBrand", "Region", "Area", "VehGas"]),
  remainder=StandardScaler(),
  verbose_feature_names_out=False
)
X_train_ct = ct.fit_transform(X_train)
X_test_ct = ct.transform(X_test)
```

Split the brand and region data apart from the rest:

```{python}
X_train_brand = X_train_ct["VehBrand"]; X_test_brand = X_test_ct["VehBrand"]
X_train_region = X_train_ct["Region"]; X_test_region = X_test_ct["Region"]
X_train_rest = X_train_ct.drop(["VehBrand", "Region"], axis=1)
X_test_rest = X_test_ct.drop(["VehBrand", "Region"], axis=1)
```

## Organise the inputs

Make a Keras `Input` for: vehicle brand, region, & others.

```{python}
veh_brand = Input(shape=(1,), name="vehBrand")
region = Input(shape=(1,), name="region")
other_inputs = Input(shape=X_train_rest.shape[1:], name="otherInputs")
```
Create embeddings and join them with the other inputs. 
```{python}
from tensorflow.keras.layers import Reshape

random.seed(1337)
veh_brand_ee = Embedding(input_dim=num_brands, output_dim=2,
    name="vehBrandEE")(veh_brand)
veh_brand_ee = Reshape(target_shape=(2,))(veh_brand_ee)

region_ee = Embedding(input_dim=num_regions, output_dim=2,
    name="regionEE")(region)
region_ee = Reshape(target_shape=(2,))(region_ee)

x = Concatenate(name="combined")([veh_brand_ee, region_ee, other_inputs])
```

## Complete the model and fit it

Feed the combined embeddings & continuous inputs to some normal dense layers.

```{python}
x = Dense(30, "relu", name="hidden")(x)
out = Dense(1, "exponential", name="out")(x)

model = Model([veh_brand, region, other_inputs], out)
model.compile(optimizer="adam", loss="poisson")

hist = model.fit((X_train_brand, X_train_region, X_train_rest),
    y_train, epochs=100, verbose=0,
    callbacks=[EarlyStopping(patience=5)], validation_split=0.2)
np.min(hist.history["val_loss"])
```

## Plotting this model

```{python}
plot_model(model)
```

## Why we need to reshape

```{python}
plot_model(model, layer_range=("", "combined"), show_shapes=True)
```


# Scale By Exposure {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Two different models

Have $\{ (\mathbf{x}_i, y_i) \}_{i=1, \dots, n}$ for $\mathbf{x}_i \in \mathbb{R}^{47}$ and $y_i \in \mathbb{N}_0$.

**Model 1**: Say $Y_i \sim \mathsf{Poisson}(\lambda(\mathbf{x}_i))$.

But, the exposures are different for each policy.
$\lambda(\mathbf{x}_i)$ is the expected number of claims for the duration of policy $i$'s contract.

**Model 2**: Say $Y_i \sim \mathsf{Poisson}(\text{Exposure}_i \times \lambda(\mathbf{x}_i))$.

Now, $\text{Exposure}_i \not\in \mathbf{x}_i$, and $\lambda(\mathbf{x}_i)$ is the rate _per year_.

## Just take continuous variables

```{python}
ct = make_column_transformer(
  ("passthrough", ["Exposure"]),
  ("drop", ["VehBrand", "Region", "Area", "VehGas"]),
  remainder=StandardScaler(),
  verbose_feature_names_out=False
)
X_train_ct = ct.fit_transform(X_train)
X_test_ct = ct.transform(X_test)
```

Split exposure apart from the rest:

```{python}
X_train_exp = X_train_ct["Exposure"]; X_test_exp = X_test_ct["Exposure"]
X_train_rest = X_train_ct.drop("Exposure", axis=1)
X_test_rest = X_test_ct.drop("Exposure", axis=1)
```

Organise the inputs:

```{python}
exposure = Input(shape=(1,), name="exposure")
other_inputs = Input(shape=X_train_rest.shape[1:], name="otherInputs")
```

## Make & fit the model

Feed the continuous inputs to some normal dense layers.

```{python}
random.seed(1337)
x = Dense(30, "relu", name="hidden1")(other_inputs)
x = Dense(30, "relu", name="hidden2")(x)
lambda_ = Dense(1, "exponential", name="lambda")(x)
```

```{python}
from tensorflow.keras.layers import Multiply

out = Multiply(name="out")([lambda_, exposure])
model = Model([exposure, other_inputs], out)
model.compile(optimizer="adam", loss="poisson")

es = EarlyStopping(patience=10, restore_best_weights=True, verbose=1)
hist = model.fit((X_train_exp, X_train_rest),
    y_train, epochs=100, verbose=0,
    callbacks=[es], validation_split=0.2)
np.min(hist.history["val_loss"])
```

## Plot the model

```{python}
plot_model(model)
```

# {data-visibility="uncounted"} 

<h2>Glossary</h2>

::: columns
:::: column
- entity embeddings
- Input layer
- Keras functional API
::::
:::: column
- Reshape layer
- skip connection
- wide & deep network structure
::::
:::

```{python}
#| echo: false
!rm model.png
```

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
