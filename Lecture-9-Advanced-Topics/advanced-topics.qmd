---
title: Advanced Topics & Revision
subtitle: ACTL3143 & ACTL5111 Deep Learning for Actuaries
author: Dr Patrick Laub
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    footer: "Slides: [Dr Patrick Laub](https://pat-laub.github.io) (@PatrickLaub)."
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    transition: none
    slide-number: c/t
    strip-comments: true
    preview-links: false
    margin: 0.2
    chalkboard:
      boardmarker-width: 6
      grid: false
      background:
        - "rgba(255,255,255,0.0)"
        - "https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"
    include-before: <div class="line right"></div>
    include-after: <script>registerRevealCallbacks();</script>
highlight-style: breeze
jupyter: python3
execute:
  keep-ipynb: true
  echo: true
---

```{python}
#| echo: false
import matplotlib

def set_square_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (2.0, 2.0)

def set_rectangular_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 2.0)

def square_fig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()

set_rectangular_figures()
matplotlib.pyplot.rcParams['figure.dpi'] = 350
matplotlib.pyplot.rcParams['savefig.bbox'] = "tight"
matplotlib.pyplot.rcParams['font.family'] = "serif"

matplotlib.pyplot.rcParams['axes.spines.right'] = False
matplotlib.pyplot.rcParams['axes.spines.top'] = False

def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()
    shortest_side = min(xl[1], yl[1])
    matplotlib.pyplot.plot([0, shortest_side], [0, shortest_side], color="black", linestyle="--")

import pandas
pandas.options.display.max_rows = 6

import numpy
numpy.set_printoptions(precision=4)
numpy.random.seed(123)

import tensorflow
tensorflow.random.set_seed(1)
tensorflow.config.set_visible_devices([], 'GPU')

tensorflow.get_logger().setLevel('ERROR')

def skip_empty(line):
  if line.strip() != "":
    print(line.strip())
```

# 

<h2>Lecture Outline</h2>

<br>

- Dissecting `model.fit`
- Object oriented programming & PyTorch
- Generative adversarial networks
- Exam preparation

<br><br><br>

## Announcements

- Previous StoryWall was quite successful.
- Young Data Analytics Working Group podcast opportunity.
- Project marks will go on Moodle next week.
- Final Story Wall is due __this Friday at noon__.

## Exam details

- Thursday 18th Aug 2 pm - 4 pm (14:00-16:00)
- Exam is a Moodle quiz
- Open book (if you see "No Exam Materials permitted" just ignore it)
- Link at the top of the Moodle page for the course
- Exam open for 2 hours, but __you have 1.5 hours__ to complete
- Complete the IT preparation checklist (MFA, speed test, read policies)

## DALL-E 2 

![DALL-E 2 example: "Teddy bears working on new AI research on the moon in the 1980s"](dall-e-2-teddy-bears-on-moon.jpeg)

::: footer
Source: OpenAI, [DALL-E 2](https://openai.com/dall-e-2/).
:::

## DALL-E 2 Beta {.smaller}

_A painting of a penguin in a library studying a textbook while eating sushi and drinking a strawberry milkshake_

::: columns
::: column
![](DALL-E/DALL·E 2022-07-28 08.55.22 - A painting of a penguin in a library studying a textbook while eating sushi and drinking a strawberry milkshake .png)
::: 
::: column
![](DALL-E/DALL·E 2022-07-28 08.55.28 - A painting of a penguin in a library studying a textbook while eating sushi and drinking a strawberry milkshake .png)
::: 
::: 

## DALL-E 2 Beta II {.smaller}

_A painting of a penguin in a library studying a textbook while eating sushi and drinking a strawberry milkshake_

::: columns
::: column
![](DALL-E/DALL·E 2022-07-28 08.55.34 - A painting of a penguin in a library studying a textbook while eating sushi and drinking a strawberry milkshake .png)
::: 
::: column
![](DALL-E/DALL·E 2022-07-28 08.55.40 - A painting of a penguin in a library studying a textbook while eating sushi and drinking a strawberry milkshake .png)
::: 
::: 

## Load packages {data-visibility="uncounted"}

<br>

```{python}
import random
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

%load_ext watermark
%watermark -p matplotlib,numpy,pandas,tensorflow
```

{{< include _pytorch.qmd >}}

# Other Useful Packages {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Tensorflow Probability

```{python}
#| echo: false
from pathlib import Path
from sklearn.datasets import fetch_openml
from sklearn.model_selection import train_test_split
from sklearn.compose import make_column_transformer
from sklearn.preprocessing import MinMaxScaler

if Path("freq_data.csv").exists():
    freq = pd.read_csv("freq_data.csv")
else:
    freq = fetch_openml(data_id=41214, as_frame=True).frame
    freq.to_csv("freq_data.csv", index=False)

# Remove the column named 'IDpol'.
freq = freq.drop("IDpol", axis=1)

# Convert categorical variables to numeric.
freq = pd.get_dummies(freq, columns=["VehGas", "Area", "VehBrand", "Region"])

features = freq.drop("ClaimNb", axis=1)
target = freq["ClaimNb"]

X_main, X_test, y_main, y_test = train_test_split(features, target, random_state=2022)
X_train, X_val, y_train, y_val = train_test_split(X_main, y_main, random_state=2022)

cts_cols = ["Exposure", "VehPower", "VehAge", "DrivAge", "BonusMalus", "Density"]

ct = make_column_transformer(
  (MinMaxScaler(), cts_cols),
  remainder="passthrough"
)

X_train = ct.fit_transform(X_train)
X_val = ct.transform(X_val)
X_test = ct.transform(X_test)
```

```{python}
import tensorflow_probability as tfp
tfd = tfp.distributions
```

```{python}
random.seed(123)
model = keras.Sequential([
  layers.Dense(24, "leaky_relu", input_dim=X_train.shape[1]),
  layers.Dense(1, "exponential"),
  tfp.layers.DistributionLambda(tfd.Poisson)
])

def NLL(y_true, y_hat):
  return -y_hat.log_prob(y_true)

model.compile(loss=NLL)
model.fit(X_train, y_train, epochs=3, verbose=0);
```

:::{.callout-tip}
## Suggested viewing

Josh Dylan (2019), [TensorFlow Probability: Learning with confidence](https://youtu.be/BrwKURU-wpk), TF Dev Summit '19, YouTube (14 mins).
:::

## Predictions are then distributions

```{python}
y_pred = model(X_val)
type(y_pred)
```

```{python}
y_pred.mean()[:3]
```

```{python}
y_pred.stddev()[:3]
```


## Zero-inflated Poisson

```{python}
def zero_inf(out): 
  rate = tf.squeeze(tf.math.exp(out[:,0:1]))
  s = tf.math.sigmoid(out[:,1:2])
  probs = tf.concat([1-s, s], axis=1)
  return tfd.Mixture(
    cat=tfd.Categorical(probs=probs),
    components=[
      tfd.Deterministic(loc=tf.zeros_like(rate)),
      tfd.Poisson(rate=rate),
    ])
```

```{python}
random.seed(123)

zip_model = keras.Sequential([
  layers.Dense(24, "leaky_relu", input_dim=X_train.shape[1]),
  layers.Dense(2),
  tfp.layers.DistributionLambda(zero_inf)
])

def NLL(y_true, y_hat):
  return -y_hat.log_prob(y_true)

zip_model.compile(loss=NLL)
```

## Evaluations are then likelihoods

```{python}
zip_model.fit(X_train, y_train, epochs=3, verbose=0);
```

```{python}
model.evaluate(X_val, y_val, verbose=0)
```

```{python}
zip_model.evaluate(X_val, y_val, verbose=0)
```

:::{.smaller}
> In statistics, sometimes we only use a single data set. To still be able to evaluate the performance of the developed prediction model on the same data, sophisticated methods have developed over a long period of time and are still in use in some parts of the statistics community. These methods account for the fact that the model saw the data during fitting and applied corrections to account for that. These methods include, for example, the Akaike Information Criterion (AIC) or the Bayesian Information Criterion (BIC). Don’t get confused. If you have a validation set, you don’t need these methods.
:::

::: footer
Source: Sic & Duerr (2020), Probabilistic Deep Learning, Chapter 5.
:::

## HuggingFace's Transformers

```{python}
import transformers
from transformers import pipeline
generator = pipeline(task="text-generation", model="gpt2", revision="6c0e608")
```

```{python}
transformers.set_seed(1)
print(generator("It's the holidays so I'm going to enjoy")[0]["generated_text"])
```

```{python}
transformers.set_seed(1337)
print(generator("It's the holidays so I'm going to enjoy")[0]["generated_text"])
```

## Reading the course profile

```{python}
context = """
StoryWall Formative Discussions: An initial StoryWall, worth 2%, is due by noon on June 3. The following StoryWalls are worth 4% each (taking the best 7 of 9) and are due at noon on the following dates:
The project will be submitted in stages: draft due at noon on July 1 (10%), recorded presentation due at noon on July 22 (15%), final report due at noon on August 1 (15%).

As a student at UNSW you are expected to display academic integrity in your work and interactions. Where a student breaches the UNSW Student Code with respect to academic integrity, the University may take disciplinary action under the Student Misconduct Procedure. To assure academic integrity, you may be required to demonstrate reasoning, research and the process of constructing work submitted for assessment.
To assist you in understanding what academic integrity means, and how to ensure that you do comply with the UNSW Student Code, it is strongly recommended that you complete the Working with Academic Integrity module before submitting your first assessment task. It is a free, online self-paced Moodle module that should take about one hour to complete.

StoryWall (30%)

The StoryWall format will be used for small weekly questions. Each week of questions will be released on a Monday, and most of them will be due the following Monday at midday (see assessment table for exact dates). Students will upload their responses to the question sets, and give comments on another student's submission. Each week will be worth 4%, and the grading is pass/fail, with the best 7 of 9 being counted. The first week's basic 'introduction' StoryWall post is counted separately and is worth 2%.

Project (40%)

Over the term, students will complete an individual project. There will be a selection of deep learning topics to choose from (this will be outlined during Week 1).

The deliverables for the project will include: a draft/progress report mid-way through the term, a presentation (recorded), a final report including a written summary of the project and the relevant Python code (Jupyter notebook).

Exam (30%)

The exam will test the concepts presented in the lectures. For example, students will be expected to: provide definitions for various deep learning terminology, suggest neural network designs to solve risk and actuarial problems, give advice to mock deep learning engineers whose projects have hit common roadblocks, find/explain common bugs in deep learning Python code.
"""
```

## Question answering 
```{python}
qa = pipeline("question-answering", model="distilbert-base-cased-distilled-squad", revision="626af31")
```

```{python}
qa(question="What weight is the exam?", context=context)
```

```{python}
qa(question="What topics are in the exam?", context=context)
```

```{python}
qa(question="When is the presentation due?", context=context)
```

```{python}
qa(question="How many StoryWall tasks are there?", context=context)
```

## Recommended reading {.smaller}

- The Verge (2022), [The Great Fiction of AI: The strange world of high-speed semi-automated genre fiction](https://www.theverge.com/c/23194235/ai-fiction-writing-amazon-kindle-sudowrite-jasper)
- Vaswani et al. (2017), [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), NeurIPS
- Bommasani et al. (2021), [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)
- Gary Marcus (2022), [Deep Learning Is Hitting a Wall](https://nautil.us/deep-learning-is-hitting-a-wall-14467/), Nautilus article
- SDS 564, [Clem Delangue on Hugging Face and Transformers](https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000556643700)
- SDS 559, [GPT-3 for Natural Language Processing](https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000554847681)
- Computerphile (2019), [AI Language Models & Transformers](https://youtu.be/rURRYI66E54) (20m)
- Computerphile (2020), [GPT3: An Even Bigger Language Model](https://youtu.be/_8yVOC4ciXc) (25m)
- Nicholas Renotte (2021), [AI Blog Post Summarization with Hugging Face Transformers...](https://youtu.be/JctmnczWg0U) (33m)
- Seattle Applied Deep Learning (2019), [LSTM is dead. Long Live Transformers!](https://youtu.be/S27pHKBEp30) (28m)

# Revision {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Lecture 1: AI

::: columns
::: column
- artificial intelligence
- ~~Deep Blue~~
- default arguments
- dictionaries
- f-strings
- function definitions
- ~~Google Colaboratory~~
- ~~`help`~~
- list
:::
::: column
- ~~minimax algorithm~~
- ~~`pip install ...`~~
- **pseudocode**
- `range`
- slicing
- tuple
- ~~`type`~~
- whitespace indentation
- zero-indexing
:::
:::

## Lecture 2: Deep Learning {.smaller}

::: columns
::: column
- **activations, activation function**
- artificial intelligence vs machine learning
- artificial neural network
- biases (in neurons)
- classification problem
- **cost/loss function**
- deep network, network depth
- dense or fully-connected layer
- epoch
- feed-forward neural network
- ~~Keras, Tensorflow, PyTorch~~
:::
::: column
- labelled/unlabelled data
- machine learning
- ~~matplotlib, seaborn~~
- **neural network architecture**
- perceptron
- ReLU
- representation learning
- sigmoid activation function
- targets
- ~~training/test split~~
- universal approximation theorem
- weights (in a neuron)
:::
:::

## Lecture 3: Math of Deep Learning

::: columns
::: column
- accuracy
- batches, batch size
- callbacks
- cross-entropy loss
- **early stopping**
- gradient-based learning, hill-climbing
:::
::: column
- metrics
- **overfitting**
- shallow neural network
- stochastic (mini-batch) gradient descent
- **training/validation/test split**
:::
:::

:::{.callout-tip}
See this set of slides for an example of [calculating the output of a dense layer](https://pat-laub.github.io/DeepLearningMaterials/Lecture-5-Recurrent-Neural-Networks-And-Time-Series/rnns-and-time-series.html#/dense-layers-in-matrices).
:::

## Lecture 4: Tabular Data

::: columns
:::: column
- confusion matrix
- dead ReLU neurons
- dropout
- ensemble model
- **entity embeddings**
- Input layer
- Keras eager execution
- Keras functional API
::::
:::: column
- $\ell_1$ & $\ell_2$ regularisation
- leaky ReLU
- Monte Carlo dropout
- regularisation
- Reshape layer
- skip connection
- wide & deep network structure
::::
:::

## Lecture 5: RNNs

- dimensions (tensor)
- GRU
- LSTM
- rank (tensor)
- recurrent neural networks
- SimpleRNN

## Lecture 6: CNNs

::: columns
::: column
- channels
- computer vision
- convolutional layer & CNN
- error analysis
- filter
:::
::: column
- flatten layer
- kernel
- max pooling
- MNIST
- stride
:::
:::

## Lecture 7: NLP

::: columns
:::: column
- ~~AlexNet~~
- bag of words
- ~~CIFAR-10 / CIFAR-100~~
- ~~GoogLeNet & Inception~~
- ImageNet
::::
:::: column
- fine-tuning
- lemmatization
- one-hot embedding
- **transfer learning**
- vocabulary
::::
:::

## Lecture 8: Generative Networks

::: columns
:::: column
- autoencoder
- bias
- ~~DeepDream~~
- greedy sampling
- ~~GloVe~~
- ~~Grad-CAM~~
- language model
::::
:::: column
- latent space
- ~~neural style transfer~~
- ~~softmax temperature~~
- stochastic sampling
- **word embeddings/vectors**
- word2vec
::::
:::

## Lecture 9

- **Dissecting `model.fit`**
- ~~Object oriented programming & PyTorch~~
- Generative adversarial networks

## StoryWalls {.smaller}

1. Chess AI: Basic Python
2. French motor #1: Basic feed-forward networks
3. Stroke prediction: Classification network, preprocessing (one-hot encoding)
4. __French motor #2__: Entity embeddings
5. __Stock price prediction__: Recurrent neural networks
6. [__Hurricane damage__](https://colab.research.google.com/drive/1WX3UQ9pLfHYiUXZ8o5DcAOQj6-AGdo5M?usp=sharing): Convolutional neural networks and hyperparameter tuning
7. __Police reports__: NLP with bag-of-words, TF-IDF.
8. Generative networks experimenting
9. Reflection


<!-- {{< include _transformers.qmd >}} -->


<!--
# Explaining Convolutional Networks {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Saliency maps

## Grad-CAM

See

https://learning.oreilly.com/library/view/deep-learning-with/9781617296864/Text/09.xhtml#heading_id_14

Listing 9.26 etc.
-->

<!--
This is in contrast to traditional computer vision models which disregard the context of their labels (in other words, a "normal" image classifier works just as well if your labels are "cat" and "dog" or "foo" and "bar"; behind the scenes it just converts them into a numeric identifier with no particular meaning).

> Imagine you're given a filing cabinet and 100,000 documents. Your job is to put them each into the correct folder out of the 1,000 folders in the cabinet. At the end of the day your boss will judge your work.
>
> Unfortunately, you're illiterate. You start off doing no better than random chance. But, one day you realize that some of the documents are crisp and white and some are tattered and yellowed. You decide to sort the documents by color and split them evenly between the folders. Your boss  is pleased and gives you slightly better marks that day. Day by day you try to discover new things that are different about the files: some are long and some are short. Some have photos and some do not. Some are paper-clipped and some are stapled.
> 
> Then, one day, after years and years of tirelessly deciphering this enigma, trying different combinations of folders and ways of dividing the documents, improving your performance bit by bit, your boss introduces you to your new coworker. You furrow your brow trying to figure out how you're going to train her to execute on your delicate and complicated system.
> 
> But, to your surprise, on her very first day, her performance exceeds yours! It turns out your new coworker is CLIP and she knows how to read. Instead of having to guess what the folders should contain she simply looks at their labels. And instead of discovering clues about the documents bit by bit, she already has prior knowledge of what those indecipherable glyphs represent. -->

<!-- https://blog.roboflow.com/openai-clip/ -->


<!-- 
## Adversarial attacks

"TextAttack 🐙 is a Python framework for adversarial attacks, data augmentation, and model training in NLP"

https://github.com/QData/TextAttack
-->

<!-- - PracticeProbs.com, [Regular Expressions in Python](https://www.practiceprobs.com/problemsets/regular-expressions-in-python/) -->

<!--
- SDS 559, [GPT-3 for Natural Language Processing](https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000554847681)
       ->   Around 1:21:00, 'We don't learn language by reading sentences but watching action and facial reactions etc.'; multi-modal learning
-->

<!-- - autoregressive language model -->
<!-- - document summarisation (abstractive, extractive) -->
<!-- - few-shot, one-shot, zero-shot -->
<!-- - GitHub Copilot -->
<!-- - GPT-3 -->
<!-- - Hugging Face -->
<!-- - deep fake -->
<!-- - generative adversarial network -->
<!-- - saliency maps -->

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["abs", "compile", "eval", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
