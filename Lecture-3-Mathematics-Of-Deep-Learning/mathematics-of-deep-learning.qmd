---
title: Mathematics of Deep Learning 
subtitle: ACTL3143 & ACTL5111 Deep Learning for Actuaries
author: Dr Patrick Laub
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    footer: "Slides: [Dr Patrick Laub](https://pat-laub.github.io) (@PatrickLaub)."
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    transition: none
    slide-number: c/t
    strip-comments: true
    preview-links: false
    margin: 0.12
    width: 1000
    chalkboard:
      boardmarker-width: 6
      grid: false
      background:
        - "rgba(255,255,255,0.0)"
        - "https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"
    include-before: |
      <div class="line right"></div>
      <!--<link rel="stylesheet" href="https://pyscript.net/alpha/pyscript.css" />-->
      <script defer src="https://pyscript.net/alpha/pyscript.js"></script>
      <py-env>
        - matplotlib
      </py-env>
    include-after: <script>registerRevealCallbacks();</script>
highlight-style: breeze
jupyter: python3
execute:
  keep-ipynb: true
  echo: true
---

```{python}
#| echo: false
import matplotlib

import cycler
colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
matplotlib.pyplot.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (2.0, 2.0)

def set_rectangular_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 2.0)

set_rectangular_figures()
matplotlib.pyplot.rcParams['figure.dpi'] = 350
matplotlib.pyplot.rcParams['savefig.bbox'] = "tight"
matplotlib.pyplot.rcParams['font.family'] = "serif"

matplotlib.pyplot.rcParams['axes.spines.right'] = False
matplotlib.pyplot.rcParams['axes.spines.top'] = False

def squareFig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()

def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()
    shortestSide = min(xl[1], yl[1])
    matplotlib.pyplot.plot([0, shortestSide], [0, shortestSide], color="black", linestyle="--")

import pandas
pandas.options.display.max_rows = 6

import numpy
numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import tensorflow
tensorflow.random.set_seed(1)
tensorflow.config.set_visible_devices([], 'GPU')

def skip_empty(line):
  if line.strip() != "":
    print(line.strip())
```

## Load packages {data-visibility="uncounted"}

<br>
<br>

```{python}
import random
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
import tensorflow as tf

from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense

from tensorflow import keras
from tensorflow.keras import layers
```

{{< include _maths-logistic-regression.qmd >}}

{{< include _loss-and-derivatives.qmd >}}

# Optimisation {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Gradient-based learning 

<div>
  <!-- Source for slider with current value shown: https://stackoverflow.com/a/18936328 -->
  Make a guess: <input type="range" min="1" max="100" value="50" class="slider" id="new_guess" oninput="this.nextElementSibling.value = this.value">
  <output>50</output>
  Show derivatives: <input type="checkbox" id="derivs" py-onClick="show_derivatives">
  Reveal function: <input type="checkbox" id="reveal" py-onClick="reveal_function">
</div>
<div id="mpl" style="text-align: center;"></div>
<py-script output="mpl" src="pyscript-demo.py" />

## Gradient descent pitfalls

![Potential problems with gradient descent.](Geron-mls2_0406-blur.png)

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 4-6.
:::

## Go over all the training data

<br>

Called _batch gradient descent_.

<br>

```python
for i in range(numEpochs):
  gradient = evaluate_gradient(loss_function, data, weights)
  weights = weights - learningRate * gradient
```

## Pick a random training example

<br>

Called _stochastic gradient descent_.

<br>

```python
for i in range(numEpochs):
  rnd.shuffle(data)
  for example in data:
    gradient = evaluate_gradient(loss_function, example, weights)
    weights = weights - learningRate * gradient
```

## Take a group of training examples

<br>

Called _mini-batch gradient descent_.

<br>

```python
for i in range(numEpochs):
  rnd.shuffle(data)
  for b in range(numBatches):
    batch = data[b*batchSize:(b+1)*batchSize]
    gradient = evaluate_gradient(loss_function, batch, weights)
    weights = weights - learningRate * gradient
```

## Mini-batch gradient descent

::: columns
::: column

Why?

1. Because we have to (data is too big)
2. Because it is faster (lots of quick noisy steps > a few slow super accurate steps)
3. The noise helps us jump out of local minima

:::
::: column

![Example of jumping from local minima.](Geron-mls2_0406-blur.png)

:::
:::

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 4-6.
:::

## Learning rates

::: columns
::: column

![The learning rate is too small](Geron-mls2_0404-blur.png)

:::
::: column

![The learning rate is too large](Geron-mls2_0405-blur.png)

:::
:::

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figures 4-4 and 4-5.
:::

## Learning rates #2

![Changing the learning rates for a robot arm.](matt-henderson-learning-rates-animation.mov){width=60%}

::: footer
Source: Matt Henderson (2021), [Twitter post](https://twitter.com/matthen2/status/1520427516997025792)
:::

## Learning rate schedule

![Learning curves for various learning rates η](Geron-mls2_1108-blur.png)

In training the learning rate may be tweaked manually.

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 11-8.
:::

## We need non-zero derivatives

This is why can't use accuracy as the loss function for classification.

This is why we can have the _dead ReLU_ problem.

<figure>
<center>
<iframe width="560" height="315" src="https://www.youtube.com/embed/KpKog-L9veg?enablejsapi=1" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture" allowfullscreen="">
</iframe>
</center>
<p></p><figcaption aria-hidden="true">The “Explain Like I’m 5” version of softmax/argmax classifiers.</figcaption><p></p>
</figure>

{{< include _model-fit.qmd >}}

{{< include _linear-regression-autograd.qmd >}}

# {data-visibility="uncounted"}

<h2>Glossary</h2>

::: columns
::: column
- accuracy
- batches, batch size
- cross-entropy loss
- gradient-based learning, hill-climbing
:::
::: column
- metrics
- overfitting
- shallow neural network
- stochastic (mini-batch) gradient descent
- universal approximation theorem
:::
:::

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
