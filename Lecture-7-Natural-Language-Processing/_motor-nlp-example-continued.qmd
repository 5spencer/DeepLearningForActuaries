
# Car Crash NLP Part II {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

::: footer
Dataset source: [Dr JÃ¼rg Schelldorfer's GitHub](https://github.com/JSchelldorfer/ActuarialDataScience/blob/master/12%20-%20NLP%20Using%20Transformers/Actuarial_Applications_of_NLP_Part_1.ipynb).
:::

## The data

```{python}
from pathlib import Path
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder

if not Path("NHTSA_NMVCCS_extract.parquet.gzip").exists():
    print("Downloading dataset")
    !wget https://github.com/JSchelldorfer/ActuarialDataScience/raw/master/12%20-%20NLP%20Using%20Transformers/NHTSA_NMVCCS_extract.parquet.gzip

df = pd.read_parquet("NHTSA_NMVCCS_extract.parquet.gzip")

features = df["SUMMARY_EN"]
target = LabelEncoder().fit_transform(df["INJSEVB"])

X_main, X_test, y_main, y_test = \
    train_test_split(features, target, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val = \
    train_test_split(X_main, y_main, test_size=0.25, random_state=1)
X_train.shape, X_val.shape, X_test.shape
```

## What is TF-IDF?

Stands for _term frequency-inverse document frequency_.

![Infographic explaining TF-IDF](tf-idf-graphic.png)

::: footer
Source: FiloTechnologia (2014), [A simple java class for tf-idf scoring](http://filotechnologia.blogspot.com/2014/01/a-simple-java-class-for-tfidf-scoring.html), Blog post.
:::

## Using Keras `TextVectorization`

```{python}
max_tokens = 1_000
vect = layers.TextVectorization(
    max_tokens=max_tokens,
    output_mode="tf_idf",
)

vect.adapt(X_train)
vocab = vect.get_vocabulary()

X_train_txt = vect(X_train)
X_val_txt = vect(X_val)
X_test_txt = vect(X_test)

print(vocab[:50])
```

## The TF-IDF vectors

```{python}
pd.DataFrame(X_train_txt, columns=vocab, index=X_train.index)
```

## Feed TF-IDF into an ANN

```{python}
random.seed(42)
tfidfModel = keras.models.Sequential([
    layers.Dense(250, "relu", input_dim=X_train_txt.shape[1]),
    layers.Dense(1, "sigmoid")
])

tfidfModel.compile("adam", "BinaryCrossentropy", metrics=["accuracy"])
tfidfModel.summary(print_fn=skip_empty)
```

## Fit & evaluate

```{python}
es = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True,
    monitor="val_accuracy", verbose=2)

if not Path("tfidfModel.h5").exists():
    tfidfModel.fit(X_train_txt, y_train, epochs=1_000, callbacks=es,
        validation_data=(X_val_txt, y_val), verbose=0)
    tfidfModel.save("tfidfModel.h5")
else:
    tfidfModel = keras.models.load_model("tfidfModel.h5")
```
```{python}
tfidfModel.evaluate(X_train_txt, y_train, verbose=0)
```
```{python}
tfidfModel.evaluate(X_val_txt, y_val, verbose=0)
```

## Keep text as sequence of tokens

```{python}
max_length = 800 
max_tokens = 1_000
vect = layers.TextVectorization(
    max_tokens=max_tokens,
    output_sequence_length=max_length,    
)

vect.adapt(X_train)
vocab = vect.get_vocabulary()

X_train_txt = vect(X_train)
X_val_txt = vect(X_val)
X_test_txt = vect(X_test)

print(vocab[:50])
```

## A sequence of integers

```{python}
X_train_txt[0]
```

## Feed LSTM a sequence of one-hots

```{python}
random.seed(42)
inputs = keras.Input(shape=(max_length,), dtype="int64")
onehot = tf.one_hot(inputs, depth=max_tokens)
x = layers.Bidirectional(layers.LSTM(32))(onehot)
x = layers.Dropout(0.5)(x) 
outputs = layers.Dense(1, activation="sigmoid")(x)    
oneHotModel = keras.Model(inputs, outputs)
oneHotModel.compile(optimizer="rmsprop",
    loss="binary_crossentropy", metrics=["accuracy"])
oneHotModel.summary(print_fn=skip_empty)
```

## Fit & evaluate

```{python}
es = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True,
    monitor="val_accuracy", verbose=2)

if not Path("oneHotModel.h5").exists():
    oneHotModel.fit(X_train_txt, y_train, epochs=1_000, callbacks=es,
        validation_data=(X_val_txt, y_val), verbose=0);
    oneHotModel.save("oneHotModel.h5")
else:
    oneHotModel = keras.models.load_model("oneHotModel.h5")
```

```{python}
oneHotModel.evaluate(X_train_txt, y_train, verbose=0)
```
```{python}
oneHotModel.evaluate(X_val_txt, y_val, verbose=0)
```

## Custom embeddings

```{python}
inputs = keras.Input(shape=(max_length,), dtype="int64")
embedded = layers.Embedding(input_dim=max_tokens, output_dim=32,
        mask_zero=True)(inputs)
x = layers.Bidirectional(layers.LSTM(32))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
embedLSTM = keras.Model(inputs, outputs)
embedLSTM.compile("rmsprop", "binary_crossentropy", metrics=["accuracy"])
embedLSTM.summary(print_fn=skip_empty)
```

## Fit & evaluate

```{python}
es = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True,
    monitor="val_accuracy", verbose=2)

if not Path("embedLSTM.h5").exists():
    embedLSTM.fit(X_train_txt, y_train, epochs=1_000, callbacks=es,
        validation_data=(X_val_txt, y_val), verbose=0);
    embedLSTM.save("embedLSTM.h5")
else:
    embedLSTM = keras.models.load_model("embedLSTM.h5")
```

```{python}
embedLSTM.evaluate(X_train_txt, y_train, verbose=0)
```
```{python}
embedLSTM.evaluate(X_val_txt, y_val, verbose=0)
```
```{python}
embedLSTM.evaluate(X_test_txt, y_test, verbose=0)
```
