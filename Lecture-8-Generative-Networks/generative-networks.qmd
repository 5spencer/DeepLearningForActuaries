---
title: Generative Networks
---

```{python}
#| echo: false
import matplotlib
import matplotlib.pyplot as plt
import cycler

colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
plt.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (2.0, 2.0)

def set_rectangular_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 2.0)

def square_fig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()

set_rectangular_figures()
matplotlib.pyplot.rcParams['figure.dpi'] = 350
matplotlib.pyplot.rcParams['savefig.bbox'] = "tight"
matplotlib.pyplot.rcParams['font.family'] = "serif"

matplotlib.pyplot.rcParams['axes.spines.right'] = False
matplotlib.pyplot.rcParams['axes.spines.top'] = False

def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()
    shortest_side = min(xl[1], yl[1])
    matplotlib.pyplot.plot([0, shortest_side], [0, shortest_side], color="black", linestyle="--")

import pandas
pandas.options.display.max_rows = 6

import numpy
numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import tensorflow
tensorflow.config.set_visible_devices([], 'GPU') # A lot faster on GPU: 34s vs 1m10s.

tensorflow.random.set_seed(1)

tensorflow.get_logger().setLevel('ERROR')

def skip_empty(line):
  if line.strip() != "":
    print(line.strip())
```


## Lecture Outline

- Text Generation
- Image Generation
- Autoencoders
- Generative adversarial networks

## Load packages {data-visibility="uncounted"}

```{python}
import random
from pathlib import Path

import matplotlib.pyplot as plt
import numpy as np
import numpy.random as rnd
import pandas as pd
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

%load_ext watermark
%watermark -p matplotlib,numpy,pandas,tensorflow
```

# Text Generation {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Generative deep learning

- Using AI as augmented intelligence rather than artificial intelligence.
- Use of deep learning to augment creative activities such as writing, music and art, to *generate* new things.
- Some applications: text generation, deep dreaming, neural style transfer, variational autoencoders and generative adversarial networks.

## Text generation

> Generating sequential data is the closest computers get to dreaming.

- Generate sequence data: Train a model to predict the next token or next few tokens in a sentence, using previous tokens as input.
- A network that models the probability of the next tokens given the previous ones is called a *language model*.

::: {.notes}
GPT-3 is a 175 billion parameter text-generation model trained by the startup OpenAI on a large text corpus of digitally available books, Wikipedia and web crawling. GPT-3 made headlines in 2020 due to its capability to generate plausible-sounding text paragraphs on virtually any topic.
:::

::: footer
Source: Alex Graves (2013), [Generating Sequences With Recurrent Neural Networks](https://arxiv.org/abs/1308.0850)
:::

## Word-level language model

![Diagram of a word-level language model.](word-level-language-model.png)

::: footer
Source: Marcus Lautier (2022).
:::

## Character-level language model

![Diagram of a character-level language model (Char-RNN)](tensorflow-text_generation_sampling.png)

::: footer
Source: Tensorflow tutorial, [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation).
:::

## Useful for speech recognition

::: {#fig-speech-recognition}

| RNN output |  Decoded Transcription |
| --- | --- |
| what is the weather like in bostin right now | what is the weather like in boston right now |
| prime miniter nerenr modi | prime minister narendra modi |
| arther n tickets for the game | are there any tickets for the game |

Examples of transcriptions directly from the RNN with errors that are fixed by addition of a language model.
:::

::: footer
Source: Hannun et al. (2014), [Deep Speech: Scaling up end-to-end speech recognition](https://arxiv.org/pdf/1412.5567.pdf), arXiv:1412.5567, Table 1.
:::

## Generating Shakespeare I

> | ROMEO:
| Why, sir, what think you, sir?
| 
| AUTOLYCUS:
| A dozen; shall I be deceased.
| The enemy is parting with your general,
| As bias should still combit them offend
| That Montague is as devotions that did satisfied;
| But not they are put your pleasure.

::: footer
Source: Tensorflow tutorial, [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation).
:::

## Generating Shakespeare II {data-visibility="uncounted"}

> | DUKE OF YORK:
| Peace, sing! do you must be all the law;
| And overmuting Mercutio slain;
| And stand betide that blows which wretched shame;
| Which, I, that have been complaints me older hours.
| 
| LUCENTIO:
| What, marry, may shame, the forish priest-lay estimest you, sir,
| Whom I will purchase with green limits o' the commons' ears!

::: footer
Source: Tensorflow tutorial, [Text generation with an RNN](https://www.tensorflow.org/text/tutorials/text_generation).
:::


## Generating Shakespeare III {data-visibility="uncounted"}

> | ANTIGONUS: 
| To be by oath enjoin'd to this. Farewell! 
| The day frowns more and more: thou'rt like to have 
| A lullaby too rough: I never saw 
| The heavens so dim by day. A savage clamour! 
|
| [Exit, pursued by a bear]


#

<h2>Sampling strategy</h2>

- *Greedy sampling* will choose the token with the highest probability. It makes the resulting sentence repetitive and predictable.
- *Stochastic sampling*: if a word has probability 0.3 of being next in the sentence according to the model, we’ll choose it 30% of the time. But the result is still not interesting enough and still quite predictable.
- Use a *softmax temperature* to control the randomness. More randomness results in more surprising and creative sentences.

## Softmax temperature

- The softmax temperature is a parameter that controls the randomness of the next token.
- The formula is: $$ \text{softmax}_\text{temperature}(x) = \frac{\exp(x / \text{temperature})}{\sum_i \exp(x_i / \text{temperature})} $$

## "I am a" ...

```{python}
#| echo: false
def softmax(x, temperature=1.0):
    return np.exp(x / temperature) / np.sum(np.exp(x / temperature))

x = np.array([0.15, 0.5, 0.1, 0.25])
x /= np.sum(x)

# Plot bar charts for the original distributions and the softmax temperature versions.
plt.figure(figsize=(6, 3))

phrase = "I am a"
next_words = ["dog", "human", "robot", "teddy"] 

plt.subplot(2, 2, 1)
temperature = 0.1
plt.bar(next_words, softmax(x, temperature), color=colors)
plt.title(f"Temp = {temperature}")

plt.subplot(2, 2, 2)
temperature = 1
plt.bar(next_words, softmax(x, temperature), color=colors)
plt.title(f"Temp = {temperature}")

temperature = 10
plt.subplot(2, 2, 3)
plt.bar(next_words, softmax(x, temperature), color=colors)
plt.title(f"Temp = {temperature}")

plt.subplot(2, 2, 4)
temperature = 100
plt.bar(next_words, softmax(x, temperature), color=colors)
plt.title(f"Temp = {temperature}")

# Clean up the layout.
plt.tight_layout()

plt.show()
```
::: footer
Idea inspired by Mehta (2023), [The need for sampling temperature and differences between whisper, GPT-3, and probabilistic model's temperature](https://shivammehta25.github.io/posts/temperature-in-language-models-open-ai-whisper-probabilistic-machine-learning/)
:::

## Generating Laub (temp = 0.01)

> _In today's lecture we will_ be different situation.
> So, next one is what they rective that each commit to be able to learn some relationships from the course, and that is part of the image that it's very clese and black problems that you're trying to fit the neural network to do there instead of like a specific though shef series of layers mean about full of the chosen the baseline of car was in the right, but that's an important facts and it's a very small summary with very scrort by the beginning of the sentence.

## Generating Laub (temp = 0.25) {data-visibility="uncounted"}

> _In today's lecture we will_ decreas before model that we that we have to think about it, this mightsks better, for chattely the same project, because you might use the test set because it's to be picked up the things that I wanted to heard of things that I like that even real you and you're using the same thing again now because we need to understand what it's doing the same thing but instead of putting it in particular week, and we can say that's a thing I mainly link it's three columns.

## Generating Laub (temp = 0.5) {data-visibility="uncounted"}

> _In today's lecture we will_ probably the adw n wait lots of ngobs teulagedation to calculate the gradient and then I'll be less than one layer the next slide will br input over and over the threshow you ampaigey the one that we want to apply them quickly. So, here this is the screen here the main top kecw onct three thing to told them, and the output is a vertical variables and Marceparase of things that you're moving the blurring and that just data set is to maybe kind of categorical variants here but there's more efficiently not basically replace that with respect to the best and be the same thing.

## Generating Laub (temp = 1) {data-visibility="uncounted"}

> _In today's lecture we will_ put it different shates to touch on last week, so I want to ask what are you object frod current.
> They don't have any zero into it, things like that which mistakes. 10 claims that the average version was relden distever ditgs and Python for the whole term wo long right to really.
> The name of these two options.
> There are in that seems to be modified version. If you look at when you're putting numbers into your, that that's over.
> And I went backwards, up, if they'rina functional pricing working with.

## Generating Laub (temp = 1.5) {data-visibility="uncounted"}

> In today's lecture we will put it could be bedinnth. Lowerstoriage nruron. So rochain the everything that I just sGiming.
> If there was a large. It's gonua draltionation.
> Tow many, up, would that black and 53% that's girter thankAty will get you jast typically stickK thing.
> But maybe. Anyway, I'm going to work on this libry two, past, at shit citcs jast
> pleming to memorize overcamples like pre pysing, why wareed to smart a one in this reportbryeccuriay.

## Generate the most likely sequence

![An example sequence-to-sequence chatbot model.](chatbot.png)

::: footer
Source: Payne (2021), [What is beam search](https://www.width.ai/post/what-is-beam-search), Width.ai blog.
:::

## Beam search

![Illustration of a beam search.](beam-search.png)

::: footer
Source: Doshi (2021), [Foundations of NLP Explained Visually: Beam Search, How It Works](https://towardsdatascience.com/foundations-of-nlp-explained-visually-beam-search-how-it-works-1586b9849a24), towardsdatascience.com.
:::


{{< include _hf-transformers.qmd >}}

# Image Generation {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Reverse-engineering a CNN

A CNN is a function $f_{\boldsymbol{\theta}}(\mathbf{x})$ that takes a vector (image) $\mathbf{x}$ and returns a vector (distribution) $\widehat{\mathbf{y}}$.

Normally, we train it by modifying $\boldsymbol{\theta}$ so that 

$$ \boldsymbol{\theta}^*\ =\  \underset{\boldsymbol{\theta}}{\mathrm{argmin}} \,\, \text{Loss} \bigl( f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y} \bigr). $$

However, it is possible to _not train_ the network but to modify $\mathbf{x}$, like

$$ \mathbf{x}^*\ =\  \underset{\mathbf{x}}{\mathrm{argmin}} \,\, \text{Loss} \bigl( f_{\boldsymbol{\theta}}(\mathbf{x}), \mathbf{y} \bigr). $$

This is very slow as we do gradient descent every single time.

## Adversarial examples

![A demonstration of fast adversarial example generation applied to GoogLeNet on ImageNet. By adding an imperceptibly small vector whose elements are equal to the sign of the elements of the gradient of the cost function with respect to the input, we can change GoogLeNet’s classification of the image.](adversarial-example.png)

::: footer
Source: Goodfellow et al. (2015), [Explaining and Harnessing Adversarial Examples](https://arxiv.org/pdf/1412.6572.pdf), ICLR.
:::

## Adversarial stickers

![Adversarial stickers.](the-verge-adversarial_patch_.0.gif)

::: footer
Source: The Verge (2018), [These stickers make computer vision software hallucinate things that aren’t there](https://www.theverge.com/2018/1/3/16844842/ai-computer-vision-trick-adversarial-patches-google).
:::

## Adversarial text

"[TextAttack](https://github.com/QData/TextAttack) 🐙 is a Python framework for adversarial attacks, data augmentation, and model training in NLP"

<img src="https://jxmo.io/files/textattack.gif" alt="TextAttack Demo GIF" style="display: block; margin: 0 auto;" />

## Deep Dream 

![Deep Dream is an image-modification program released by Google in 2015.](deep-dream.jpeg)

::: footer
Source: Wikipedia, [DeepDream page](https://commons.wikimedia.org/wiki/File:Aurelia-aurita-3-0009.jpg).
:::

## DeepDream

- Even though many deep learning models are black boxes, convnets are quite interpretable via visualization. Some visualization techniques are: visualizing convnet outputs shows how convnet layers transform the input, visualizing convnet filters shows what visual patterns or concept each filter is receptive to, etc.
- The activations of the first few layers of the network carries more information about the visual contents, while deeper layers encode higher, more abstract concepts.

## DeepDream

- Each filter is receptive to a visual pattern. To visualize a convnet filter, gradient ascent is used to maximize the response of the filter. Gradient ascent maximize a loss function and moves the image in a direction that activate the filter more strongly to enhance its reading of the visual pattern. 
- DeepDream maximizes the activation of the entire convnet layer rather than that of a specific filter, thus mixing together many visual patterns all at once.
- DeepDream starts with an existing image, latches on to preexisting visual patterns, distorting elements of the image in a somewhat artistic fashion.


## Original

![A sunny day on the Mornington peninsula.](deep-dream-melbourne-original.jpg)

## Transformed

![Deep-dreaming version.](deep-dream-melbourne.png)

::: footer
Generated by [Keras' Deep Dream tutorial](https://keras.io/examples/generative/deep_dream/).
:::

#

<h2>Neural style transfer</h2>

Applying the style of a reference image to a target image while conserving the content of the target image.

![An example neural style transfer.](neuralstyletransfer.png)

::: {.notes}
- Style: textures, colors, visual patterns (blue-and-yellow circular brushstrokes in Vincent Van Gogh's Starry Night)
- Content: the higher-level macrostructure of the image (buildings in the Tübingen photograph).
:::

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Figure 12.9.
:::


## Goal of NST

What the model does:

- Preserve content by maintaining similar deeper layer activations between the original image and the generated image. The convnet should “see” both the original image and the generated image as containing the same things.

- Preserve style by maintaining similar correlations within activations for both low level layers and high-level layers. Feature correlations within a layer capture textures: the generated image and the style-reference image should share the same textures at different spatial scales.

## A wanderer in Greenland

::: columns
::: {.column width="50%"}
Content

![Some striking young hiker in Greenland.](ninja.jpg)
:::

::: {.column width="50%"}
Style

![_Wanderer above the Sea of Fog_ by Caspar David Friedrich.](wanderer.jpg)
:::
:::

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::

## A wanderer in Greenland II

::: columns
::: {.column width="45%"}
![Animation of NST in progress.](ninja.gif)
:::

::: {.column width="55%"}
![One result of NST.](ninja-wanderer.png)
:::
:::

:::{.callout-tip}
## Question

How would you make this faster for one specific style image?
:::

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::

## A new style image

![Hokusai's Great Wave off Kanagawa](wave.jpg)

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::

## A new content image

![The seascape in Qingdao](qingdao.jpg)

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::

## Another neural style transfer

![The seascape in Qingdao in the style of Hokusai's Great Wave off Kanagawa](qwave.jpg)

::: footer
Source: Laub (2018), [On Neural Style Transfer](https://pat-laub.github.io/2018/01/07/neural-style-transfer.html), Blog post.
:::


## Why is this important?

Taking derivatives with respect to the input image can be a first step toward explainable AI for convolutional networks.

- [Saliency maps](https://youtu.be/y8cwyeccuy4)
- [Grad-CAM](https://youtu.be/xGZfAoh0xKs)

# Autoencoders {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}


## Autoencoder

An autoencoder takes a data/image, maps it to a latent space via en encoder module, then decodes it back to an output with the same dimensions via a decoder module.

![Schematic of an autoencoder.](autoencoder.png)

::: footer
Source: Marcus Lautier (2022).
:::

## Autoencoder II {.smaller}

- An autoencoder is trained by using the same image as both the input and the target, meaning an autoencoder learns to reconstruct the original inputs. Therefore it's _not supervised learning_, but _self-supervised learning_.
- If we impose constraints on the encoders to be low-dimensional and sparse, _the input data will be compressed_ into fewer bits of information. 
- Latent space is a place that stores low-dimensional representation of data. It can be used for _data compression_, where data is compressed to a point in a latent space.
- An image can be compressed into a latent representation, which can then be reconstructed back to a _slightly different image_. 

::: {.notes}
For image editing, an image can be projected onto a latent space and moved inside the latent space in a meaningful way (which means we modify its latent representation), before being mapped back to the image space. This will edit the image and allow us to generate images that have never been seen before.
:::

{{< include _psam-gan.qmd >}}

{{< include _variational-autoencoder.qmd >}}


# Diffusion Models {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Using KerasCV

<center>

{{< video https://www.youtube.com/watch?v=pstsh2C2roc width="1000px" height="600px" >}}

</center>


# {data-visibility="uncounted"}

<h2>Glossary</h2>

::: columns
:::: column
- autoencoder (variational)
- beam search
- bias
- ChatGPT (& RLHF)
- DeepDream
- greedy sampling
::::
:::: column
- HuggingFace
- language model
- latent space
- neural style transfer
- softmax temperature
- stochastic sampling
::::
:::


<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
