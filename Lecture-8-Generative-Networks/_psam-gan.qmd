
## Example: PSAM 

```{python}
#| echo: false
#| output: false 
from tensorflow.keras.utils import image_dataset_from_directory

dataDir = "mandarin-split"
batchSize = 32
imgHeight = 80
imgWidth = 80
imgSize = (imgHeight, imgWidth)

trainDS = image_dataset_from_directory(
    dataDir + "/train",
    image_size=imgSize,
    batch_size=batchSize,
    shuffle=False,
    color_mode="grayscale")

valDS = image_dataset_from_directory(
    dataDir + "/val",
    image_size=imgSize,
    batch_size=batchSize,
    shuffle=False,
    color_mode="grayscale")

testDS = image_dataset_from_directory(
    dataDir + "/test",
    image_size=imgSize,
    batch_size=batchSize,
    shuffle=False,
    color_mode="grayscale")

# NB: Need shuffle=False earlier for these X & y to line up.
X_train = np.concatenate(list(trainDS.map(lambda x, y: x)))
y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))

X_val = np.concatenate(list(valDS.map(lambda x, y: x)))
y_val = np.concatenate(list(valDS.map(lambda x, y: y)))

X_test = np.concatenate(list(testDS.map(lambda x, y: x)))
y_test = np.concatenate(list(testDS.map(lambda x, y: y)))
```

Loading the dataset off-screen (using [Lecture 6 code](https://pat-laub.github.io/DeepLearningMaterials/Lecture-6-Computer-Vision/computer-vision.html#/downloading-the-dataset)).

::: columns
::: column
```{python}
plt.imshow(X_train[0], cmap="gray");
```
:::
::: column
```{python}
plt.imshow(X_train[42], cmap="gray");
```
:::
:::


## A compression game

::: columns
::: column
```{python}
plt.imshow(X_train[42], cmap="gray");
print(imgWidth * imgHeight)
```
:::
::: column
> _A 4 with a curly foot, a flat line goes across the middle of the 4, two feet come off the bottom._

96 characters

> _A Dōng character, rotated counterclockwise 15 degrees._

54 characters

:::
:::


## Make a basic autoencoder

```{python}
numHiddenLayer = 400
print(f"Compress from {imgHeight * imgWidth} pixels to {numHiddenLayer} latent variables.")
```

```{python}
random.seed(123)

model = keras.models.Sequential([
    layers.Rescaling(1./255, input_shape=(imgHeight, imgWidth, 1)),
    layers.Flatten(),
    layers.Dense(numHiddenLayer, "relu"),
    layers.Dense(imgHeight*imgWidth, "sigmoid"),
    layers.Reshape((imgHeight, imgWidth, 1)),
    layers.Rescaling(255),
])

model.compile("adam", "mse")
epochs = 1_000
es = keras.callbacks.EarlyStopping(
    patience=5, restore_best_weights=True)
model.fit(X_train, X_train, epochs=epochs, verbose=0,
    validation_data=(X_val, X_val), callbacks=es);
```

## The model

```{python}
model.summary(print_fn=skip_empty)
```

```{python}
model.evaluate(X_val, X_val, verbose=0)
```

## Some recovered image

```{python}
X_val_rec = model.predict(X_val, verbose=0)
```

::: columns
::: column
```{python}
plt.imshow(X_val[42], cmap="gray");
```
:::
::: column
```{python}
plt.imshow(X_val_rec[42], cmap="gray");
```
:::
:::

## Invert the images

::: columns
::: column
```{python}
plt.imshow(255 - X_train[0], cmap="gray");
```
:::
::: column
```{python}
plt.imshow(255 - X_train[42], cmap="gray");
```
:::
:::

## Try inverting the images

```{python}
random.seed(123)

model = keras.models.Sequential([
    layers.Rescaling(1./255, input_shape=(imgHeight, imgWidth, 1)),
    layers.Lambda(lambda x: 1 - x),
    layers.Flatten(),
    layers.Dense(numHiddenLayer, "relu"),
    layers.Dense(imgHeight*imgWidth, "sigmoid"),
    layers.Lambda(lambda x: 1 - x),
    layers.Reshape((imgHeight, imgWidth, 1)),
    layers.Rescaling(255),
])

model.compile("adam", "mse")
model.fit(X_train, X_train, epochs=epochs, verbose=0,
    validation_data=(X_val, X_val), callbacks=es);
```


<!-- and here (Trouble between here ... -->



## The model

```{python}
model.summary(print_fn=skip_empty)
```

```{python}
model.evaluate(X_val, X_val, verbose=0)
```


## Some recovered image

```{python}
X_val_rec = model.predict(X_val, verbose=0)
```

::: columns
::: column
```{python}
plt.imshow(X_val[42], cmap="gray");
```
:::
::: column
```{python}
plt.imshow(X_val_rec[42], cmap="gray");
```
:::
:::

## CNN-enhanced encoder

```{python}
random.seed(123)

encoder = keras.models.Sequential([
    layers.Rescaling(1./255, input_shape=(imgHeight, imgWidth, 1)),
    layers.Lambda(lambda x: 1 - x),
    layers.Conv2D(16, 3, padding="same", activation="relu"),
    layers.MaxPooling2D(),
    layers.Conv2D(32, 3, padding="same", activation="relu"),
    layers.MaxPooling2D(),
    layers.Conv2D(64, 3, padding="same", activation="relu"),
    layers.MaxPooling2D(),
    layers.Flatten(),
    layers.Dense(numHiddenLayer, "relu")
])
```

## CNN-enhanced decoder

```{python}
decoder = keras.models.Sequential([
    keras.Input(shape=(numHiddenLayer,)),
    layers.Dense(20*20),
    layers.Reshape((20, 20, 1)),
    layers.Conv2D(128, 3, padding="same", activation="relu"),
    layers.UpSampling2D(),
    layers.Conv2D(64, 3, padding="same", activation="relu"),
    layers.UpSampling2D(),
    layers.Conv2D(1, 1, padding="same", activation="relu"),
    layers.Lambda(lambda x: 1 - x),
    layers.Rescaling(255),
])

model = keras.models.Sequential([encoder, decoder])
model.compile("adam", "mse")
model.fit(X_train, X_train, epochs=epochs, verbose=0,
    validation_data=(X_val, X_val), callbacks=es);
```

## Encoder summary

```{python}
encoder.summary(print_fn=skip_empty)
```

## Decoder summary

```{python}
decoder.summary(print_fn=skip_empty)
```

```{python}
model.evaluate(X_val, X_val, verbose=0)
```

## Some recovered image

```{python}
X_val_rec = model.predict(X_val, verbose=0)
```

::: columns
::: column
```{python}
plt.imshow(X_val[42], cmap="gray");
```
:::
::: column
```{python}
plt.imshow(X_val_rec[42], cmap="gray");
```
:::
:::

## Latent space vs word embedding {.smaller}

- We revisit the concept of word embedding, where words in the vocabulary are mapped into vector representations. Words with similar meaning should lie close to one another in the word-embedding space.
- Latent space contains low-dimensional representation of data. Data/Images that are similar should lie close in the latent space.
- There are pre-trained word-embedding spaces such as those for English-language movie review, German-language legal documents, etc. Semantic relationships between words differ for different tasks. Similarly, the structure of latent spaces for different data sets (humans faces, animals, etc) are different.

## Latent space vs word embedding

- Given a latent space of representations, or an embedding space, certain directions in the space may encode interesting axes of variation in the original data. 
- A **concept vector** is a direction of variation in the data. For example there may be a smile vector such that if $z$ is the latent representation of a face, then $z+s$ is the representation of the same face, smiling. We can generate an image of the person smiling from this latent representation. 

## Intentionally add noise to inputs

::: columns
::: column
```{python}
mask = rnd.random(size=X_train.shape[1:]) < 0.5
plt.imshow(mask * (255 - X_train[0]), cmap="gray");
```
:::
::: column
```{python}
mask = rnd.random(size=X_train.shape[1:]) < 0.5
plt.imshow(mask * (255 - X_train[42]) * mask, cmap="gray");
```
:::
:::

## Denoising autoencoder

Can be used to do [feature engineering for supervised learning problems](https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/discussion/44629)

> It is also possible to include input variables as outputs to infer missing values or just help the model “understand” the features – in fact the winning solution of a claims prediction Kaggle competition heavily used denoising autoencoders together with model stacking and ensembling – read more here.

Jacky Poon

::: footer
Source: Poon (2021), [_Multitasking Risk Pricing Using Deep Learning_](https://actuariesinstitute.github.io/cookbook/docs/multitasking_risk_pricing.html), Actuaries' Analytical Cookbook.
:::