
# Transformers {visibility="uncounted"}

## Transformer architecture

> GPT makes use of a mechanism known as attention, which removes the need for recurrent layers (e.g., LSTMs). It works like an information retrieval system, utilizing queries, keys, and values to decide how much information it wants to extract from each input token.
>
>Attention heads can be grouped together to form what is known as a multihead attention layer. These are then wrapped up inside a Transformer block, which includes layer normalization and skip connections around the attention layer. Transformer blocks can be stacked to create very deep neural networks.

::: footer
Source: David Foster (2023), Generative Deep Learning, 2nd Edition, O'Reilly Media, Chapter 9.
:::

## Transformer architecture reference

<center>

{{< video https://www.youtube.com/watch?v=LE3NfEULV6k width="1000px" height="600px" >}}

</center>

## ü§ó Transformers package

```{python}
import transformers
from transformers import pipeline
generator = pipeline(task="text-generation", model="gpt2", revision="6c0e608")
```

```{python}
transformers.set_seed(1)
print(generator("It's the holidays so I'm going to enjoy")[0]["generated_text"])
```

```{python}
transformers.set_seed(1337)
print(generator("It's the holidays so I'm going to enjoy")[0]["generated_text"])
```

## Reading the course profile

```{python}
context = """
StoryWall Formative Discussions: An initial StoryWall, worth 2%, is due by noon on June 3. The following StoryWalls are worth 4% each (taking the best 7 of 9) and are due at noon on the following dates:
The project will be submitted in stages: draft due at noon on July 1 (10%), recorded presentation due at noon on July 22 (15%), final report due at noon on August 1 (15%).

As a student at UNSW you are expected to display academic integrity in your work and interactions. Where a student breaches the UNSW Student Code with respect to academic integrity, the University may take disciplinary action under the Student Misconduct Procedure. To assure academic integrity, you may be required to demonstrate reasoning, research and the process of constructing work submitted for assessment.
To assist you in understanding what academic integrity means, and how to ensure that you do comply with the UNSW Student Code, it is strongly recommended that you complete the Working with Academic Integrity module before submitting your first assessment task. It is a free, online self-paced Moodle module that should take about one hour to complete.

StoryWall (30%)

The StoryWall format will be used for small weekly questions. Each week of questions will be released on a Monday, and most of them will be due the following Monday at midday (see assessment table for exact dates). Students will upload their responses to the question sets, and give comments on another student's submission. Each week will be worth 4%, and the grading is pass/fail, with the best 7 of 9 being counted. The first week's basic 'introduction' StoryWall post is counted separately and is worth 2%.

Project (40%)

Over the term, students will complete an individual project. There will be a selection of deep learning topics to choose from (this will be outlined during Week 1).

The deliverables for the project will include: a draft/progress report mid-way through the term, a presentation (recorded), a final report including a written summary of the project and the relevant Python code (Jupyter notebook).

Exam (30%)

The exam will test the concepts presented in the lectures. For example, students will be expected to: provide definitions for various deep learning terminology, suggest neural network designs to solve risk and actuarial problems, give advice to mock deep learning engineers whose projects have hit common roadblocks, find/explain common bugs in deep learning Python code.
"""
```

## Question answering 
```{python}
qa = pipeline("question-answering", model="distilbert-base-cased-distilled-squad", revision="626af31")
```

```{python}
qa(question="What weight is the exam?", context=context)
```

```{python}
qa(question="What topics are in the exam?", context=context)
```

```{python}
qa(question="When is the presentation due?", context=context)
```

```{python}
qa(question="How many StoryWall tasks are there?", context=context)
```

## ChatGPT is Transformer + RLHF

> At the time of writing, there is no official paper that describes how ChatGPT works in detail, but from the official blog post we know that it uses a technique called reinforcement learning from human feedback (RLHF) to fine-tune the GPT-3.5 model.

::: footer
Source: David Foster (2023), Generative Deep Learning, 2nd Edition, O'Reilly Media, Chapter 9.
:::

## ChatGPT internals

![It uses a fair bit of human feedback](ChatGPT-Diagram.png)

::: footer
Source: [OpenAI blog](https://openai.com/blog/chatgpt).
:::

## ChatGPT

> While ChatGPT still has many limitations (such as sometimes ‚Äúhallucinating‚Äù factually incorrect information), it is a powerful example of how Transformers can be used to build generative models that can produce complex, long-ranging, and novel output that is often indistinguishable from human-generated text. The progress made thus far by models like ChatGPT serves as a testament to the potential of AI and its transformative impact on the world.

::: footer
Source: David Foster (2023), Generative Deep Learning, 2nd Edition, O'Reilly Media, Chapter 9.
:::

## Recommended reading {.smaller}

- The Verge (2022), [The Great Fiction of AI: The strange world of high-speed semi-automated genre fiction](https://www.theverge.com/c/23194235/ai-fiction-writing-amazon-kindle-sudowrite-jasper)
- Vaswani et al. (2017), [Attention Is All You Need](https://arxiv.org/pdf/1706.03762.pdf), NeurIPS
- Bommasani et al. (2021), [On the Opportunities and Risks of Foundation Models](https://arxiv.org/pdf/2108.07258.pdf)
- Gary Marcus (2022), [Deep Learning Is Hitting a Wall](https://nautil.us/deep-learning-is-hitting-a-wall-14467/), Nautilus article
- SDS 564, [Clem Delangue on Hugging Face and Transformers](https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000556643700)
- SDS 559, [GPT-3 for Natural Language Processing](https://podcasts.apple.com/au/podcast/super-data-science/id1163599059?i=1000554847681)
- Computerphile (2019), [AI Language Models & Transformers](https://youtu.be/rURRYI66E54) (20m)
- Computerphile (2020), [GPT3: An Even Bigger Language Model](https://youtu.be/_8yVOC4ciXc) (25m)
- Nicholas Renotte (2021), [AI Blog Post Summarization with Hugging Face Transformers...](https://youtu.be/JctmnczWg0U) (33m)
- Seattle Applied Deep Learning (2019), [LSTM is dead. Long Live Transformers!](https://youtu.be/S27pHKBEp30) (28m)
