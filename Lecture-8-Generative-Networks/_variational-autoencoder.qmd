# Variational Autoencoders {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Variational autoencoder

::: {.notes}
A slightly different sample from the distribution in the latent space will be decoded to a slightly different image. The stochasticity of this process improves robustness and forces the latent space to encode meaningful representation everywhere: every point in the latent space is decoded to a valid output. So the latent spaces of VAEs are continuous and highly-structured.
:::

![Schematic of a variational autoencoder.](chollet-VAE-blur.png)

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Figure 12.17.
:::

## VAE schematic process

![Keras code for a VAE.](chollet-VAEcode-blur.png)

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Unnumbered listing in Chapter 12.
:::

## Focus on the decoder

![Sampling new artificial images from the latent space.](chollet-latentspace-blur.png)

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Figure 12.13.
:::

## Exploring the MNIST latent space

![Example of MNIST-like images generated from the latent space.](chollet-VAEdecoded-blur.png)

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Figure 12.18.
:::
