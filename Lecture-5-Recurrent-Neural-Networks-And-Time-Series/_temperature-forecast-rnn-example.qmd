
# RNNs & CNNs For Time Series {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## The temperature in Jena, Germany

Temperature recorded every 10 minutes over 2009-2016 (8 years), which gives 420,551 data points.

::: {layout-ncol=2}
![All temperature data, yearly periodicity is observed.](chollet-temperatureall-blur.png)

![10-day data shows some daily periodicity.](chollet-temperature10days-blur.png)
:::

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Figures 10.1 & 10.2.
::: 

## A temperature-forecasting example

Given a few months of data, predict the average temperature for the next month. This is an easy problem due to the reliable year-scale periodicity of the data.

Given hourly temperature data of the previous 5 days, predict the temperature in 24 hours. This is a harder problem because on a daily scale, data is more chaotic.

## The baseline model

Build a model for 24-hour temperature forecast: Use a baseline model to evaluate the performance of the models that we build. 

For our forecast model, the baseline model is to set the temperature 24 hours from now to be equal to the temperature right now. The validation error (mean absolute error) of this model is then 2.44 degrees Celsius, and the test error is 2.62 degrees.

## A 24-hour forecast 

Neither the densely connected network or the 1D convnet work well.
Their validation errors are higher than the baseline model.

::: {layout-ncol=2}
![Using a densely connected network.](chollet-feedforwardresult-blur.png)

![Using a 1D convolutional network.](chollet-convoresult-blur.png)
:::

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Figures 10.3 & 10.4.
::: 

## Why the dense model doesn't work

For the 2-layer densely connected neural network, even though a good solution technically exists where the neural network finds the baseline solution and improves on it, finding such solution in the hypothesis space of all possible 2-layer neural network with the configuration we defined is sometimes like finding a needle in the haystack. Good feature engineering and relevant network architecture is important in that case: you need to tell the model precisely what it should be looking for.

## Why the CNN doesn't work

Order of data in the sequence (such as a time series) matters a lot. For temperature forecast, recent data is more informative for predicting the next day's temperature. 

Convnet is unable to preserve order. In the pooling layer such as max pooling, the max value from a grid is retrieved while discarding information about the exact location of the max value in the grid. By losing positional information, the network fails to capture information about the spatial or temporal relation between the inputs.

## Going forward & backwards in time

::: {layout-ncol=2}
![Using a dropout-regularised LSTM.](chollet-10-11-blur.png)

![Using an LSTM on reversed sequences.](chollet-10-13-blur.png)
:::

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Figures 10.11 & 10.13.
::: 

## Why the backwards direction fails

> The reversed-order LSTM strongly underperforms even the common-sense baseline, indicating that in this case, chronological processing is important to the success of the approach. This makes perfect sense: the underlying LSTM layer will typically be better at remembering the recent past than the distant past, and naturally the more recent weather data points are more predictive than older data points for the problem (that’s what makes the common-sense baseline fairly strong). Thus the chronological version of the layer is bound to outperform the reversed-order version.

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Chapter 10.
::: 

## Bidirectional RNN

::: columns
::: {.column width="40%"}
![Illustration of a bidirectional RNN.](chollet-10-14-blur.png)
:::
::: {.column width="60%"}
Wrap a normal RNN layer inside the `Bidirectional` layer to get Keras to go forward & backwards in time.

```python
from tensorflow.keras.layers \
    import Bidirectional

inputs = Input(shape=inpShape)
x = Bidirectional(LSTM(16))(inputs)
outputs = Dense(1)(x)
model = Model(inputs, outputs) 
```
:::
:::

::: footer
Source: François Chollet (2021), _Deep Learning with Python_, Second Edition, Chapter 10.
::: 