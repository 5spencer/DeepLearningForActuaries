---
title: Recurrent Neural Networks
subtitle: "ACTL3143/5111: Deep Learning for Actuaries"
author: Dr Patrick Laub
date: Week 5
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    footer: "Slides: [Dr Patrick Laub](https://pat-laub.github.io) (@PatrickLaub)."
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    transition: none
    slide-number: c/t
    strip-comments: true
    preview-links: false
    margin: 0.2
    chalkboard:
      boardmarker-width: 6
      grid: false
      background:
        - "rgba(255,255,255,0.0)"
        - "https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"
    include-before: <div class="line right"></div>
    include-after: <script>registerRevealCallbacks();</script>
highlight-style: breeze
jupyter: python3
execute:
  keep-ipynb: true
  echo: true
---

```{python}
#| echo: false
import matplotlib

import cycler
colors = ["#91CCCC", "#FF8FA9", "#CC91BC", "#3F9999", "#A5FFB8"]
matplotlib.pyplot.rcParams["axes.prop_cycle"] = cycler.cycler(color=colors)

def set_square_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (2.0, 2.0)
  # matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 3.0)
  # matplotlib.pyplot.rcParams['figure.dpi'] = 350

def set_rectangular_figures():
  matplotlib.pyplot.rcParams['figure.figsize'] = (5.0, 2.0)

set_rectangular_figures()
matplotlib.pyplot.rcParams['figure.dpi'] = 350
matplotlib.pyplot.rcParams['savefig.bbox'] = "tight"
matplotlib.pyplot.rcParams['font.family'] = "serif"

matplotlib.pyplot.rcParams['axes.spines.right'] = False
matplotlib.pyplot.rcParams['axes.spines.top'] = False

def squareFig():
    return matplotlib.pyplot.figure(figsize=(2, 2), dpi=350).gca()

def add_diagonal_line():
    xl = matplotlib.pyplot.xlim()
    yl = matplotlib.pyplot.ylim()

    minLeft = min(xl[0], yl[0])
    maxRight = max(xl[1], yl[1])

    matplotlib.pyplot.plot([minLeft, maxRight], [minLeft, maxRight], color="black", linestyle="--")

    matplotlib.pyplot.xlim([minLeft, maxRight])
    matplotlib.pyplot.ylim([minLeft, maxRight])
    
    # shortestLeftSide = max(xl[0], yl[0])
    # shortestRightSide = min(xl[1], yl[1])
    # matplotlib.pyplot.plot([shortestLeftSide, shortestRightSide], [shortestLeftSide, shortestRightSide], color="black", linestyle="--")

import pandas
pandas.options.display.max_rows = 4

import numpy
numpy.set_printoptions(precision=2)
numpy.random.seed(123)

import tensorflow
tensorflow.random.set_seed(1)
tensorflow.config.set_visible_devices([], 'GPU')

tensorflow.get_logger().setLevel('ERROR')
```

## Lecture Outline

- **Guest lecture: Jacky Poon**
- Demo: Australian House Prices 
- Recurrent neural networks

## Load packages {data-visibility="uncounted"}

<br>
<br>

```{python}
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import tensorflow as tf

from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.callbacks import EarlyStopping

%load_ext watermark
%watermark -p numpy,pandas,tensorflow
```

# Tensors {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Shapes of data

![Illustration of tensors of different rank.](medium-tensor-rank.png)

::: footer
Source: Paras Patidar (2019), [Tensors — Representation of Data In Neural Networks](https://medium.com/mlait/tensors-representation-of-data-in-neural-networks-bbe8a711b93b), Medium article.
:::

## Shapes of photos

![A photo is a rank 3 tensor.](rgb-channels.png)

::: footer
Source: Kim et al (2021), [Data Hiding Method for Color AMBTC Compressed Images Using Color Difference](https://www.mdpi.com/applsci/applsci-11-03418/article_deploy/html/images/applsci-11-03418-g001.png), Applied Sciences.
:::

## The `axis` argument in numpy

Starting with a $(3, 4)$-shaped matrix:
```{python}
X = np.arange(12).reshape(3,4)
X
```

::: columns
::: column
`axis=0`: $(3, 4) \leadsto (4,)$.
```{python}
X.sum(axis=0)
```
:::
::: column
`axis=1`: $(3, 4) \leadsto (3,)$.
```{python}
X.prod(axis=1)
```
:::
:::

The return value's rank is one less than the input's rank.

::: {.callout-important}
The `axis` parameter tells us which dimension is removed.
:::

## Using `axis` & `keepdims`

With `keepdims=True`, the rank doesn't change.

```{python}
X = np.arange(12).reshape(3,4)
X
```

::: columns
::: column
`axis=0`: $(3, 4) \leadsto (1, 4)$.
```{python}
X.sum(axis=0, keepdims=True)
```
:::
::: column
`axis=1`: $(3, 4) \leadsto (3, 1)$.
```{python}
X.prod(axis=1, keepdims=True)
```
:::
:::

::: columns
::: column
```{python}
#| error: true
X / X.sum(axis=1)
```
:::
::: column
```{python}
X / X.sum(axis=1, keepdims=True)
```
:::
:::

## The rank of a time series

Say we had $n$ observations of a time series $x_1, x_2, \dots, x_n$. 

This $\mathbf{x} = (x_1, \dots, x_n)$ would have shape $(n,)$ & rank 1.

If instead we had a batch of $b$ time series'

$$
\mathbf{X} = \begin{pmatrix}
x_7 & x_8 & \dots & x_{7+n-1} \\
x_2 & x_3 & \dots & x_{2+n-1} \\
\vdots & \vdots & \ddots & \vdots \\
x_3 & x_4 & \dots & x_{3+n-1} \\
\end{pmatrix}  \,,
$$

the batch $\mathbf{X}$ would have shape $(b, n)$ & rank 2.

## Multivariate time series

::: columns
::: {.column width="35%"}

<center>

```{python}
#| echo: false
from IPython.display import Markdown
t = range(4)
x = [f"$x_{i}$" for i in t]
y = [f"$y_{i}$" for i in t]
df = pandas.DataFrame({"$x$": x, "$y$": y})
df.index.name='$t$'
Markdown(df.to_markdown())
```

</center>

:::
::: {.column width="65%"}
Say $n$ observations of the $m$ time series, would be a shape $(n, m)$ matrix of rank 2.

In Keras, a batch of $b$ of these time series has shape $(b, n, m)$ and has rank 3.
:::
:::

::: {.callout-note}
Use $\mathbf{x}_t \in \mathbb{R}^{1 \times m}$ to denote the vector of all time series at time $t$.
Here, $\mathbf{x}_t = (x_t, y_t)$.
:::


# Recurrent Neural Networks {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Basic facts of RNNs

- A recurrent neural network is a type of neural network that is designed to process sequences of data (e.g. time series, sentences).
- A recurrent neural network is any network that contains a recurrent layer.
- A recurrent layer is a layer that processes data in a sequence.
- An RNN can have one or more recurrent layers.

## Diagram of an RNN

The RNN processes each data in the sequence one by one, while keeping memory of what came before.

![Illustration of *unrolling the network through time*.](Geron-mls2_1501-blur.png)

Common RNN structures are: LSTM and GRU cells.

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figure 15-1 (__redacted__).
:::


## Australian House Price Indices

```{python}
#|echo: false
housePrices = pd.read_csv("sa3-capitals-house-price-index.csv", index_col=0)
housePrices.index = pd.to_datetime(housePrices.index)
housePrices.index.name = "Date"
housePrices = housePrices[housePrices.index > pd.to_datetime("1990")]
housePrices.columns = ["Brisbane", "East_Bris", "North_Bris", "West_Bris",
  "Melbourne", "North_Syd", "Sydney"]
housePrices.plot(legend=False);
```

## Percentage changes {.smaller}

```{python}
changes = housePrices.pct_change().dropna()
changes.round(2)
```

```{python}
#| echo: false
pandas.options.display.max_rows = 7
``` 

## Percentage changes 


```{python}
#| eval: false
changes.plot();
```
```{python}
#| echo: false
changes.plot(lw=1);
matplotlib.pyplot.legend(bbox_to_anchor=(0.5, 1.0), loc="lower center", frameon=False, ncol=3);
```

## The size of the changes

::: columns
::: column
```{python}
changes.mean()
```

```{python}
changes *= 100
```

```{python}
changes.mean()
```
:::
::: column
```{python}
#| echo: false
# set_square_figures() 
```
```{python}
changes.plot(legend=False);
```
:::
:::

## Split _without_ shuffling

```{python}
numTrain = int(0.6 * len(changes))
numVal = int(0.2 * len(changes))
numTest = len(changes) - numTrain - numVal
print(f"# Train: {numTrain}, # Val: {numVal}, # Test: {numTest}")
```

```{python}
#| echo: false
changes.iloc[:numTrain,0].plot(c=colors[0], lw=1, alpha=0.5);
changes.iloc[numTrain:(numTrain+numVal),0].plot(c=colors[1], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[(numTrain+numVal):,0].plot(c=colors[2], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[:numTrain,1:].plot(c=colors[0], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[numTrain:(numTrain+numVal),1:].plot(c=colors[1], ax=plt.gca(), lw=1, alpha=0.5);
changes.iloc[(numTrain+numVal):,1:].plot(c=colors[2], ax=plt.gca(), lw=1, alpha=0.5);
plt.legend(["Train", "Val", "Test"], frameon=False, ncol=3);
```

```{python}
#| echo: false
set_rectangular_figures() 
```

## Subsequences of a time series

Keras has a built-in method for converting a time series into subsequences/chunks.

```{python}
from tensorflow.keras.utils import timeseries_dataset_from_array

integers = range(10)                                
dummyDataset = timeseries_dataset_from_array(
    data=integers[:-3],                                 
    targets=integers[3:],                               
    sequence_length=3,                                      
    batch_size=2,                                           
)

for inputs, targets in dummyDataset:
    for i in range(inputs.shape[0]):
        print([int(x) for x in inputs[i]], int(targets[i]))
```

::: footer
Source: Code snippet in Chapter 10 of Chollet.
:::

# Predicting Sydney House Prices {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}


## Creating dataset objects

::: columns
::: column
```{python}
# Num. of input time series.
numTS = changes.shape[1]

# How many prev. months to use.
seqLength = 6

# Predict the next month ahead.
ahead = 1

# The index of the first target.
delay = (seqLength+ahead-1)
```
:::
::: column
```{python}
# Which suburb to predict.
targetSuburb = changes["Sydney"]

trainDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=targetSuburb[delay:],
    sequence_length=seqLength,
    end_index=numTrain)
```
:::
:::

::: columns
::: column
```{python}
valDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=targetSuburb[delay:],
    sequence_length=seqLength,
    start_index=numTrain,
    end_index=numTrain+numVal)
```
:::
::: column
```{python}
testDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=targetSuburb[delay:],
    sequence_length=seqLength,
    start_index=numTrain+numVal)
```
:::
:::

## Converting `Dataset` to numpy

The `Dataset` object can be handed to Keras directly, but if we really need a numpy array, we can run:
```{python}
X_train = np.concatenate(list(trainDS.map(lambda x, y: x)))
y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))
```

The shape of our training set is now:

```{python}
X_train.shape
```

```{python}
y_train.shape
```

Later, we need the targets as numpy arrays:

```{python}
y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))
y_val = np.concatenate(list(valDS.map(lambda x, y: y)))
y_test = np.concatenate(list(testDS.map(lambda x, y: y)))
```

## A dense network

```{python}
from tensorflow.keras.layers import Input, Flatten
tf.random.set_seed(1)
modelDense = Sequential([
    Input(shape=(seqLength, numTS)),
    Flatten(),
    Dense(50, activation="leaky_relu"),
    Dense(20, activation="leaky_relu"),
    Dense(1, activation="linear")
])
modelDense.compile(loss="mse", optimizer="adam")
print(f"This model has {modelDense.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = modelDense.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```

## Plot the model
```{python}
from tensorflow.keras.utils import plot_model
plot_model(modelDense, show_shapes=True)
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelDense.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelDense.predict(valDS)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="Dense")
plt.legend(frameon=False);
```

## A `SimpleRNN` layer

```{python}
from tensorflow.keras.layers import SimpleRNN

tf.random.set_seed(1)

modelSimple = Sequential([
    SimpleRNN(50, input_shape=(seqLength, numTS)),
    Dense(1, activation="linear")
])
modelSimple.compile(loss="mse", optimizer="adam")
print(f"This model has {modelSimple.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = modelSimple.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelSimple.evaluate(valDS, verbose=0)
```

## Plot the model
```{python}
plot_model(modelSimple, show_shapes=True)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelSimple.predict(valDS)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="SimpleRNN")
plt.legend(frameon=False);
```

## A `LSTM` layer

```{python}
from tensorflow.keras.layers import LSTM

tf.random.set_seed(1)

modelLSTM = Sequential([
    LSTM(50, input_shape=(seqLength, numTS)),
    Dense(1, activation="linear")
])

modelLSTM.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelLSTM.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelLSTM.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelLSTM.predict(valDS)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="LSTM")
plt.legend(frameon=False);
```

## A `GRU` layer

```{python}
from tensorflow.keras.layers import GRU

tf.random.set_seed(1)

modelGRU = Sequential([
    GRU(50, input_shape=(seqLength, numTS)),
    Dense(1, activation="linear")
])

modelGRU.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelGRU.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelGRU.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelGRU.predict(valDS)
# plt.scatter(y_val, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="GRU")
plt.legend(frameon=False);
```

## Two `GRU` layers

```{python}
tf.random.set_seed(1)

modelTwoGRUs = Sequential([
    GRU(50, input_shape=(seqLength, numTS), return_sequences=True),
    GRU(50),
    Dense(1, activation="linear")
])

modelTwoGRUs.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelTwoGRUs.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelTwoGRUs.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

```{python}
#| echo: false
y_pred = modelTwoGRUs.predict(valDS)
# plt.scatter(y_val, y_pred)
# plt.show()

plt.plot(y_val, label="Sydney")
plt.plot(y_pred, label="2 GRUs")
plt.legend(frameon=False);
```

## Compare the models

```{python}
#| echo: false
models = [modelDense, modelSimple, modelLSTM, modelGRU, modelTwoGRUs]
modelNames = ["Dense", "SimpleRNN", "LSTM", "GRU", "2 GRUs"]
mseVal = {name: model.evaluate(valDS, verbose=0) for name, model in zip(modelNames, models)}
valResults = pd.DataFrame({
    "Model": mseVal.keys(), "MSE": mseVal.values()
})
valResults.sort_values("MSE", ascending=False)
```

The network with a single GRU layer is the best. 

```{python}
modelGRU.evaluate(testDS, verbose=0)
```

## Test set

```{python}
#| echo: false
y_pred = modelGRU.predict(testDS)
# plt.scatter(y_test, y_pred)
# add_diagonal_line()
# plt.show()

plt.plot(y_test, label="Sydney")
plt.plot(y_pred, label="GRU")
plt.legend(frameon=False);
```

# Predicting Multiple Time Series {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Creating dataset objects

::: columns
::: column
Change the `targets` argument to include all the suburbs.
:::
::: column
```{python}
trainDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seqLength,
    end_index=numTrain)
```
:::
:::

::: columns
::: column
```{python}
valDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seqLength,
    start_index=numTrain,
    end_index=numTrain+numVal)
```
:::
::: column
```{python}
testDS = \
  timeseries_dataset_from_array(
    changes[:-delay],
    targets=changes[delay:],
    sequence_length=seqLength,
    start_index=numTrain+numVal)
```
:::
:::

## Converting `Dataset` to numpy

The shape of our training set is now:

```{python}
X_train = np.concatenate(list(trainDS.map(lambda x, y: x)))
X_train.shape
```

```{python}
Y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))
Y_train.shape
```

Later, we need the targets as numpy arrays:

```{python}
Y_train = np.concatenate(list(trainDS.map(lambda x, y: y)))
Y_val = np.concatenate(list(valDS.map(lambda x, y: y)))
Y_test = np.concatenate(list(testDS.map(lambda x, y: y)))
```

## A dense network

```{python}
tf.random.set_seed(1)
modelDense = Sequential([
    Input(shape=(seqLength, numTS)),
    Flatten(),
    Dense(50, activation="leaky_relu"),
    Dense(20, activation="leaky_relu"),
    Dense(numTS, activation="linear")
])
modelDense.compile(loss="mse", optimizer="adam")
print(f"This model has {modelDense.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = modelDense.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```

## Plot the model
```{python}
plot_model(modelDense, show_shapes=True)
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelDense.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelDense.predict(valDS)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="Dense")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="Dense")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="Dense")
plt.legend(frameon=False);
```
:::
:::



## A `SimpleRNN` layer

```{python}
tf.random.set_seed(1)

modelSimple = Sequential([
    SimpleRNN(50, input_shape=(seqLength, numTS)),
    Dense(numTS, activation="linear")
])
modelSimple.compile(loss="mse", optimizer="adam")
print(f"This model has {modelSimple.count_params()} parameters.")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)
%time hist = modelSimple.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```

## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelSimple.evaluate(valDS, verbose=0)
```

## Plot the model
```{python}
plot_model(modelSimple, show_shapes=True)
```


## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelSimple.predict(valDS)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="SimpleRNN")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="SimpleRNN")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="SimpleRNN")
plt.legend(frameon=False);
```
:::
:::


## A `LSTM` layer

```{python}
tf.random.set_seed(1)

modelLSTM = Sequential([
    LSTM(50, input_shape=(seqLength, numTS)),
    Dense(numTS, activation="linear")
])

modelLSTM.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelLSTM.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0);
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelLSTM.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelLSTM.predict(valDS)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="LSTM")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="LSTM")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="LSTM")
plt.legend(frameon=False);
```
:::
:::

## A `GRU` layer

```{python}
tf.random.set_seed(1)

modelGRU = Sequential([
    GRU(50, input_shape=(seqLength, numTS)),
    Dense(numTS, activation="linear")
])

modelGRU.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelGRU.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelGRU.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelGRU.predict(valDS)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="GRU")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="GRU")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="GRU")
plt.legend(frameon=False);
```
:::
:::

## Two `GRU` layers

```{python}
tf.random.set_seed(1)

modelTwoGRUs = Sequential([
    GRU(50, input_shape=(seqLength, numTS), return_sequences=True),
    GRU(50),
    Dense(numTS, activation="linear")
])

modelTwoGRUs.compile(loss="mse", optimizer="adam")

es = EarlyStopping(patience=50, restore_best_weights=True, verbose=1)

%time hist = modelTwoGRUs.fit(trainDS, epochs=1_000, \
  validation_data=valDS, callbacks=[es], verbose=0)
```


## Assess the fits

```{python}
#| echo: false
plt.plot(hist.history["loss"], label="Train")
plt.plot(hist.history["val_loss"], label="Val")
plt.legend(frameon=False);
```

```{python}
modelTwoGRUs.evaluate(valDS, verbose=0)
```

## Plotting the predictions {.smaller}

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelTwoGRUs.predict(valDS)
plt.scatter(Y_val, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_val[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="2 GRUs")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_val[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="2 GRUs")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_val[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="2 GRUs")
plt.legend(frameon=False);
```
:::
:::

## Compare the models

```{python}
#| echo: false
models = [modelDense, modelSimple, modelLSTM, modelGRU, modelTwoGRUs]
modelNames = ["Dense", "SimpleRNN", "LSTM", "GRU", "2 GRUs"]
mseVal = {name: model.evaluate(valDS, verbose=0) for name, model in zip(modelNames, models)}
valResults = pd.DataFrame({
    "Model": mseVal.keys(), "MSE": mseVal.values()
})
valResults.sort_values("MSE", ascending=False)
```

The network with two GRU layers is the best. 

```{python}
modelTwoGRUs.evaluate(testDS, verbose=0)
```

## Test set

::: columns
::: column
```{python}
#| echo: false
Y_pred = modelTwoGRUs.predict(testDS)
plt.scatter(Y_test, Y_pred)
add_diagonal_line()
plt.show()

plt.plot(Y_test[:,4], label="Melbourne")
plt.plot(Y_pred[:,4], label="GRU")
plt.legend(frameon=False);
```

:::
::: column
```{python}
#| echo: false
plt.plot(Y_test[:,0], label="Brisbane")
plt.plot(Y_pred[:,0], label="GRU")
plt.legend(frameon=False);
plt.show()

plt.plot(Y_test[:,6], label="Sydney")
plt.plot(Y_pred[:,6], label="GRU")
plt.legend(frameon=False);
```
:::
:::

## Quiz

Say $X$ is a batch of time series with shape $(4,3,2)$.
```{python}
#| echo: false
X = np.arange(4*3*2).reshape(4,3,2)
#X.shape
```

::: columns
:::: column
```{python}
#| eval: false
X[0,0,0]
```
::::: fragment
```{python}
#| echo: false
X[0,0,0]
```
:::::
::::

:::: column
```{python}
#| eval: false
X[0,1,2]
```
::::: fragment
```{python}
#| echo: false
#| error: true
X[0,1,2]
```
:::::
::::
:::


::: columns
:::: column
```{python}
#| eval: false
X[1].shape
```
::::: fragment
```{python}
#| echo: false
X[1].shape
```
:::::
::::

:::: column
```{python}
#| eval: false
X[:,-1,:].shape
```
::::: fragment
```{python}
#| echo: false
X[:,-1,:].shape
```
:::::
::::
:::


::: columns
:::: column
```{python}
#| eval: false
np.max(X, axis=1) 
```
::::: fragment
```{python}
#| echo: false
np.max(X, axis=1)
```
:::::
::::
:::: column
```{python}
#| eval: false
#| error: true
X.median(axis=2)
```
::::: fragment
```{python}
#| echo: false
#| error: true
X.median(axis=2)
```
:::::
::::
:::

# Dense Layers in Matrices {data-background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Logistic regression

```{python}
#| echo: false
set_square_figures()
```

::: columns
::: column

Observations: $\mathbf{x}_{i,\bullet} \in \mathbb{R}^{2}$.

Target: $y_i \in \{0, 1\}$.

Predict: $\hat{y}_i = \mathbb{P}(Y_i = 1)$.

<br>

__The model__

For $\mathbf{x}_{i,\bullet} = (x_{i,1}, x_{i,2})$:
$$
z_i = x_{i,1} w_1 + x_{i,2} w_2 + b
$$

$$
\hat{y}_i = \sigma(z_i) = \frac{1}{1 + \mathrm{e}^{-z_i}} .
$$

:::
::: column

```{python}
import sympy
sympy.plot("1/(1 + exp(-z))");
```
:::
:::

```{python}
#| echo: false
set_rectangular_figures()
```

## Multiple observations

```{python}
data = pd.DataFrame({"x_1": [1, 3, 5], "x_2": [2, 4, 6], "y": [0, 1, 1]})
data
```

Let $w_1 = 1$, $w_2 = 2$ and $b = -10$.

```{python}
w_1 = 1; w_2 = 2; b = -10
data["x_1"] * w_1 + data["x_2"] * w_2 + b 
```

## Matrix notation

::: columns
::: column
Have $\mathbf{X} \in \mathbb{R}^{3 \times 2}$.

```{python}
X_df = data[["x_1", "x_2"]]
X = X_df.to_numpy()
X
```
:::
::: column
Let $\mathbf{w} = (w_1, w_2)^\top \in \mathbb{R}^{2 \times 1}$.

```{python}
w = np.array([[1], [2]])
w
```
:::
:::

$$
\mathbf{z} = \mathbf{X} \mathbf{w} + b , \quad \mathbf{a} = \sigma(\mathbf{z})
$$

::: columns
::: column
```{python}
z = X.dot(w) + b
z
```
:::
::: column
```{python}
1 / (1 + np.exp(-z))
```
:::
:::

## In Keras

```{python}
#| echo: false
tf.random.set_seed(1234) 
```

```{python}
model = Sequential([
    Dense(1, input_dim=2, activation='sigmoid'),
])

w, b = model.get_weights()
print(f"w's shape is {w.shape}, b's shape is {b.shape}")
```

::: columns
::: column
```{python}
w
```
:::
::: column
```{python}
b
```
:::
:::

```{python}
model(X)
```

## In Keras with fixed weights & bias

```{python}
from tensorflow.keras.initializers import Constant
```

```{python}
model = Sequential([
    Dense(1, input_dim=2, activation='sigmoid',
      kernel_initializer=Constant(value=[1.0, 2.0]),
      bias_initializer=Constant(value=-10))
])

w, b = model.get_weights()
```

::: columns
::: column
```{python}
w
```
:::
::: column
```{python}
b
```
:::
:::

```{python}
model(X)
```

## Using a softmax output

::: columns
::: column
Observations: $\mathbf{x}_{i,\bullet} \in \mathbb{R}^{2}$.
Predict: $\hat{y}_{i,j} = \mathbb{P}(Y_i = j)$.
:::
::: column
Target: $\mathbf{y}_{i,\bullet} \in \{(1, 0), (0, 1)\}$.
:::
:::

__The model__: For $\mathbf{x}_{i,\bullet} = (x_{i,1}, x_{i,2})$
$$
\begin{aligned}
z_{i,1} &= x_{i,1} w_{1,1} + x_{i,2} w_{2,1} + b_1 , \\
z_{i,2} &= x_{i,1} w_{1,2} + x_{i,2} w_{2,2} + b_2 .
\end{aligned}
$$

$$
\begin{aligned}
\hat{y}_{i,1} &= \text{Softmax}_1(\mathbf{z}_i) = \frac{\mathrm{e}^{z_{i,1}}}{\mathrm{e}^{z_{i,1}} + \mathrm{e}^{z_{i,2}}} , \\
\hat{y}_{i,2} &= \text{Softmax}_2(\mathbf{z}_i) = \frac{\mathrm{e}^{z_{i,2}}}{\mathrm{e}^{z_{i,1}} + \mathrm{e}^{z_{i,2}}} .
\end{aligned}
$$

## Multiple observations

::: columns
::: column
```{python}
#| echo: false
data = pd.DataFrame({
  "x_1": [1, 3, 5], "x_2": [2, 4, 6],
  "y_1": [1, 0, 0], "y_2": [0, 1, 1]})
```

```{python}
data
```
:::
::: column
Choose:

$w_{1,1} = 1$, $w_{2,1} = 2$,

$w_{1,2} = 3$, $w_{2,2} = 4$, and

$b_1 = -10$, $b_2 = -20$.

:::
:::

```{python}
w_11 = 1; w_21 = 2; b_1 = -10
w_12 = 3; w_22 = 4; b_2 = -20
data["x_1"] * w_11 + data["x_2"] * w_21 + b_1
```

## Matrix notation

::: columns
::: column
Have $\mathbf{X} \in \mathbb{R}^{3 \times 2}$.

```{python}
X
```
:::
::: column
$\mathbf{W}\in \mathbb{R}^{2\times2}$, $\mathbf{b}\in \mathbb{R}^{2}$


```{python}
W = np.array([[1, 3], [2, 4]])
b = np.array([-10, -20])
display(W); b
```
:::
:::

$$
  \mathbf{Z} = \mathbf{X} \mathbf{W} + \mathbf{b} , \quad \mathbf{A} = \text{Softmax}(\mathbf{Z}) .
$$

::: columns
::: column
```{python}
Z = X @ W + b
Z
```
:::
::: column
```{python}
np.exp(Z) / np.sum(np.exp(Z),
  axis=1, keepdims=True)
```
:::
:::

## In Keras

```{python}
#| echo: false
tf.random.set_seed(1234) 
```

```{python}
model = Sequential([
    Dense(2, input_dim=2, activation='softmax'),
])

W, b = model.get_weights()
print(f"W's shape is {W.shape}, b's shape is {b.shape}")
```

::: columns
::: column
```{python}
W
```
:::
::: column
```{python}
b
```
:::
:::

::: columns
::: column
```{python}
model(X)
```
:::
::: column
```{python}
tf.reduce_sum(model(X),
    axis=1, keepdims=True)
```
:::
:::

## In Keras with fixed weights & bias

```{python}
model = Sequential([
    Dense(2, input_dim=2, activation='softmax',
      kernel_initializer=Constant(value=[[1.0, 3.0], [2.0, 4.0]]),
      bias_initializer=Constant(value=[-10, -20]))
])

W, b = model.get_weights()
```

::: columns
::: column
```{python}
W
```
:::
::: column
```{python}
b
```
:::
:::

::: columns
::: column
```{python}
model(X)
```
:::
::: column
```{python}
tf.reduce_sum(model(X),
    axis=1, keepdims=True)
```
:::
:::

## Quiz

Given a batch of time series in `X`, how would you get get:

1. A $(b, n)$-shaped matrix which is the time series holding the point-wise average of the original $m$ time series'?
2. A $(b, n, 1)$-shaped tensor of the minimum of each time series?
3. A $(b,)$-shaped vector of the average of each observation?


# SimpleRNN {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Recurrence relation

<br>

> A recurrence relation is an equation that expresses each element of a sequence as a function of the preceding ones. More precisely, in the case where only the immediately preceding element is involved, a recurrence relation has the form
> 
> $$ u_n = \psi(n, u_{n-1}) \quad \text{ for } \quad n > 0.$$

<br>

__Example__: Factorial $n! = n (n-1)!$ for $n > 0$ given $0! = 1$.

::: footer
Source: Wikipedia, [Recurrence relation](https://en.wikipedia.org/wiki/Recurrence_relation#Definition). 
:::

## A SimpleRNN cell.

![Diagram of a SimpleRNN cell.](colah-LSTM3-SimpleRNN.png)

<br>

All the outputs before the final one are often discarded.

::: footer
Source: Christopher Olah (2015), [Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs), Colah's Blog.
:::

## SimpleRNN

Say each prediction is a vector of size $d$, so $\mathbf{y}_t \in \mathbb{R}^{1 \times d}$.

Then the main equation of a SimpleRNN, given $\mathbf{y}_0 = \mathbf{0}$, is

$$ \mathbf{y}_t = \psi\bigl( \mathbf{x}_t \mathbf{W}_x + \mathbf{y}_{t-1} \mathbf{W}_y + \mathbf{b} \bigr) . $$

Here,
$$
\begin{aligned}
&\mathbf{x}_t \in \mathbb{R}^{1 \times m}, \mathbf{W}_x \in \mathbb{R}^{m \times d}, \\
&\mathbf{y}_{t-1} \in \mathbb{R}^{1 \times d}, \mathbf{W}_y \in \mathbb{R}^{d \times d}, \text{ and } \mathbf{b} \in \mathbb{R}^{d}.
\end{aligned}
$$

## SimpleRNN (in batches)

Say we operate on batches of size $b$, then $\mathbf{Y}_t \in \mathbb{R}^{b \times d}$.

Then the main equation of a SimpleRNN, given $\mathbf{Y}_0 = \mathbf{0}$, is

$$ \mathbf{Y}_t = \psi\bigl( \mathbf{X}_t \mathbf{W}_x + \mathbf{Y}_{t-1} \mathbf{W}_y + \mathbf{b} \bigr) . $$


Here,
$$
\begin{aligned}
&\mathbf{X}_t \in \mathbb{R}^{b \times m}, \mathbf{W}_x \in \mathbb{R}^{m \times d}, \\
&\mathbf{Y}_{t-1} \in \mathbb{R}^{b \times d}, \mathbf{W}_y \in \mathbb{R}^{d \times d}, \text{ and } \mathbf{b} \in \mathbb{R}^{d}.
\end{aligned}
$$

::: fragment
::: {.callout-note}
Remember, $\mathbf{X} \in \mathbb{R}^{b \times n \times m}$, $\mathbf{Y} \in \mathbb{R}^{b \times d}$, and $\mathbf{X}_t$ is equivalent to `X[:, t, :]`.
:::
:::

## Simple Keras demo

```{python}
numObs = 4
numTimeSteps = 3
numTimeSeries = 2

X = np.arange(numObs*numTimeSteps*numTimeSeries).astype(np.float32) \
        .reshape([numObs, numTimeSteps, numTimeSeries])

outputSize = 1
y = np.array([0, 0, 1, 1])
```

::: columns
::: column
```{python}
X[:2]
```
:::
::: column
```{python}
X[2:]
```
:::
:::


## Keras' SimpleRNN

As usual, the `SimpleRNN` is just a layer in Keras. 

```{python}
tf.random.set_seed(1234)
model = Sequential([
  SimpleRNN(outputSize, activation="sigmoid")
])
model.compile(loss="binary_crossentropy", metrics=["accuracy"])

hist = model.fit(X, y, epochs=500, verbose=False)
model.evaluate(X, y, verbose=False)
```

The predicted probabilities on the training set are:

```{python}
model.predict(X)
```

## SimpleRNN weights
```{python}
model.get_weights()
```

```{python}
def sigmoid(x):
  return 1 / (1 + np.exp(-x))

W_x, W_y, b = model.get_weights()

Y = np.zeros((numObs, outputSize), dtype=np.float32)
for t in range(numTimeSteps):
    X_t = X[:, t, :]
    z = X_t @ W_x + Y @ W_y + b
    Y = sigmoid(z)

Y
```

# {data-visibility="uncounted"} 

<h2>Glossary</h2>

- dimensions (tensor)
- GRU
- LSTM
- rank (tensor)
- recurrent neural networks
- SimpleRNN

```{python}
#| echo: false
!rm model.png
```

<script defer>
    // Remove the highlight.js class for the 'compile', 'min', 'max'
    // as there's a bug where they are treated like the Python built-in
    // global functions but we only ever see it as methods like
    // 'model.compile()' or 'predictions.max()'
    buggyBuiltIns = ["compile", "min", "max", "round", "sum"];

    document.querySelectorAll('.bu').forEach((elem) => {
        if (buggyBuiltIns.includes(elem.innerHTML)) {
            elem.classList.remove('bu');
        }
    })

    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>
