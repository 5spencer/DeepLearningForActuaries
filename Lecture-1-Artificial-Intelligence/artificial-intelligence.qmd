---
title: Artificial Intelligence
subtitle: ACTL3143 & ACTL5111 Deep Learning for Actuaries
author: Dr Patrick Laub
format:
  revealjs:
    theme: [serif, custom.scss]
    controls: true
    controls-tutorial: true
    logo: unsw-logo.svg
    footer: "Slides: [Dr Patrick Laub](https://pat-laub.github.io) (@PatrickLaub)."
    title-slide-attributes:
      data-background-image: unsw-yellow-shape.png
      data-background-size: contain !important
    transition: none
    slide-number: c/t
    strip-comments: true
    preview-links: false
    margin: 0.12
    width: 1000
    chalkboard:
      boardmarker-width: 6
      grid: false
      background:
        - "rgba(255,255,255,0.0)"
        - "https://github.com/rajgoel/reveal.js-plugins/raw/master/chalkboard/img/blackboard.png"
    include-before: <div class="line right"></div>
    include-after: <script>registerRevealCallbacks();</script>
highlight-style: breeze
jupyter: python3
execute:
  keep-ipynb: true
  echo: true
---

# Artificial Intelligence {background-image="unsw-yellow-shape.png"}

## Different goals of AI {.smaller}

Artificial intelligence describes an agent which is capable of:

----------------     -----------------------
Thinking humanly     Thinking rationally
Acting humanly       Acting rationally 
----------------     -----------------------

::: columns
::: {.column width="33%"}

![Turing test (Imitation Game).](Turing_test_diagram.png)

:::

::: {.column width="33%"}

<br><br>

>Socrates is a human
>
>All humans are mortal
>
>$\therefore$ Socrates is mortal.
:::

::: {.column width="33%"}

![A step-by-step answer from Wolfram Alpha.](wolfram-alpha-step-by-step.png)

:::
:::

::: footer
Sources: Russell and Norvig (2002), _Artificial Intelligence: A Modern Approach_, Chapter 1, and [Wikipedia](https://en.wikipedia.org/wiki/Turing_test#/media/File:Turing_test_diagram.png), and [Wolfram Alpha](https://www.wolframalpha.com/input?i=derivative+of+x%5E4+%2B+9x%5E3+%2B+7x+-+2).
:::

## AI is a combination of ...

- philosophy,
- mathematics,
- economics,
- neuroscience and psychology,
- computer science,
- computer engineering,
- control theory and cybernetics,
- linguistics,
- ...

## Early enthusiasm

<br>

> The early years of AI [1952-1969] were full of successes—in a limited way.
> Given the primitive computers and programming tools of the time and the fact that only a few years earlier computers were seen as things that could do arithmetic and no more, **it was astonishing whenever a computer did anything remotely clever**.

<br>

::: footer
Source: Russell and Norvig (2002), _Artificial Intelligence: A Modern Approach_, p. 18.
:::


## Samuel's checkers program

> Starting in 1952, Arthur Samuel wrote a series of programs for checkers (draughts) that eventually learned to play at a strong amateur level.
> Along the way, he disproved the idea that computers can do only what they are told to: **his program quickly learned to play a better game than its creator**.
> The program was demonstrated on television in February 1956, creating a strong impression.

::: footer
Source: Russell and Norvig (2002), _Artificial Intelligence: A Modern Approach_, pp. 18-19.
:::

## The minimax algorithm

![Illustration of minimax on a game of Chess.](sebastian-lague-minimax.png)

::: footer
Source: Sebastian Lague (2018), [Algorithms Explained – minimax and alpha-beta pruning](https://youtu.be/l-hh51ncgDI).
:::

## Deep Blue (1997)

::: columns
::: {.column width="68%"}
![Gary Kasparov playing Deep Blue.](deep-blue.jpeg)
:::
::: {.column width="28%"}
![Cartoon of the match.](deep-blue-cartoon.l2005-6.telegraph1131746-matt-pritchett.jpeg)
:::
:::

::: footer
Sources: Mark Robert Anderson (2017), [Twenty years on from Deep Blue vs Kasparov](https://theconversation.com/twenty-years-on-from-deep-blue-vs-kasparov-how-a-chess-match-started-the-big-data-revolution-76882), The Conversation article, and [Computer History Museum](https://www.computerhistory.org/chess/stl-431e1a079ea63/).
:::

## False optimism

> It is not my aim to surprise or shock you—but the simplest way I can summarize is to say that there are now in the world machines that think, that learn and that create.
> Moreover, their ability to do these things is going to increase rapidly until—in a visible future—the range of problems they can handle will be **coextensive with the range to which the human mind has been applied**.

::: fragment
Herbert Simon (1957)
:::

::: footer
Source: Russell and Norvig (2002), _Artificial Intelligence: A Modern Approach_, Chapter 1.
:::

## False optimism

::: columns
::: {.column width="65%"}

Claimed that in 10 years AI would:

- beat a chess grandmaster (took 40),
- prove a significant mathematical theorem (took 19).

![Map of USA in four colours.](Map_of_United_States_accessible_colors_shown.svg){width="40%"}

:::
::: {.column width="30%"}
![xkcd #1425: Tasks.](xkcd-tasks_2x.png)
:::
:::

::: footer
Sources: Wikipedia page for the [Four colour theorem](https://en.wikipedia.org/wiki/File:Map_of_United_States_accessible_colors_shown.svg), and Randall Munroe (2014), [xkcd #1425: Tasks](https://xkcd.com/1425/).
:::

## AlphaGo (2016)

::: columns
::: {.column width="68%"}
![Lee Sedol plays AlphaGo.](New-Yorker-House-Alpha-Go-2.jpeg)
:::
::: {.column width="28%"}

<br>

Deep Blue was a win for AI.

<br>

AlphaGo was a win for ML/DL.
:::
:::

::: footer
Source: Patrick House (2016), [AlphaGo, Lee Sedol, and the Reassuring Future of Humans and Machines](https://www.newyorker.com/tech/annals-of-technology/alphago-lee-sedol-and-the-reassuring-future-of-humans-and-machines), New Yorker article.
:::

# Machine Learning {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Definition

::: columns 
::: {.column width="55%"}

> "[Machine Learning is the] field of study that gives computers the ability to learn without being explicitly programmed"
Arthur Samuel (1959)

:::
::: {.column width="45%"}
![](xkcd-machine_learning_2x.png)
:::
:::

::: footer
Source: Randall Munroe (2017), [xkcd #1838: Machine Learning](https://xkcd.com/1838/).
:::

## Traditional AI versus ML

<br>

::: columns
::: column
![The traditional approach.](Geron-mls2_0101-blur.png)
:::
::: column
![The machine learning approach.](Geron-mls2_0102-blur.png)
:::
:::

:::footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figures 1-1 and 1-2.
:::

## Benefits of ML

<br>

::: columns
::: {.column .aligned-column}
![Machine learning can automatically adapt to change.](Geron-mls2_0103-blur.png)
:::
::: {.column .aligned-column}
![Machine learning can help humans to learn.](Geron-mls2_0104-blur.png)
::::
:::

:::footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Figures 1-3 and 1-4.
:::

# Categories of Machine Learning Problems {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Types of ML problems

- _Supervised learning_
- Self-supervised learning

Others:

- Unsupervised learning
- Reinforcement learning
- Active learning
- Semi-supervised learning

## Supervised learning

### Regression

- Given policy $\hookrightarrow$ predict the rate of claims.
- Given policy $\hookrightarrow$ predict claim severity.
- Given a reserving triangle $\hookrightarrow$ predict future claims.

### Classification

- Given a claim $\hookrightarrow$ classify as fraudulent or not.
- Given a customer $\hookrightarrow$ predict customer retention patterns.

## Supervised learning: mathematically

![A recipe for supervised learning.](recipe-for-supervised-ml.png)

::: footer
Source: Matthew Gormley (2021), [Introduction to Machine Learning Lecture Slides](https://www.cs.cmu.edu/~mgormley/courses/10601-s17/slides/lecture20-backprop.pdf), Slide 67.
:::

## Self-supervised learning

Data which 'labels itself'. Example: language model.

<center>
![](Chaudhary-nlp-ssl-causal-language-modeling-steps.png)
</center>

!['Autoregressive' (e.g. GPT) versus 'masked' model (e.g. BERT).](Chaudhary-nlp-ssl-masked-lm.png)

::: footer
Source: Amit Chaudhary (2020), [Self Supervised Representation Learning in NLP](https://amitness.com/2020/05/self-supervised-learning-nlp/).
:::

## Example: image inpainting

![Image inpainting example using Mads Mikkelsen in the CelebA-HQ dataset.](liu-2018-fig-8.png)

<br>

Other examples: image super-resolution, denoising images.

::: footer
Source: Liu et al. (2018), [Image Inpainting for Irregular Holes using Partial Convolutions](https://arxiv.org/pdf/1804.07723.pdf), Figure 8.
:::

## Example: Deoldify images #1

![A deoldified version of the famous "Migrant Mother" photograph.](deoldify-migrant-mother.jpeg)

:::footer
Source: [Deoldify package](https://github.com/jantic/DeOldify).
:::

## Example: Deoldify images #2

![A deoldified Golden Gate Bridge under construction.](deoldify-golden-gate-bridge.jpeg)

:::footer
Source: [Deoldify package](https://github.com/jantic/DeOldify).
:::

<script defer>
    var registerRevealCallbacks = function() {
        Reveal.on('overviewshown', event => {
            document.querySelector(".line.right").hidden = true;
        });
        Reveal.on('overviewhidden', event => {
            document.querySelector(".line.right").hidden = false;
        });
    };
</script>