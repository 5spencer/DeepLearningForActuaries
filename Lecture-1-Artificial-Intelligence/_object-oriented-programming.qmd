
# Object-oriented programming {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

## Remember this class?

```{python}
COURSE_CREDITS = {"ACTL3143": 6, "ACTL5001": 12}

class Student:
  def __init__(self, name, zID, grades):
    self.name = name
    self.zID = zID
    self.grades = grades

  def wam(self):
    """
    Calculate the weighted average mark for this student.
    """
    total_credits = 0
    total_grade = 0
    for course, grade in self.grades.items():
      total_credits += COURSE_CREDITS[course]
      total_grade += grade * COURSE_CREDITS[course]
    return total_grade / total_credits
```

## Calling the `wam` method

Now every student object can calculate its own WAM.

```{python}
don = Student("Don Quixote", 111222,
    {"ACTL3143": 100, "ACTL5001": 50})

zhuge = Student("Zhuge Liang", 123456, 
    {"ACTL3143": 100, "ACTL5001": 100})
```

```{python}
don.wam()
```

```{python}
zhuge.wam()
```

## Customising an existing class

```{python}
class PhDStudent(Student):
  def __init__(self, name, zID, grades, supervisor):
    super().__init__(name, zID, grades)
    self.supervisor = supervisor
    self.timeTillGraduation = float("inf")
    self.chanceToFindFreeFood = 0.999
```

```{python}
mei = PhDStudent("Mei Changsu", 123456, 
    {"ACTL3143": 100, "ACTL5001": 100},
    "Lin Chen")

mei.supervisor
```

```{python}
mei.wam()
```

## Load MNIST dataset

```{python}
import random
import numpy
import tensorflow as tf
from tensorflow import keras
from tensorflow.keras import layers

(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()
X_train = X_train.astype("float32") / 255.0
X_test = X_test.astype("float32") / 255.0

# Reserve 10,000 samples for validation.
X_val = X_train[-10000:]
y_val = y_train[-10000:]
X_train = X_train[:-10000]
y_train = y_train[:-10000]

# Prepare the training dataset.
batch_size = 64
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
# train_dataset = train_dataset.shuffle(buffer_size=1024)
train_dataset = train_dataset.batch(batch_size)

# Prepare the validation dataset.
val_dataset = tf.data.Dataset.from_tensor_slices((X_val, y_val))
val_dataset = val_dataset.batch(batch_size)
```

::: footer
Adapted from: Chollet (2020), [Writing a training loop from scratch](https://keras.io/guides/writing_a_training_loop_from_scratch/), Keras docs.
:::

## Example: Monte Carlo dropout

```{python}
random.seed(42)
model = keras.Sequential([
    layers.Flatten(input_shape=[28, 28]),
    layers.Dropout(rate=0.2),
    layers.Dense(300, activation="relu"),
    layers.Dropout(rate=0.2),
    layers.Dense(100, activation="relu"),
    layers.Dropout(rate=0.2),
    layers.Dense(10, activation="softmax")
])
```

```{python}
#| echo: false
numpy.set_printoptions(precision=2)
```

::: columns
::: column
```{python}
model.predict(X_train[[0]], verbose=0)
```
```{python}
model(X_train[[0]], training=True).numpy()
```
:::
::: column
```{python}
model.predict(X_train[[0]], verbose=0)
```
```{python}
model(X_train[[0]], training=True).numpy()
```

:::
:::

## Custom `MCDropout` layer

```{python}
class MCDropout(layers.Dropout):
  def call(self, inputs):
    return super().call(inputs, training=True)
```

```{python}
random.seed(42)
model = keras.Sequential([
    layers.Flatten(input_shape=[28, 28]),
    MCDropout(rate=0.2),
    layers.Dense(300, activation="relu"),
    MCDropout(rate=0.2),
    layers.Dense(100, activation="relu"),
    MCDropout(rate=0.2),
    layers.Dense(10, activation="softmax")
])
model.compile("adam", "mse")
```

::: columns
::: column
```{python}
model.predict(X_train[[0]], verbose=0)
```
:::
::: column
```{python}
model.predict(X_train[[0]], verbose=0)
```
:::
:::

::: footer
Source: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, p. 370 & p. 367
:::

## Encouraging callbacks

[Callback](https://github.com/keras-team/keras/blob/v2.9.0/keras/callbacks.py#L575-L881) is a Keras class that is meant to be subclassed.

```{python}
class EncouragingCallback(keras.callbacks.Callback):
  def on_epoch_end(self, epoch, logs):
    phrases = ["Great work!", "Nearly there", "加油"]
    encourage = phrases[epoch%len(phrases)]
    print(f"Epoch {epoch}: loss={logs['loss']}, {encourage}")
```

```{python}
ec = EncouragingCallback()
model = model.fit(train_dataset, epochs=3, 
        callbacks = [ec], verbose=0);
```

::: footer
Inspired by: Aurélien Géron (2019), _Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow_, 2nd Edition, Chapter 10.
:::

## Keras-tuner 

```{python}
import keras_tuner as kt

def build_model(hp):
  numHidden = hp.Int("units", min_value=32, max_value=512, step=32)
  model = keras.Sequential([
    layers.Flatten(),
    layers.Dense(numHidden, "relu"),
    layers.Dense(10, activation="softmax")
  ])
  model.compile("adam", "sparse_categorical_crossentropy",
      metrics=["accuracy"])
  return model

tuner = kt.RandomSearch(build_model, objective="val_accuracy",
    max_trials=3, seed=42, project_name="optimise-num-hidden-units")
tuner.search(X_train, y_train, epochs=2, validation_data=(X_val, y_val))
tuner.get_best_hyperparameters()[0].get("units")
```

::: footer
Adapted from: Invernizzi et al. (2021), [Getting started with KerasTuner](https://keras.io/guides/keras_tuner/getting_started/), Keras docs.
:::

## Tune fitting hyperparameters

```{python}
class MyHyperModel(kt.HyperModel):
  def build(self, hp):
    numHidden = hp.Int("units", min_value=32, max_value=512, step=32)
    model = keras.Sequential([
      layers.Flatten(),
      layers.Dense(numHidden, "relu"),
      layers.Dense(10, activation="softmax")
    ])
    model.compile("adam", "sparse_categorical_crossentropy",
        metrics=["accuracy"])
    return model

  def fit(self, hp, model, *args, **kwargs):
    batchSize = hp.Int("batchSize", min_value=32, max_value=512, step=32)
    return model.fit(*args, batch_size = batchSize, **kwargs)

tuner = kt.RandomSearch(MyHyperModel(), objective="val_accuracy",
  max_trials=3, seed=123, project_name="optimise-batch-size")
tuner.search(X_train, y_train, epochs=2, validation_data=(X_val, y_val))
tuner.get_best_hyperparameters()[0].get("batchSize")
```

::: footer
Source code for [keras-tuner.HyperModel](https://github.com/keras-team/keras-tuner/blob/1.1.3/keras_tuner/engine/hypermodel.py#L17).
:::