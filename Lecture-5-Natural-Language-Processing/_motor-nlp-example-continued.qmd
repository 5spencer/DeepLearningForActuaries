
# Car Crash NLP Part II {background-image="unsw-yellow-shape.png" data-visibility="uncounted"}

::: footer
Dataset source: [Dr JÃ¼rg Schelldorfer's GitHub](https://github.com/JSchelldorfer/ActuarialDataScience/blob/master/12%20-%20NLP%20Using%20Transformers/Actuarial_Applications_of_NLP_Part_1.ipynb).
:::

## Predict injury severity

```{python}
features = df["SUMMARY_EN"]
target = LabelEncoder().fit_transform(df["INJSEVB"])

X_main, X_test, y_main, y_test = \
    train_test_split(features, target, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val = \
    train_test_split(X_main, y_main, test_size=0.25, random_state=1)
X_train.shape, X_val.shape, X_test.shape
```

## Using Keras `TextVectorization`

```{python}
max_tokens = 1_000
vect = layers.TextVectorization(
    max_tokens=max_tokens,
    output_mode="tf_idf",
    standardize="lower_and_strip_punctuation",
)

vect.adapt(X_train)
vocab = vect.get_vocabulary()

X_train_txt = vect(X_train)
X_val_txt = vect(X_val)
X_test_txt = vect(X_test)

print(vocab[:50])
```

## The TF-IDF vectors

```{python}
pd.DataFrame(X_train_txt, columns=vocab, index=X_train.index)
```

## Feed TF-IDF into an ANN

```{python}
random.seed(42)
tfidf_model = keras.models.Sequential([
    layers.Dense(250, "relu", input_dim=X_train_txt.shape[1]),
    layers.Dense(1, "sigmoid")
])

tfidf_model.compile("adam", "BinaryCrossentropy", metrics=["accuracy"])
tfidf_model.summary(print_fn=skip_empty)
```

## Fit & evaluate

```{python}
es = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True,
    monitor="val_accuracy", verbose=2)

if not Path("tfidf-model.h5").exists():
    tfidf_model.fit(X_train_txt, y_train, epochs=1_000, callbacks=es,
        validation_data=(X_val_txt, y_val), verbose=0)
    tfidf_model.save("tfidf-model.h5")
else:
    tfidf_model = keras.models.load_model("tfidf-model.h5")
```
```{python}
tfidf_model.evaluate(X_train_txt, y_train, verbose=0, batch_size=1_000)
```
```{python}
tfidf_model.evaluate(X_val_txt, y_val, verbose=0, batch_size=1_000)
```

## Keep text as sequence of tokens

```{python}
max_length = 500
max_tokens = 1_000
vect = layers.TextVectorization(
    max_tokens=max_tokens,
    output_sequence_length=max_length,
    standardize="lower_and_strip_punctuation",
)

vect.adapt(X_train)
vocab = vect.get_vocabulary()

X_train_txt = vect(X_train)
X_val_txt = vect(X_val)
X_test_txt = vect(X_test)

print(vocab[:50])
```

## A sequence of integers

```{python}
X_train_txt[0]
```

## Feed LSTM a sequence of one-hots

```{python}
random.seed(42)
inputs = keras.Input(shape=(max_length,), dtype="int64")
onehot = tf.one_hot(inputs, depth=max_tokens)
x = layers.Bidirectional(layers.LSTM(24))(onehot)
x = layers.Dropout(0.5)(x) 
outputs = layers.Dense(1, activation="sigmoid")(x)    
one_hot_model = keras.Model(inputs, outputs)
one_hot_model.compile(optimizer="rmsprop",
    loss="binary_crossentropy", metrics=["accuracy"])
one_hot_model.summary(print_fn=skip_empty)
```

## Fit & evaluate

```{python}
es = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True,
    monitor="val_accuracy", verbose=2)

if not Path("one-hot-model.h5").exists():
    one_hot_model.fit(X_train_txt, y_train, epochs=1_000, callbacks=es,
        validation_data=(X_val_txt, y_val), verbose=0);
    one_hot_model.save("one-hot-model.h5")
else:
    one_hot_model = keras.models.load_model("one-hot-model.h5")
```

```{python}
one_hot_model.evaluate(X_train_txt, y_train, verbose=0, batch_size=1_000)
```
```{python}
one_hot_model.evaluate(X_val_txt, y_val, verbose=0, batch_size=1_000)
```

## Custom embeddings

```{python}
inputs = keras.Input(shape=(max_length,), dtype="int64")
embedded = layers.Embedding(input_dim=max_tokens, output_dim=32,
        mask_zero=True)(inputs)
x = layers.Bidirectional(layers.LSTM(24))(embedded)
x = layers.Dropout(0.5)(x)
outputs = layers.Dense(1, activation="sigmoid")(x)
embed_lstm = keras.Model(inputs, outputs)
embed_lstm.compile("rmsprop", "binary_crossentropy", metrics=["accuracy"])
embed_lstm.summary(print_fn=skip_empty)
```

## Fit & evaluate

```{python}
es = keras.callbacks.EarlyStopping(patience=3, restore_best_weights=True,
    monitor="val_accuracy", verbose=2)

if not Path("embed-lstm.h5").exists():
    embed_lstm.fit(X_train_txt, y_train, epochs=1_000, callbacks=es,
        validation_data=(X_val_txt, y_val), verbose=0);
    embed_lstm.save("embed-lstm.h5")
else:
    embed_lstm = keras.models.load_model("embed-lstm.h5")
```

```{python}
embed_lstm.evaluate(X_train_txt, y_train, verbose=0, batch_size=1_000)
```
```{python}
embed_lstm.evaluate(X_val_txt, y_val, verbose=0, batch_size=1_000)
```
```{python}
embed_lstm.evaluate(X_test_txt, y_test, verbose=0, batch_size=1_000)
```
